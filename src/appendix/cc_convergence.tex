\section{Faster Update Rule}\label{app:s:cc_convergence}
We are now going to state and prove a series of Lemmas that culminate in
the proof of Lemma~\ref{l:cc_convergence}. We first turn our attention to
the problem of calculating the size of window $B$, such that with high probability
all agents have outdateness at most $B$.
We first state a useful fact concerning the coupons collector problem.

\begin{lemma}\label{l:coupons_lemma}
Suppose that the collector picks coupons with mixed
probabilities, where $n$ is the number of distinct coupons.
Let $w$ be the minimum of these probabilities.
If he selects $\ln n/w+ c/w$ coupons, then:
$$
\Prob{\text{collector hasn't seen all coupons}} \leq \frac{1}{\me^c}
$$
\end{lemma}

For convenience of reasoning, we will divide the time in "epochs".
The length of each epoch is $B$, so times $1$ to $B$ belong to the first
epoch etc.
The next lemma is a calculation of the appropriate size of $B$.
\begin{lemma}
Suppose we run Algorithm~\ref{alg:cc_upper} for $t$ rounds. If the size of
each epoch is
\[
B = \frac{2}{\min_{ij}p_{ij}}\ln \frac{nt}{p}
\]
then with probability at least $1-p$ all agents pick all their neighbours
at least once in every epoch.
\end{lemma}
\begin{proof}
In our setting, coupon $i$ corresponds to the selection of neighbour $i$. Each node is a
collector and wants to gather all $n-1$ coupons during each epoch.
Suppose $d = \max_i d_i$ is the maximum degree of the graph.
Then, if we set  $c = \ln (\frac{nt}{p})$ , using Lemma~\ref{l:coupons_lemma}
we get that a node hasn't seen at least one neighbour after $c/w + \ln d/w$ samples
with probability at most $\frac{p}{nt}$. This means that if we set
$D = c/w + \ln d/w =  \ln \frac{nt}{p}/w +  \ln d/w \geq 2/w\ln\frac{nt}{p} $ when $p$ is
small enough, then the probability that a specific agent at a specific epoch hasn't collected
all neighbouring opinions at least once is at most $\frac{p}{nt}$. By a simple union bound argument,
we get that all agents have seen all their neighbours during all epochs
 with probability at least $1 - p$.
\end{proof}
Since all neighbours are picked at least once during each epoch,
the outdatedness of each agent is at most twice the length of the
epoch. We now show Lemma~\ref{l:outdatedness_induction}
from which we get that bounded outdatedness preserves the exponential
convergence.
\begin{lemma}
  Let $\rho = \min_i a_i$, and $\pi_{ij}(t) \in \N$ be the most recent round
  before round $t$, that agent $i$ met agent $j$.
  If for all $t\geq B$, $t-B \leq \pi_{ij}(t) \leq t-1$ then, for
  all $t \geq k B$,
  \(\norm{\infty}{x(t) - x^*} \leq (1-\rho)^k\).
\end{lemma}
\begin{proof}
  To prove our claim we use induction on $k$. For the induction base $k=1$,
  \begin{align*}
    |x_i(t) - x_i^*|
    =
    |(1-\alpha_i)\sum_{j \in N_i}p_{ij}(x_j(\pi_{ij}(t)) -x_j^*)|
    \leq
    (1-\alpha_i)\sum_{j \in N_i}p_{ij}|(x_j(\pi_{ij}(t))-x_j^*)|\leq (1-\rho)
  \end{align*}
  From the induction hypothesis we have for $\pi_{ij}(t) \geq (k-1)B$,
  that $|x_j(\pi_{ij}(t))-x_j^*| \leq (1-\rho)^{k-1}$.
  For $k\geq 2$, we again have that
  $|x_i(t) - x_i^*|\leq (1-\rho)\sum_{j \in N_i}p_{ij}|(x_j(\pi_{ij}(t))-x_j^*)|$.
  Since $t-B \leq \pi_{ij}(t)$ and $t\geq kB$, we have
  that $\pi_{ij}(t) \geq (k-1)B$ and the induction hypothesis applies.
\end{proof}

Combining this with Lemma~\ref{l:outdatedness_induction} we have
the following.
\begin{corollary}\label{cor:window}
If we run Algorithm~\ref{alg:cc_upper} for $t$ rounds, then with probability at least
$1-p$
$$ \norm{\infty}{x^{t}-x^*} \leq (1-\rho)^{\frac{t}{2B}}
\leq \exp \lp(-\frac{\rho t\min_{ij}p_{ij}}{2\ln( \frac{nt}{p})} \rp)$$
\end{corollary}
We now prove Lemma~\ref{l:cc_convergence} using the previous results.
\repeatlemma{l:cc_convergence}
\begin{proof}
Let $u(t) = \norm{\infty}{x^t-x^*}$ and $w = \min_{ij}p_{ij}$.
From Corollary~\ref{cor:window} we obtain:
$$ \Prob{u(t) >\exp\lp(-\frac{\rho wt}{2\ln( \frac{nt}{p})} \rp)} \leq p $$
for every probability $p \in [0,1]$. Also, since all the
parameters of the problem lie in $[0,1]$, we have
$$\Exp{u(t)|u(t) > r} \leq 1$$
Now, by the conditional expectations identity, we get:
\begin{align*}
\Exp{u(t)} &= \Exp{u(t)|u(t) > r}\Prob{u(t) > r} +\Exp{u(t)|u(t) \leq r}\Prob{u(t) \leq r}\\
&\leq p + r
\end{align*}
where $r = \exp \lp(-\frac{\rho wt}{2\ln( \frac{nt}{p})}\rp)$.
If we set $p = \exp \lp(-\frac{\rho w\sqrt{t}}{2\ln nt}\rp)$, then:
$$
\Exp{u(t)} \leq \exp \lp(-\frac{\rho w\sqrt{t}}{2\ln nt}\rp)
+ \exp \lp(-\frac{\rho wt}{2\ln( \frac{nt}{p})}\rp)
$$
We now evaluate $r$ for our choice of probability $p$:
\begin{align*}
r
&= \exp \lp(-\frac{\rho wt}{2\ln\lp( \frac{nt}{p}\rp)} \rp)\\
&= \exp \lp(-\frac{\rho wt}{2\ln\lp( \frac{nt}{\exp \lp(-\frac{\rho w\sqrt{t}}{2\ln nt}\rp) }\rp) } \rp) \\
&= \exp \lp(-\frac{\rho wt}{2\ln nt + 2\frac{\rho w\sqrt{t}}{2\ln nt} }\rp)\\
&\leq \exp \lp(-\frac{\rho w t}{4\ln(nt) \sqrt{t}}\rp)\\
&= \exp \lp(-\frac{\rho w\sqrt{t}}{4\ln(nt)}\rp)
\end{align*}

Using the previous calculation, we obtain:
\begin{align*}
\Exp{u(t)} &\leq \exp \lp(-\frac{\rho w\sqrt{t}}{2\ln (nt)}\rp) +
\exp \lp(-\frac{\rho w\sqrt{t}}{4\ln(nt)}\rp) \\
&\leq 2\exp \lp(-\frac{\rho w\sqrt{t}}{4\ln(nt)}\rp)\\
&=
    2\exp \lp(- \rho  \min_{ij} p_{ij} \frac{\sqrt{t}}{4\ln(nt)} \rp)
\end{align*}
\end{proof}
