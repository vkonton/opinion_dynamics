\section{Full memory with limited information exchange}
In each round, every agent learns a neighbour's opinion and updates the appropriate cell in memory. Because an agent learns only one opinion in each round, he uses outdated information about his other neighbours in order to compute his opinion. We first introduce some convinient notation.
\begin{definition}
$\pi_{ij}(t)$ is the last time that $i$ learned $j$'s opinion until time $t$.
\end{definition}

Obviously, if at time $t$ an agent $i$ learned $k$'s opinion, then $\pi_{ik}(t) = t$. The rule, according to which agent $i$ updates his opinion at time $t$ is the following(for simplicity, assume that $\alpha_i = \frac{1}{d+1}$ and that the graph is $d$-regular):$$x_{i}(t+1)=(1-\alpha_i)\frac{\sum_{j \in N_i}x_j(\pi_{ij}(t))}{d_i}+\alpha_i s_i$$
Notice that in contrast with the traditional model, instead of $x_j(t)$ we have $x_j(\pi_{ij}(t))$. That is because the information about $j$ is probably outdated.
Denote with $x^*$ the solution vector of the system. We would like to prove that such a process converges to the solution of the linear system involving the laplacian matrix of the graph. In order to analyse the algorithm, we will divide the time into "epochs". Each epoch has a length $D \ge d$, so the first epoch is from time 1 to time $D$, the second from $D+1$ to $2D$ e.t.c. Time $0$ does not belong at any epoch. The numbering of epochs begins from $1$. We will also assume that during each epoch, each agent picks every one of his neighbours for update at least once. Later we are going to find a suitable epoch length D, such that this holds with high probability. In other words, for a specific time in epoch $i$ every agent has information which has "bounded" outdateness, since every coordinate was updated at least once during the $i-1$ epoch. We are going to use this fact to prove the following lemma.

\begin{lemma}
Denote with $x^*$ the solution vector of the system. For every time $t$, which belongs to epoch $T$, it holds:
$$ ||x(t)-x^*||_{\infty} \leq \left( 1-\alpha_i\right)^T||x(0)-x^*||_{\infty}$$
\end{lemma}

\begin{proof}
We are going to use induction in time. The base case is for time $t=1$. We are going to prove that: $$ ||x(1)-x^*||_{\infty} \leq (1-\alpha_i)||x(0)-x^*||_{\infty}$$
We assume that everybody has the same initial vector $x(0)$ stored in memory, so for each $i$ holds:
$$x_i(1) = (1-\alpha_i)\frac{\sum_{j \in N_i}x_j(0)}{d}+\alpha_i s_i (1)$$Since $x^*$ is the solution of the system, we have:
$$ x_i^* = (1-\alpha_i)\frac{\sum_{j \in N_i}x_j^*}{d}+\alpha_i s_i (2)$$
Subtracting $(2)$ from $(1)$ we get:
\begin{eqnarray*}
|x_i(1) - x_i^*| &=& |(1-\alpha_i)\frac{\sum_{j \in N_i}(x_j(0)-x_j^*)}{d}|\\
&\leq& (1-\alpha_i)\frac{\sum_{j \in N_i}|x_j(0)-x_j^*|}{d} \\
&\leq& (1-\alpha_i) ||x(0)-x^*||_{\infty}
\end{eqnarray*}
$$ \Rightarrow ||x(1)-x^*||_{\infty} \leq (1-\alpha_i)||x(0)-x^*||_{\infty}$$
So the base case is verified. Let the inductive hypothesis hold for all times until $t$. Suppose that time $t+1$ belongs to epoch $T$. We fix an agent $i$. Then, if $\pi_{ij}(t)$ belongs to the previous epoch, by the induction hypothesis it holds:
$$ |x_j(\pi_{ij}(t)) - x_j^*| \leq \left( 1-\alpha_i\right)^{T-1}||x(0)-x^*||_{\infty}$$
 On the other hand, if $\pi_{ij}(t)$ belongs to the current epoch $T$, by the induction hypothesis we have:
 $$ |x_j(\pi_{ij}) - x_j^*| \leq \left( 1-\alpha_i\right)^{T}||x(0)-x^*||_{\infty} \leq \left( 1-\alpha_i\right)^{T-1}||x(0)-x^*||_{\infty}$$
 So, for every neighbour of agent $i$, the value that $i$ stores for this neighbour is "close" to the optimal. Now, using again equation $(1)$, we have:
 \begin{eqnarray*}
|x_i(t+1) - x_i^*| &=& |(1-\alpha_i)\frac{\sum_{j \in N_i}(x_j(\pi_{ij}(t))-x_j^*)}{d}|\\
&\leq& (1-\alpha_i)\frac{\sum_{j \in N_i}|x_j(\pi_{ij}(t))-x_j^*|}{d}\\
&\leq& (1-\alpha_i)\frac{\sum_{j \in N_i}||x(\pi_{ij}(t))-x^*||_\infty}{d}\\
&\leq& \frac{\sum_{j \in N_i}\left(1-\alpha_i\right)^{T-1}||x(0)-x^*||_{\infty}}{d+1}\\
&=& (1-\alpha_i)\left(1-\alpha_i\right)^{T-1}||x(0)-x^*||_{\infty}\\
&=& \left(1-\alpha_i\right)^{T}||x(0)-x^*||_{\infty}
 \end{eqnarray*}

\end{proof}

\begin{corollary}
If the algorithm runs for $O(D \log \frac{1}{\epsilon})$ iterations, it gets $\epsilon$-close to the solution $x^*$.
\end{corollary}
