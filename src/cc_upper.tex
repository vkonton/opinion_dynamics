\section{Full memory with limited information exchange}
In each round, every agent learns a neighbour's opinion and updates the appropriate cell in memory. Because an agent learns only one opinion in each round, he uses outdated information about his other neighbours in order to compute his opinion. We first introduce some convinient notation.
\begin{definition}
$\pi_{ij}(t)$ is the last time that $i$ learned $j$'s opinion until time $t$.
\end{definition}

Obviously, if at time $t$ an agent $i$ learned $k$'s opinion, then $\pi_{ik}(t) = t$. The rule, according to which agent $i$ updates his opinion at time $t$ is the following(for simplicity, assume that $\alpha_i = \frac{1}{d+1}$ and that the graph is $d$-regular):$$x_{i}(t+1)=(1-\alpha_i)\frac{\sum_{j \in N_i}x_j(\pi_{ij}(t))}{d_i}+\alpha_i s_i$$
Notice that in contrast with the traditional model, instead of $x_j(t)$ we have $x_j(\pi_{ij}(t))$. That is because the information about $j$ is probably outdated.
Denote with $x^*$ the solution vector of the system. We would like to prove that such a process converges to the solution of the linear system involving the laplacian matrix of the graph. In order to analyse the algorithm, we will divide the time into "epochs". Each epoch has a length $D \ge d$, so the first epoch is from time 1 to time $D$, the second from $D+1$ to $2D$ e.t.c. Time $0$ does not belong at any epoch. The numbering of epochs begins from $1$. We will also assume that during each epoch, each agent picks every one of his neighbours for update at least once. Later we are going to find a suitable epoch length D, such that this holds with high probability. In other words, for a specific time in epoch $i$ every agent has information which has "bounded" outdateness, since every coordinate was updated at least once during the $i-1$ epoch. We are going to use this fact to prove the following lemma.

\begin{lemma}
Denote with $x^*$ the solution vector of the system. For every time $t$, which belongs to epoch $T$, it holds:
$$ ||x(t)-x^*||_{\infty} \leq \left( 1-\alpha_i\right)^T||x(0)-x^*||_{\infty}$$
\end{lemma}

\begin{proof}
We are going to use induction in time. The base case is for time $t=1$. We are going to prove that: $$ ||x(1)-x^*||_{\infty} \leq (1-\alpha_i)||x(0)-x^*||_{\infty}$$
We assume that everybody has the same initial vector $x(0)$ stored in memory, so for each $i$ holds:
$$x_i(1) = (1-\alpha_i)\frac{\sum_{j \in N_i}x_j(0)}{d}+\alpha_i s_i (1)$$Since $x^*$ is the solution of the system, we have:
$$ x_i^* = (1-\alpha_i)\frac{\sum_{j \in N_i}x_j^*}{d}+\alpha_i s_i (2)$$
Subtracting $(2)$ from $(1)$ we get:
\begin{eqnarray*}
|x_i(1) - x_i^*| &=& |(1-\alpha_i)\frac{\sum_{j \in N_i}(x_j(0)-x_j^*)}{d}|\\
&\leq& (1-\alpha_i)\frac{\sum_{j \in N_i}|x_j(0)-x_j^*|}{d} \\
&\leq& (1-\alpha_i) ||x(0)-x^*||_{\infty}
\end{eqnarray*}
$$ \Rightarrow ||x(1)-x^*||_{\infty} \leq (1-\alpha_i)||x(0)-x^*||_{\infty}$$
So the base case is verified. Let the inductive hypothesis hold for all times until $t$. Suppose that time $t+1$ belongs to epoch $T$. We fix an agent $i$. Then, if $\pi_{ij}(t)$ belongs to the previous epoch, by the induction hypothesis it holds:
$$ |x_j(\pi_{ij}(t)) - x_j^*| \leq \left( 1-\alpha_i\right)^{T-1}||x(0)-x^*||_{\infty}$$
 On the other hand, if $\pi_{ij}(t)$ belongs to the current epoch $T$, by the induction hypothesis we have:
 $$ |x_j(\pi_{ij}) - x_j^*| \leq \left( 1-\alpha_i\right)^{T}||x(0)-x^*||_{\infty} \leq \left( 1-\alpha_i\right)^{T-1}||x(0)-x^*||_{\infty}$$
 So, for every neighbour of agent $i$, the value that $i$ stores for this neighbour is "close" to the optimal. Now, using again equation $(1)$, we have:
 \begin{eqnarray*}
|x_i(t+1) - x_i^*| &=& |(1-\alpha_i)\frac{\sum_{j \in N_i}(x_j(\pi_{ij}(t))-x_j^*)}{d}|\\
&\leq& (1-\alpha_i)\frac{\sum_{j \in N_i}|x_j(\pi_{ij}(t))-x_j^*|}{d}\\
&\leq& (1-\alpha_i)\frac{\sum_{j \in N_i}||x(\pi_{ij}(t))-x^*||_\infty}{d}\\
&\leq& \frac{\sum_{j \in N_i}\left(1-\alpha_i\right)^{T-1}||x(0)-x^*||_{\infty}}{d+1}\\
&=& (1-\alpha_i)\left(1-\alpha_i\right)^{T-1}||x(0)-x^*||_{\infty}\\
&=& \left(1-\alpha_i\right)^{T}||x(0)-x^*||_{\infty}
 \end{eqnarray*}

\end{proof}

\begin{corollary}
If the algorithm runs for $O(D \log \frac{1}{\epsilon})$ iterations, it gets $\epsilon$-close to the solution $x^*$.
\end{corollary}

We now turn our attention to the problem of computing the appropriate length $D$ of epochs. A useful fact concerning the coupons collector problem is the following.

\begin{lemma}
Suppose that the collector picks $n\ln n + cn$ coupons, where $n$ is the number of distinct coupons. Then: 
$$
\Prob{\text{collector hasn't seen all coupons}} \leq \frac{1}{\me^c}
$$
\end{lemma}

\begin{theorem}
After $t$ rounds, with probability at least $1 - p$: $\norm{\infty}{x^{t}-x^*} \leq (1-a)^{\frac{t}{2d\ln( \frac{dt}{p})} }$
\end{theorem}
\begin{proof}
In our setting, coupon $i$ corresponds to the selection of neighbour $i$. Each node is a collector and wants to gather all $d_i$ coupons during each epoch. Suppose $d = \max_i d_i$ is the maximum degree of the graph. Then, if we set $n = d$ and $c = \ln (\frac{dt}{p})$ , using the previous lemma we get that a node hasn't seen at least one neigbour after $cd + d\ln d$ samples with probability at most $\frac{p}{dt}$. This means that if we set $D = cd + d\ln d = d \ln \frac{dt}{p} + d \ln d \geq 2d\ln\frac{dt}{p} $ when $p$ is small enough, then the probability that a specific agent at a specific epoch hasn't collected all neighbouring opinions at least once is at most $\frac{p}{dt}$. By a simple union bound argument, we get that all agents have seen all their neighbours during all epochs with probability at least $1 - p$. We observe that at time $t$ approximately $\frac{t}{D}$ epochs have passed. Therefore, using the previous result about the convergence rate, we get that with probability at least $1 - p$:
$$ \norm{\infty}{x^{t}-x^*} \leq (1-a)^{\frac{t}{D}} \leq (1-a)^{\frac{t}{2d\ln( \frac{dt}{p})} }$$

\end{proof}
