\section{Introduction}

\subsection{Friedkin-Johsen Model and Opinion Formation Games}
In \cite{BKO11} the following \emph{opinion formation game} was introduced.
A weighted directed graph $G(V,E,w)$ is assumed were the vertices stand for the
agent and the acres the social influence among them. Each agent $i \in V$ poses an
\emph{internal opinion} $s_i \in [0,1]$ and a \emph{self confidence coefficient} $w_i>0$. The strategy of
each agent $i$ is the opinion $x_i\in [0,1]$ that she publicly expresses incuring her cost

\begin{equation}\label{eq:BKO_cost}
  C_i(x_i,x_{-i}) = \sum_{j \in N_i}w_{ij}(x_i-x_j)^2 + w_i(x_i -s_i)^2
\end{equation}
\noindent where $N_i$ denotes $i$'s \emph{neighbors} and $w_{ij}$ stands for
the social influence $j$ imposes on $i$. In \cite{BKO11} they proved that
the above game always admits a \emph{Pure Nash Equilibrium} (PNE) $x^* \in [0,1]^n$
and studied the efficiency of $x^*$. The proved that the \emph{Price of Anarchy}
is less than $9/8$ in case $G$ is bidirectional and $w_{ij}=w_{ji}$.

In the repeated version of the game defined in \ref{eq:BKO_cost}, at each round $t$ each agent $i$
selects an opinion $x_i(t)$ and then suffers cost $C_i(x_i(t),x_{-i}(t))$. If each agent updates
her opinion to the \emph{best response} of $x(t-1)$,
\begin{equation}\label{eq:FJ_model}
x_i(t) = \text{argmin}_{x \in [0,1]}C_i(x,x_{-i}(t-1))=\frac{\sum_{j \in N_i}w_{ij}x_j(t-1) + w_is_i}{\sum_{j \in N_i}w_{ij} + w_i}
\end{equation}
\noindent we obtain the Friedkin-Johsen model (FJ-model), which is one of the most influential models in opinion dynamics.
The convergence properties of the FJ-model have been extensively studied. In \cite{GS14} the proved that the $x(t)$ always converges
to the PNE $x^*$ and they provided bounds for the convergence time for various graph topologies. As a result, the
above \emph{opinion formation game} has some nice algorithmic properties: It always admits a unique equilibrium point $x^*$ and there
exits a simple but most importantly rational update rule for selfish agents that leads the overall system to equilibrium.

\subsection{Opinion Formation Games with Random Payoffs}
Our work is motivated by the fact that the definition of the cost $C_i(x_i,x_{-i})$
in~\ref{eq:BKO_cost} implies that agent $i$ meets with of her neighbors. This is more
clear in the update rule \ref{eq:FJ_model}. Each agent in order to compute her best response
has to learn the opinion of all her neighbors. The latter is seems quite unatural in today's
huge social networks (e.g. Facebook, Twitter e.t.c.) in which each agent may have
several hundreds of friends. With this in mind it is far more reasonable to assume
that each day an agent meets a small subset of her acquaintances and
suffers a cost based on how much she disagrees with them.To capture the above thoughts,
we introduce the following variant of the opinion formation game with random
payoffs.

\begin{definition}\label{d:random_payoff_game}
  %Let $I$ and instance of the opinion formation game defined in \ref{eq:BKO_cost}.
  For a given opinion vector $x \in [0,1]^n$, the cost of agent $i$
  is the random variable $C_i(x_i,x_{-i})$ defined as follows:
  \begin{itemize}
    \item $i$ meets one of her neighbors $j$ with probability $p_{ij}= w_{ij}/\sum_{j\in N_i}w_{ij}$
    \item suffers cost $(1-a_i)(x_i-x_j)^2 + a_i(x_i-s_i)^2$
  \end{itemize}
  where $\alpha_i = w_i/(\sum_{j\in N_i}w_{ij}+w_i)$
\end{definition}
\noindent Our variant has a very natural interpretation: The cost $C_i(x_i,x_{-i})$ in (\ref{eq:BKO_cost})
can be written equivalently
\begin{equation}\label{eq:BKO11_cost_equivalent}
C_i(x_i,x_{-i}) = W_i\lp( (1-\alpha_i)\sum_{j \in N_i} p_{ij}(x_i-x_j)^2  + \alpha_i(x_i-s_i)^2\rp)
\end{equation}
where $W_i=\sum_{j\in N_i}w_{ij} + w_i$ is a positive constant independent of the opinion vector $x\in [0,1]^n$. As a result, the coefficient $\alpha_i$
measures the reluctancy of agent $i$ to adopt an opinion other than $s_i$, while the $p_{ij}$ can be seen the \emph{real} influence that $j$ poses on $i$.
Following the common belief that we are influence more by those we interact oftently, the \emph{real} influence $p_{ij}$ can intepreted as measure of how
often $i$ meets with $j$. In our random payoff variant $p_{ij}$ (Definition ~\ref{d:random_payoff_game}) is exactly the frequency that $i$ meets $j$. 

Equation (\ref{eq:BKO11_cost_equivalent}) also helps us to establish the existence of PNE for the our random payoff variant.
The notion of \emph{Pure Nash Equilibrium} is properly extended in our case with respect to the expected cost of each agent. Namely, 
$x^* \in [0,1]$ is PNE if and only if $\Exp{C(x_i, x_{-i})} \leq \Exp{C(x_i', x_{-i})}$ 
for each agent $i$. Since $\Expnew{}{C_i(x_i,x_{-i})}=(1-\alpha_i)\sum_{j \in N_i}p_{ij}(x_i-x_j)^2 + \alpha_i(x_i-s_i)^2$, 
it follows from (\ref{eq:BKO11_cost_equivalent}) that the \emph{opinion formation game with random payoffs} has the same equilibrium $x^*$ with 
the original \emph{opinion formation game}.

Instead of denoting an instance of the opinion formation game using a graph $G$
and weights $w_{ij}$, $w_i$ we adopt the following more convenient notation.

\begin{definition}\label{d:random_payof_game_instance}
We denote an instance of the opinion formation game with random payoffs as $(P,s,\alpha)$.
\begin{itemize}
 \item $P$ is a $n \times n$  matrix with non-negative elements $p_{ij}$,
  with $p_{ii}=0$ and $\sum_{j=1}^n p_{ij}$ is either $0$ or $1$.
 \item $s \in [0,1]^n$ is the internal opinion vector.
 \item $\alpha \in [0,1]^n$ the self confidence coefficient vector.
 \end{itemize}
\end{definition}
We use this matrix $P$ to simplify notation, $p_{ij} = w_{ij}/(\sum_{j \in N_i}w_{ij}+w_i)$ if $j \in N_i$ and $0$ otherwise.
If $N_i \neq \emptyset$ then $\sum_{j=1}^n p_{ij}=1$ otherwise it is $0$. We remark that in case $N_i=\emptyset$, $\alpha_i=1$
and agent $i$ suffers cost $(x_i-s_i)^2$.
Abusing notation we will sometimes refer to the graph $G$.

%Many recent works (see e.g. \cite{Zhou17}) study games
%with random payoffs, that is agent's $i$ cost ($C_i(x_i,x_{-i})$) is a
%random variable. The random payoff setting can be much more realistic,
%since randomness may naturally occur because of incomplete information, noise
%or other stochastic factors. Motivated by this
%line of research we introduce a random payoff variant of the opinion
%formation game.

\subsection{Our Results}

In this work we study the repeated version of the game in Definition~\ref{d:random_payoff_game}. 
At round $t$, each agent $i$ selects an opinion $x_i(t) \in [0,1]$ and then suffers the
random cost $C_i(x_i(t),x_{-i}(t))$, where $x(t) \in [0,1]$ is the opinion vector at round $t$. 
We are intested in simple and natural update rules
that the agents can adopt such as the resulting opinion vector $x(t)$ converges to $x^*$.
Moreover we require that this update rules be a \emph{rational} behavioral assumption for
selfish agents.

In Section~\ref{s:fictitious_convergence}, we study the convergence properties of $x(t)$ if all agents adopt \emph{fictitious play}
as their update rule. At round $\tau < t$ each agent $i$ experiences disagreement cost
\[(1-\alpha_i)(x-x_{W_i^\tau}(\tau))^2+\alpha_i(x-s_i)^2 \]where $W_i^\tau$
denotes the agent that $i$ met at round $\tau$. For agent $i$ updating her opinion according to \emph{fictitious play}
means that she selects selects her opinion $x_i(t)$ in order to minimize her aggregated cost until round $t$,
\begin{equation}\label{eq:fictitious_play}
x_i(t) =\argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}(1-\alpha_i)(x-x_{W_i^\tau}(\tau))^2+\alpha_i(x-s_i)^2
\end{equation}
\noindent Generally speaking if all agents adopt \emph{fictitious play} the resulting
strategy vector does not converge to the equilibrium. However, in our case if all agent adopt update rule~\ref{eq:fictitious_play}
the resulting opinion vector $x(t)$ converges to $x^*$ with the following
rate.
\begin{theorem}\label{t:fictitious_convergence}
  Let $I = (P,s, \alpha)$ be any instance of the opinion formation
  game of Definition~\ref{d:random_payoff_game} with equilibrium
  $x^* \in [0,1]^n$.  The opinion vector $x(t)$ produced by
  update rule~\ref{eq:fictitious_play} after $t$ rounds satisfies
  \[
    \Expnew{}{\norm{\infty}{x(t) - x^*}} \leq
    C \sqrt{\log n}\frac{(\log t)^{2}}{t^{\min(1/2,\rho)}},
  \]
  where $\rho = \min_{i \in V} a_i$ and $C$ is a universal constant.
\end{theorem}
The update rule (\ref{eq:fictitious_play}) guarantees convergence
while vastly reducing the information exchange between the agents
at each round. In (\ref{eq:fictitious_play}) each agent $i$ learns the opinion of only one agent
at each round whereas in the classical FJ-model (\ref{eq:FJ_model}), agent $i$ must
learn the opinions of all her neighbors. In terms of
total communication needed to get within distance $\eps$ of the
equilibrium $x^*$, the update rule (\ref{eq:fictitious_play}) needs
$O(n \log n)$ communication while (\ref{eq:FJ_model}) needs
$O(|E|)$, where $E$ is the number of edges of graph $G$.
Of course for this difference to be significant we need
each agent to have at least $O(\log n)$ friend (agents $j$ with $p_{ij}>0$). A large social
network like Facebook has approximately $2$ billion users and each user
has usually more that $100$ friends which far more than $\log(2\ 10^9)$.

In Section~\ref{s:fictitious_noregret} we argue that update rule~(\ref{eq:fictitious_play})
is a \emph{rational} behavioral assumption for selfish agents in the sense that 
it ensures \emph{no-regret} for the agents that adopt it. Namely if agent $i$ uses (\ref{eq:fictitious_play})
to select $x_i(t)$, then the average disagreement cost that she experiences is similar to the average cost that she would experience
by expressing the best fixed opinion. The latter holds no matter the way that the other agents select
their opinions and the agents that $i$ meets. The main result of this section is Theorem~\ref{t:fictitious_noregret},
\begin{theorem}\label{t:fictitious_noregret}
Consider the fuction $f:[0,1]^2 \mapsto [0,1]$ with $f(x,b) = (1-\alpha)(x-b)^2 + \alpha(x-s)^2$.
Let $\{b_t\}_{t=1}^\infty$ be an arbitrary sequence with $b_t \in [0,1]$. If $x_t = \argmin_{x \in [0,1]}\sum_{\tau=1}^{t-1}f(x_,b_\tau)$
then for all $t$, 
\[\sum_{\tau=1}^{t}f(x_\tau,b_\tau) \leq \min_{x \in [0,1]}\sum_{\tau=1}^tf(x,b_\tau) + \bigOh{\log t}\]
\end{theorem}
The no-regret property of \emph{fictitious play} follows by direct application of Theorem~\ref{t:fictitious_noregret} 
with $f(x,b) = (1-\alpha_i)(x-b)^2 + \alpha(x-s_i)^2$ and $b_t = x_{W_i^t}(t)$, where $\alpha_i,s_i$ are 
respectively the self confidence coefficient,the internal opinion of $i$ and $W_i^t$ the neighbor 
that agent $i$ meets at round $t$.

Even though our update rule (\ref{eq:fictitious_play}) has the above
desired properties, for a fixed instance $I=(P,s,\alpha)$ it only achieves convergence rate of
$\widetilde{O}(1/t^{\text{min}(\rho,1/2)})$, while the original FJ-model
achieves convergence rate $O(e^{-\rho t})$ \cite{GS14}.
In section \ref{s:lower_bound} we explain this exponential gap.
The reason is that the update rule (\ref{eq:fictitious_play})
only depends on the opinions of the agents that agent $i$ meets,
$\alpha_i$, and $s_i$. We call such update rules \enquote{opinion dependent}.
Observe that fictitious play (\ref{eq:fictitious_play}) is opinion dependent.
  In Theorem~\ref{t:lower_bound} we
show that for any opinion dependent update rule there exists an instance
$I = (P,s,\alpha)$ such that $\poly(1/\eps)$ rounds are required to
achieve convergence within error $\eps$.
\begin{theorem}\label{t:lower_bound}
  Let $A$ be an \emph{opinion dependent} update rule, which all
  agents use to update their opinions.
  For any $c>0$ there exists an instance $I=(P,s,a)$ such that
  \[
    \Exp{\norm{\infty}{x_A(t) - x^*}} = \Omega(1/t^{1+c}),
  \]
where $x_A(t)$ denotes the opinion vector produced by $A$.
\end{theorem}

In order to bypass the lower bound of Theorem~\ref{t:lower_bound},
the update rules must use more information than simply the opinions
of the neighbors, e.g. the indices of the neighbors that agent $i$ meets.
Observe that update rules that ensure no regret for the agents must be opinion
dependent; Theorem~\ref{t:lower_bound} rules out the possibility
that they achieve exponential convergence rate.
In Section~\ref{s:exponential_update_rules} we also argue that
update rules that converge exponentially fast,
are an unrealistic choice for modelling the true behavior of
the agents because of their complicated form.
Precisely, we explicitly construct $2$ update rules that, while they
achieve exponential convergence rate, they are indeed complicated.
The first one if a function of the opinions and the indices of
the agents that $i$ meets. The second one is a function of
the opinions, and the number of neighbors of each agent $i$.
In conclusion, the results of this work indicate that
natural dynamics in our limited information exchange setting
come at the price of slow convergence rate.


% observe that an update rule that ensures no regret must be opinion dependent.
% Furthermore, in Section~\ref{s:exponential_update_rules} we provide $2$ examples of
% update rules that while they converge exponentially fast, they are
% unnatural in the sense that selfish agents wou
%  If an update rule also depends on these indices,
% then the lower bound of Theorem~\ref{t:lower_bound} does not hold.


% To prove this we show that any \emph{limited information update rule} $A$
% (that achieves this convergence rate to the equilibrium) implies
% the existence of an algorithm that uses i.i.d samples from a Bernoulli random
% variable $B(p)$ to estimate its success probability $p$ with the same asymptotic
% error rate for all $p \in [0,1]$.
% Then we prove that such an estimator does not exists (Theorem ??).



\subsection{Related Work}
Our work belongs to the line of work studying the seminal Friedkin-Jonhsen
model \cite{FJ90}. Bindel et al. in \cite{BKO11} defined an opinion
formation game based on the FJ-model and bounded the inefficiency
of its equilibrium point with respect to the total disagreement cost.
Subsequent work bounded the inefficiency of its equilibrium in variants
of the latter game \cite{BGM13, EFHS17, CKO13, BFM16}.
In \cite{GS14} they show that the convergence time depends on
the spectral radius of the adjacency matrix of the graph $G$
and provided bounds in special graph topologies.
In \cite{BFM16}, a variant of the opinion formation game in which social
relations depend on the expressed opinions, is studied.
They prove that, the discretized version of the above game admits
a potential function and thus best-response converges to the
Nash equilibrium. Convergence results in other discretized variants of
the FJ-model can be found in \cite{YOASS13, FGV16}.


In \cite{FV97}, \cite{FS99}, \cite{HM00} they prove that in a finite
if each agent updated her mixed strategy according to a no-regret
algorithm the resulting time-averaged distribution converges to
Coarse Correlated Equilibrium. In the same spirit in \cite{BEL06}
they proved that no-regret dynamics converge to NE in the
case of congestion games. Later in \cite{EMN09} they studied no regret
dynamics in games with infinite strategy space. They proved that for a large
class of games with concave utility function (socially concave games), the
time-averaged strategy vector converges to the pure Nash equilibrium.
More recent work investigate a stronger notion of convergence of
no-regret dynamics. In \cite{CHM17} they show that,
in $n$-person finite generic games that admit unique Nash equilibrium,
the strategy vector converges \emph{locally} and exponentially fast
to the PNE. They also provide conditions for \emph{global} convergence.
Our results fit in this line of research since we show that
for a game with \emph{infinite} strategy space, the strategy vector (not the
time-averaged) converges to the unique Nash equilibrium.

