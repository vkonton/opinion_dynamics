

%matrix with $P_{ij}=(1-a_i)p_{ij}$
%matrix with $P_{ij}=(1-a_i)p_{ij}$
%Equation~\ref{eq:fj_dynamics}
%can also be written in the following matrix form:

%\begin{equation}\label{eq:fj_matrix_form}
%  x(t) =  P x(t-1) + A s,
%\end{equation}
%where $x(t)=(x_1(t),\ldots,x_n(t))$ is the opinion vector at round $t$, $s$ is the initial opinion vector, $P_{n \times n}$ is a substochastic
%matrix with $P_{ij}=(1-a_i)p_{ij}$ and $A_{n \times n}$ is diagonal matrix with $A_{ii}=\alpha_i$. 


\section{Introduction}

\subsection{Friedkin-Johsen Model and Opinion Formation Games}
In the Friedkin-Johsen model (FJ-model), there exist a set of $n$ agents.
Each agent $i$ poses an internal opinion $s_i \in [0,1]$ and a self
confidence coefficient $\alpha_i \in (0,1]$. At each round $t \geq 1$,
agent $i$ \emph{publicly} expresses the opinion $x_i(t)$ that is defined as follows:

\begin{equation}\label{eq:fj_dynamics}
  x_i(t) = (1-\alpha_i) \sum_{j \neq i}p_{ij}x_j(t-1) + \alpha_i s_i
\end{equation}
where $p_{ij} \in [0,1]$ is the influence that $j$ has
imposes on $i$ and it is assumed that $\sum_{j \neq i}p_{ij}=1$. The simplicity of the 
update rule (\ref{eq:fj_dynamics}) makes the FJ-model plausible, because in real social
networks it is very unlikely that agents change their opinions according
to complex rules.

Based on the FJ-model, in \cite{BKO11}, they propose an \emph{opinion formation game} 
in which the strategy that each agent
$i$ adopts, is the opinion $x_i \in [0,1]$
that she publicly expresses incurring her a cost

\begin{equation}\label{eq:kleinberg_cost}
  C_i(x_i,x_{-i})=(1-\alpha_i)\sum_{j \neq i}p_{ij}(x_i -x_j)^2 + \alpha_i(x_i-s_i)^2.
\end{equation}

\noindent In \cite{BK011} they prove that it always admits a unique
Nash Equilibrium $x^*$ and they provide bound for the Price of Anarchy.
It is easy to see that if each agent $i$ at round $t \geq 1$ updates her opinion so as to minimize her
individual cost defined in (\ref{eq:kleinberg_cost}), the resulting dynamics (\emph{best response dynamics}) 
is the dynamics defined by the FJ-model. We denote an instance of the above game as $(P,s,\alpha)$ where
$P_{n \times n}$ is a stochastic matrix with $P_{ij}=p_{ij}$ and zero diagonal entries, $s$ is the initial 
opinion vector and $\alpha$ the self confidence vector. In \cite{GS14} it is proved that for any instance $I=(P,s,a)$ of the FJ-model, the opinion
vector $x(t)=(x_1(t),\ldots,x_n(t))$ converges to the unique equilibrium point
$x^*$. As a result, the game defined in \cite{BKO11} admits several nice properties. It admits
a PNE $x^*$ and that a simple but more importantly a \emph{rational} update rule, 
in the sense the agents they minimize their individual cost, that leads the agents to 
the $x^*$.

\subsection{Opinion Formation Games with Random Payoffs}
In the game defined by (\ref{eq:kleinberg_cost}),
agent's $i$ cost $C_i(x_i,x_{-i})$ is a deterministic function of the
opinion vector $x$. Many recent works (see e.g. \cite{Zhou17}) study games
with random payoffs, that is agent's $i$ cost ($C_i(x_i,x_{-i})$) is a
random variable. The random payoff setting can be much more realistic,
since randomness may naturally occur because of incomplete information, noise
or other stochastic factors. Motivated by this
line of research we introduce a random payoff variant of the opinion
formation game (\ref{eq:kleinberg_cost}).

\begin{definition}\label{d:random_payoff_game}
  Let $I=(P,s,a)$ an instance of the opinion formation game
  and $x$ the opinion vector. Each agent $i$,
  \begin{itemize}
    \item meets another agent $j$ with probability $p_{ij}$
    \item suffers cost
      $C_i(x_i,x_{-i}) = (1-a_i)\sum_{j \in N_i}(x_i-x_j)^2 + a_i(x_i-s_i)^2$
  \end{itemize}
\end{definition}
\noindent Our \emph{Opinion Formation Game with Random Payoffs} is similar to
the \emph{Opinion Formation Game} defined in \cite{BKO11} with the main difference that
for a given opinion vector $x \in [0,1]^n$, the cost $C_i(x_i,x_{-i})$ that agent $i$ 
suffers, instead of being $(1-\alpha_i)\sum_{j \neq i}p_{ij}(x_i-x_j)^2 + \alpha_i(x_i-s_i)^2$,
is a random variable with $\Expnew{}{C_i(x_i,x_{-i})}=(1-\alpha_i)\sum_{j \neq i}p_{ij}(x_i-x_j)^2 + \alpha_i(x_i-s_i)^2$. 
This game is more compatible to a realistic setting since
in real world networks (e.g. Facebook, Twitter e.t.c.), each agent may have
several hundreds of friends. As a result it is far more reasonable to assume that
every day, each agent meets a random small subset of her acquaintances and
suffers a cost based on how much she disagrees with them.
In our random payoff variant, we can define
the Pure Nash Equilibrium (PNE) with respect to the expected cost of each agent.
More precisely, an opinion vector $x^*$ is a PNE if and only if for each agent $i$,
\[\Exp{C(x_i, x_{-i})} \leq \Exp{C(x_i', x_{-i})}, \text{ for all }x_i' \in [0,1]\]
The existence of the unique the PNE $x^*$ follows directly by the results in \cite{BKO11}.

\noindent In the the repeated version of our game, all agents adopt their internal opinions i.e. $x_i(0)=s_i$. 
At each round $t$,
\begin{itemize}
 \item each agent $i$ adopts an opinion $x_i(t)$
 \item meets one agent $j$ with probability $p_{ij}$
 \item suffers cost $C_i(x_i(t),x_j(t)) = (1-\alpha_i)(x_i(t)-x_j(t)) +\alpha_is_i$
\end{itemize}

\noindent In this work we investigate whether there exists a simple and natural update rule that the agents can 
adopt such that the respective opinion vector converges to the unique equilibrium point $x^*$.
Since at each round $t$ each agent $i$ meets just one agent $j$, we require that this update
rule to be a function of just the opinions that $i$ has seen until round $t$, the internal opinion 
$s_i$ and the self confidence $\alpha_i$. Moreover, we would like this update rule to be 
a \emph{rational} behavioral assumption for selfish agents.

%In this work, we investigate whether there exists \emph{natural} update rules that the agents
%can adopt and the resulting  vector opinion $x(t)$ converges to $x^*$. 
%By \emph{natural} we mean that each agent $i$ at round $t$ just knows the opinions of the agents that 
%she has met until round $t$ and this is the only information that she can 
%use to update her opinion. For example, \emph{best response} is not natural in this sense because
%each agent must be aware the opinion of all the other agents at round $t$ in order to update her opinion.
%At the same time the update rule must be a \emph{rational} behavioral assumption for selfish agents. Thus,
%we consider update rules $A$ having the following of the following form:
%\begin{itemize}
% \item $x_i(t)= A^t(Y_1^{t-1},\alpha_i,s_i)~\text{limited information assumption}$ 
% \item $\sum \limits_{\tau=1}^tC_i(x_i(\tau),Y_\tau) \leq \argmin_{x \in [0,1]}\sum \limits_{\tau=1}^tC_i(x,Y_\tau) + o(t)~\text{no regret assumption}$
%\end{itemize}


%\begin{algorithm}
%  \caption{Fictitious Play}
%  \label{alg:fictitious_play}
%  \begin{algorithmic}[1]
%    \State Initially, each agent $i$ adopts the opinion $s_i$.
%    \State At round $t \geq 1$, each agent $i$ meets another agent $j$ with probability $p_{ij}$.
%    \State Suffers cost
%    \(
%      C^t_i(x_i(t-1),x_{j}(t-1))=(1-a_i)(x_i(t-1)
%      -x_j(t-1))^2 + a_i(x_i(t-1)-s_i)^2
%    \)
%    \State Updates opinion as follows:
%    \begin{equation}\label{eq:fictitious_play}
%      x_i(t) =
%      \argmin_{x \in [0,1]}
%      \sum_{\tau=1}^tC^{\tau}_i(x,x_j(\tau-1))
%    \end{equation}
%\end{algorithmic}
%\end{algorithm}

\subsection{Our Results and Techniques}
In this work, we study the convergence properties
of the opinion vector $x(t)$ if all agents play according to \emph{fictitious play}.
Namely, if $W_i^\tau$ denotes the agent that agent $i$ met at round $\tau$, then
\begin{equation}\label{eq:fictitious_play}
x_i(t) =\argmin_{x \in [0,1]}\sum_{\tau=1}^{t-1}(1-\alpha_i)(x-x_{W_i^\tau}(\tau))^2+\alpha_i(x-s_i)^2
\end{equation}
Generally speaking \emph{fictitious play} does not guarantee
convergence to the equilibrium. In Section~\ref{s:fictitious_convergence} 
we show that, for our game with random payoffs, fictitious play converges to equilibrium, with the following
rate.
\begin{theorem}\label{t:fictitious_convergence}
  Let $I = (P,s, \alpha)$ be any instance of the opinion formation
  game of Definition~\ref{d:random_payoff_game} with equilibrium
  $x^* \in [0,1]^n$.  The opinion vector $x(t)$ produced by
  Algorithm~\ref{alg:fictitious_play} after $t$ rounds satisfies
  \[
    \Expnew{}{\norm{\infty}{x(t) - x^*}} \leq
    C \sqrt{\log n}\frac{(\log t)^{2}}{t^{\min(1/2,\rho)}},
  \]
  where $\rho = \min_{i \in V} a_i$ and $C$ is a universal constant.
\end{theorem}
The update rule (\ref{eq:fictitious_play}) guarantees convergence
while vastly reducing the information exchange between the agents
at each round. In (\ref{eq:fictitious_play}) each agent $i$ learns the opinion of only one agent
at each round whereas in the classical FJ-model (\ref{eq:fj_dynamics}), agent $i$ must
learn the opinions of all the agents $j$ with with $p_{ij}>0$. In terms of
total communication needed to get within distance $\eps$ of the
equilibrium $x^*$, the update rule (\ref{eq:fictitious_play}) needs
$O(n \log n)$ communication while (\ref{eq:fj_dynamics}) needs
$O(|E|)$, where $E$ is the set of positive entries in matrix $P$. 
Of course for this difference to be significant we need
each agent to have at least $O(\log n)$ friend (agents $j$ with $p_{ij}>0$). A large social
network like Facebook has approximately $2$ billion users and each user
has usually more that $100$ friends which far more than $\log(2\ 10^9)$.

Apart from converging to the equilibrium, our update rule
(\ref{eq:fictitious_play}) also ensures \emph{no-regret} for the agents.
Having no-regret means that the average cost for each agent $i$
after $T$ rounds is close to the average cost that she would
suffer by expressing any fixed opinion. This is a very important
feature of our update rule because even players that selfishly
will to minimize their incurred cost, could choose to play according
to it. In Section~\ref{s:fictitious_noregret} we show the following
theorem.

\begin{theorem}\label{t:fictitious_noregret}
For every instance $I=(P,s,a)$, for every agent $i$
\[\sum_{\tau=1}^tC_i(x_i(\tau),x_{W_i^\tau}(\tau)) \leq \text{min}_{x \in [0,1]}\sum_{\tau=1}^tC_i(x,x_{W_i^\tau}(\tau)) + \bigOh{\log t}\]
\end{theorem}

Even though our update rule (\ref{eq:fictitious_play}) has the above
desired properties, for a fixed instance $I=(P,s,\alpha)$ it only achieves convergence rate of
$\widetilde{O}(1/t^{\text{min}(\rho,1/2)})$, while the original FJ-model outperforms
our update rule since it achieves convergence rate $O(e^{-\rho t})$ \cite{GS14}.
In section 5 define the \emph{limited information update rules}, that it is the class
of update rules that for each agent $i$: depend only on the opinions that she agent has met
until time $t$ and her $s_i,\alpha_i$ (\emph{fictitious play} in \ref{eq:fictitious_play} 
is clearly an \emph{limited information update rule}).
In section 5, we prove if each agent $i$ updates her 
opinion at round $t$ according to $A$. Then the resulting opinion vector $x_A(t)$ cannot converge to the equilibrium $x^*$ 
with exponential asymptotic rate. More precisely,
\begin{theorem}\label{thm:lower_bound}
If all agents update their opinion according to limited information update rule $A$. Then for any $c>0$ there
exists an instance $I=(P,s,a)$ such that $\Exp{\norm{\infty}{x_A(t) - x^*}} = \Omega(1/t^{1+c})$.
\end{theorem}

To prove this we show that any \emph{limited information update rule} $A$
(that achieves this convergence rate to the equilibrium) implies
the existence of an algorithm that uses i.i.d samples from a Bernoulli random
variable $B(p)$ to estimate its success probability $p$ with the same asymptotic
error rate for all $p \in [0,1]$. Then we prove that such an estimator does not exists (Theorem ??).

In section 6, we investigate whether this lower bound can be extended to more
general class of estimators. We present an update rule that also depends on some parameters of the influence matrix $P$ 
and acheives exponential convergence. The latter implies that \emph{limited information update rules}
is the maximal class that the lower bound applies.

% In section 4, we investigate where a better convergence rate,
% $E[||x(t)-x^*||_{\infty}]$ can be acheived if agent selected another no-regret
% algorithm. More precisely, we investigate the following question, \emph{Is
%   there a no-regret algorithm such that for every instance }
% $I,~E[||x(t)-x^*||_{\infty}] \in \bigOh{\frac{1}{t}}?$  We reduce this question
% to the following question in statistical estimation, \emph{Is there a Bernoulli
%   estimator }$\hat{\theta}$\emph{, such that for all }$q \in [0,1]$, $\lim_{t
%   \rightarrow \infty} t E_q[|\hat{\theta^t} -q|]=0?$  To the best of our
% knowledge this question is not answered in statistics literature. However, we
% use standard tecnhiques for lower bounds in the statistical estimation to prove
% the following theorem.

% \begin{theorem}
%   For any Bernoulli estimator
%   $\hat{\theta}$, for all $[a,b] \subseteq [0,1]:~$ $\lim_{t \rightarrow
%     \infty}t \int_{a}^bE_p[|\hat{\theta^t} -p|]= +\infty$
% \end{theorem}
% This result indicates that the second question and consequently the
% first is very unlike to hold, especially for any reasonable no-regret
% algorithm. We believe that there exists no such algorithm and we leave this
% proof as an open problem.\\

% Finally in section 5, we present an interesting side result. The reason that
% there does not exists a no-regret algorithm, that ensures faster convergence
% rate, is the algorithm's ignorance to the instance $I$ that defines the
% dynamics. More formally, a no-regret algorithm does needs the values $s_i,a_i$
% and $Y_1,\ldots,Y_t$ to determine $x_i(t)$. We find interesting that we design
% a distributed algorithm that uses $s_i,a_i,Y_1,\ldots,Y_t$ and additionally the
% $d_\text{max}$ of $G$ that achieves for every instance $I$ convergence rate,
% $E||x(t)-x^*||_{\infty} \in \bigOh{2^{-\frac{\sqrt{t}}{d^3}}}$.

\subsection{Related Work}
Our work belongs to the line of work studying the seminal Friedkin-Jonhsen
model \cite{FJ90}. Bindel et al. in \cite{BKO11} defined an opinion
formation game based on the FJ-model and bounded the inefficiency
of its equilibrium point with respect to the total disagreement cost.
Subsequent work bounded the inefficiency of its equilibrium in variants
of the latter game \cite{BGM13, EFHS17, CKO13, BFM16}.
In \cite{GS14} they show that the convergence time depends on
the spectral radius of the adjacency matrix of the graph $G$
and provided bounds in special graph topologies.
In \cite{BFM16}, a variant of the opinion formation game in which social
relations depend on the expressed opinions, is studied.
They prove that, the discretized version of the above game admits
a potential function and thus best-response converges to the
Nash equilibrium. Convergence results in other discretized variants of
the FJ-model can be found in \cite{YOASS13, FGV16}.


In \cite{FV97}, \cite{FS99}, \cite{HM00} they prove that in a finite
if each agent updated her mixed strategy according to a no-regret
algorithm the resulting time-averaged distribution converges to
Coarse Correlated Equilibrium. In the same spirit in \cite{BEL06}
they proved that no-regret dynamics converge to NE in the
case of congestion games. Later in \cite{EMN09} they studied no regret
dynamics in games with infinite strategy space. They proved that for a large
class of games with concave utility function (socially concave games), the
time-averaged strategy vector converges to the pure Nash equilibrium.
More recent work investigate a stronger notion of convergence of
no-regret dynamics. In \cite{CHM17} they show that,
in $n$-person finite generic games that admit unique Nash equilibrium,
the strategy vector converges \emph{locally} and exponentially fast
to the PNE. They also provide conditions for \emph{global} convergence.
Our results fit in this line of research since we show that
for a game with \emph{infinite} strategy space, the strategy vector (not the
time-averaged) converges to the unique Nash equilibrium.

