\section{Introduction}
The formation and dynamics of opinions are an important aspect in modern 
society and have been studied extensively for decades (see e.g., \cite{Jackson}).
Opinion formation is based on information exchange, between that socially 
connected people (e.g., family, friends, colleagues) who interact often 
and affect each other's opinion. Moreover, opinion
formation is often \emph{dynamic} in the sense that discussions and 
interactions lead to changes in the expressed opinions. With the 
advent of the internet and social media the dynamic aspects of 
opinion formation have become ever more dominant. To capture opinion formation 
on a formal level, several models have been proposed 
(see e.g., \cite{DeGroot,FJ90,HK,BKO11,GS14,BGM13} for 
continuous opinions and \cite{FGV12,YOASS13,BFM16} for discrete ones). 
A common assumption, that dates back to DeGroot \cite{DeGroot}, is 
that opinions evolve through a form of repeated averaging of 
information collected from the agent social neighborhoods.

\subsection{Motivation}
Our work builds on the influential model of Friedkin and
Johnsen \cite{FJ90}. According to FJ-model, each agent $i$ holds an 
internal opinion $s_i\in [0,1]$, which is private and
invariant over time and a public opinion $x_i \in [0,1]$
Initially, agents start with their internal opinion and at 
each round $t\geq1$, update their public opinion
$x_i(t)$ to a weighted average of public opinion of 
their social neighbors and their internal opinion, i.e.
\[x_i(t)= \frac{\sum_{j\neq i}w_{ij}x_j(t-1) + w_{ii}s_i}{\sum_{j\neq i}w_{ij}+w_{ii}}\]
where the weights $w_{ij}$ indicate the influence between
the agents and $w_{ii}$ the self confidence towards their 
internal belief. 

The FJ-model is one of the most intensively studied models
in opinion dynamics. It admits a 
unique equilibrium point $x^* \in [0,1]^n$ to which
the opinion vector $x(t)$ converges fast exponentially fast
\cite{GS14}. The FJ-model can also be seen as the \emph{best response
dynamics} in the repeated version of the following one shot
opinion formation game, introduced by Kleinberg et al. in \cite{BKO11}. 
The strategy of each agent $i$ is the public opinion $x_i$ that she expresses,
icuring her a disagreement cost \[C_i(x_i,x_{-i})=\sum_{j \neq i}w_{ij}(x_i-x_j)^2 + w_{ii}(x_i-s_i)^2\]
In \cite{BKO11}, they studied the efficiency of equilibrium $x^*$ in terms
of the total disagreement cost and proved that the 
\emph{Price of Anarchy} is at most $9/8$ in case $w_{ij}=w_{ji}$.

From a game-theoretic perspective FJ-model admits
very nice properties. It has a simple update, making it
a plausible choice for modeling natural behavior, that
is also the \emph{best response} of a well 
defined opinion formation game. Moreover it ensures
convergence to the unique equilibrium $x^*$ of the opinion 
formation game. However from a 
distributed computing point of view, things are not
so great. The update rule of FJ-model requires that each agent learns
the opinion of all her social neighbors at each 
round. In todays's large social networks each user
may have several hundrends of friends and obviously
she cannot learn the opinion of all them each day.
This introduce some skeptism on how well the 
FJ-model resembles the opinion formation process in such
network.

Our work is motivated by the following questions.
\emph{
Can we find models similar to FJ-model that 
require less information exchange between the agents
and ensure convergence to the equilibrium $x^*$?
Can these models be justified as natural behavior for selfish
agents under a game-theoretic solution concept?}

Generally speaking, it is not hard to 
design distributed protocols that simulate FJ-model
and require each agent to learn \emph{just one} opinion 
of her neighbors at each iteration. The problem is that these protocols 
are way too \enquote{\emph{algorithmic}} and thus not suitable modeling 
natural behavior. In order to formally establish what
the word \enquote{\emph{natural}} means, we introduce a randomized 
variant of the one shot game defined in \cite{BKO11},
called \emph{opinion formation game with random payoffs}.
For an public opinion vector $x \in [0,1]^n$, the disagreement 
cost $C_i(x_i,x_{-i})$ that each agent $i$ receives is
a random variable defined as follows:
\begin{itemize}
 \item each agent $i$ meets \emph{one} of her neighbors $j$ with probability
 $p_{ij}=w_{ij}/(\sum_{j\neq i}w_{ij}+w_{ii})$
 \item and receives disagreement cost, $(1-\alpha_i)(x_i-x_j)^2 + \alpha_i(x_i-s_i)^2$ 
\end{itemize}
where $\alpha_i=w_{ii}/(\sum_{j\neq i}w_{ij}+w_{ii})$. The above game
has the same Nash equilibrium $x^*$ (w.r.t. the expected cost) as
the original opinion formation game and admits a nice interpretation
that we discuss latter.

Throughout the paper, we consider the agents to be engaged in the repeated 
version of the above one shot game. The reason for this
consideration is twofold. The first is convience, the repeated
version of the above specifies the way that the agents communicate i.e.
at each round $t$ each agent $i$ learns just the opinion 
of the agent that she randomly met. The second reason is that we can now define 
what natural behavior is. Since at each round each agent
suffers a (random) disagreement cost that she selfishly want 
to minimize, a natural update rule must provide garantees about
the cost that the agent experiences during the game play.
This work provides answers to the following questons:

\emph{How can the agents update their opinions in the above
repeated game such that:}
\begin{itemize}
\item \emph{Their experienced disagreement cost of each agent is in a sense minimized.}
\item \emph{The produced opinion vector $x(t)$ converges to equilibrium $x^*$
 relatively fast.}
\end{itemize}

\subsection{Contribution}
We introduce a simple and intuitive update rule,
similar to that of FJ-model, that the agents can adopt
and the resulting opinion vector $x(t)$ converges to $x^*$.
Our update rule is a \emph{Follow the Leader alogrithm}
meaning that each round $t$, each agent updates her opinion
to the minimizer of total disagreement cost that
she experienced until round $t-1$. In section~\ref{s:fictitious_convergence},
we bound its convergence time and we show that in order
to acheive $\eps$ distance form $x^*$, poly($1/\eps$) rounds 
are needed. In section~\ref{s:fictitious_noregret},
we show that any agent has \emph{no-regret} in adopting
this update rule. Namely, the average disagreement cost (that
the agent experiences) per round approaches that
of expressing the best opinion in hindsight. The latter
makes our algorithm a natural choice for agents that 
selfishly want to minize their incured disagreement cost.
Our results contribute to showing that the FJ-model 
can be extended with simple variants to explain
the opinion formation process in enviroments with limited
information exchange. 

In section~\ref{s:lower_bound}, we show 
that for any update rule that ensures \emph{no-regret}
to the agents, the resulting opinion vector $x(t)$
cannot converge to $x^*$ faster than polynomially. We
prove the latter for a larger class of update rules,
\emph{opinion dependent update rules} using information
theoretic arguments. This implies that in our limited 
information setting natural models cannot converge exponentially
fast. Finally in Section~\ref{s:cc_convergence}, we present 
an update rule that is not opinion dependent and acheives exponential
convergence. Our results indicate the fundamental reason that
the FJ-model converges exponential fast and this has little 
to do with the \enquote{large} information exchange that it requires,
they also serve as an \enquote{algorithmic guide} for future 
variants of the FJ-model. 


\subsection{Related Work}
Apart from the forementioned results there exists a large amount 
of literature concerning the FJ-model.
Many recent works \cite{BGM13,CKO13,BFM16,EFHS17} bound the 
inefficiency of equilibrium in variants of opinion formation game 
defined in \cite{BKO11}. In \cite{GS14} they bound that the convergence 
time of the FJ-model in in special graph topologies.
In \cite{BFM16}, a variant of the opinion formation game in which social
relations depend on the expressed opinions, is studied.
They prove that, the discretized version of the above game admits
a potential function and thus best-response converges to the
Nash equilibrium. Convergence results in other discretized variants of
the FJ-model can be found in \cite{YOASS13,FGV16}. In \cite{FPS16} the convergence
properties of limited information variants of the Heglesmann-Krause model \cite{HK} 
and the FJ model, are examined.


Other works, that relate to ours, concern the convergence
properties of dynamics based on no-regret learning algorithms.
In \cite{FV97,FS99,SA00,SALS15} it is proved that in a finite $n$-person
game if each agent updates her mixed strategy according to a no-regret
algorithm the resulting \emph{time-averaged} strategy vector converges to
Coarse Correlated Equilibrium. The convergence properties of no-regret dynamics 
for games with infinite strategy spaces were considered in \cite{EMN09}.
They proved that for a large class of games with concave utility function
(socially concave games), the time-averaged strategy vector converges to
the PNE. More recent work investigate a stronger notion of convergence of
no-regret dynamics. In \cite{CHM17} they show that,
in $n$-person finite generic games that admit unique Nash equilibrium,
the strategy vector converges \emph{locally} and exponentially fast
to it. They also provide conditions for \emph{global} convergence.
Our results fit in this line of research since we show that
for a game with \emph{infinite} strategy space, the strategy vector
(and not the time-averaged) converges to the unique Nash equilibrium.

No-regret dynamics under limited information are also examined
in other settings. In Kleinberg et al. in \cite{KPT09} treated 
load-balancing in distributed systems as
a repeated game and analyzed the convergence properties
no-regret online algorithms under the \emph{full information assumption} 
that each agent learns the load of every machine. 
In a subsequent work \cite{KPT11}, the same authors consider the
same problem in a \emph{limited information setting} (\enquote{bulletin board model})
in which each agent learns the load of just the machine 
that served him. In \cite{HCM17,MS17} they examine the convergence
properties of online learnig algorithms in case the payoffs oberved 
by the agents are contaminated with some random noise. 



%Blum et. al in \cite{BEL06,BHLR08}
%studied the efficiency of outcomes produced by no-regret
%learning algorithm in congestion games and showed that
%their quality is close to that of a NE.
%.
%A subsequent similar work, which is perhaps closer to ours, is \cite{KPT11},
%where they assumed a more realistic \emph{limited information setting},
%namely \enquote{bulletin board model} for load balancing.


\subsection{Friedkin-Johsen Model and Opinion Formation Games}
In \cite{BKO11} the following \emph{opinion formation game} was introduced.
A weighted directed graph $G(V,E,w)$ is assumed where and the vertices
stand for the $n$ agents ($|V| = n$) and the acres for the social
influence among them. Each agent $i \in V$ possess an
\emph{internal opinion} $s_i \in [0,1]$ and a \emph{self confidence coefficient}
$w_i>0$. The strategy of each agent $i$ is the opinion $x_i\in [0,1]$ that
she publicly expresses incurring her cost
%
\begin{equation}\label{eq:BKO_cost}
  C_i(x_i,x_{-i}) = \sum_{j \in N_i}w_{ij}(x_i-x_j)^2 + w_i(x_i -s_i)^2
\end{equation}
%
where $N_i$ denotes $i$'s \emph{neighbors} and $w_{ij}$ stands for
the social influence $j$ imposes on $i$. In \cite{BKO11} they proved that
the above game always admits a \emph{Pure Nash Equilibrium} (PNE) $x^* \in [0,1]^n$
and studied its efficiency with respect to the total disagreement cost.
They proved that the \emph{Price of Anarchy}
is less than $9/8$ in case $G$ is bidirectional and $w_{ij}=w_{ji}$.

In the repeated version of the game defined in (\ref{eq:BKO_cost}),
at each round $t$ each agent $i$ selects an opinion $x_i(t)$ and then suffers
cost $C_i(x_i(t),x_{-i}(t))$. If each agent updates her opinion to be the
\emph{best response} of $x(t-1)$,
%
\begin{equation}\label{eq:FJ_model}
  x_i(t) =
  \argmin_{x \in [0,1]}C_i(x,x_{-i}(t-1))=
  \frac{\sum_{j \in N_i}w_{ij}x_j(t-1) + w_is_i}{\sum_{j \in N_i}w_{ij} + w_i}
\end{equation}
%
we obtain the Friedkin-Johsen model (FJ-model), which is one of
the most influential models in opinion dynamics.
The convergence properties of the FJ-model have been extensively studied.
In \cite{GS14} they proved that $x(t)$ always converges
to the PNE $x^*$ and provided bounds for the convergence time for various
graph topologies. As a result, the
above \emph{opinion formation game} has some nice algorithmic properties: It
always admits a unique equilibrium point $x^*$ and there
exists a simple but most importantly rational update rule for selfish agents
that leads the overall system to equilibrium.

\subsection{Opinion Formation Games with Random Payoffs}
Our work is motivated by the fact that the definition of the cost $C_i(x_i,x_{-i})$
in~(\ref{eq:BKO_cost}) implies that agent $i$ meets with all of
her neighbors. This is more
clear in the update rule (\ref{eq:FJ_model}). Each agent, in order to
compute her best response, has to learn the opinion of all her neighbors.
The latter seems quite unnatural in today's huge social networks
(e.g. Facebook, Twitter etc.), in which each user may have
several hundreds of friends. Thus, it is far more reasonable to assume
that each day an agent meets a small subset of her acquaintances and
suffers a cost based on how much she disagrees with them. To capture the above thoughts,
we introduce a variant of the opinion formation game in which the
disagreement cost of each agent $i$ is a random variable depending
on the random meetings of $i$.
%
\begin{definition}\label{d:random_payoff_game}
  For a given opinion vector $x \in [0,1]^n$, the disagreement cost of agent $i$
  is the random variable $C_i(x_i,x_{-i})$ defined as follows:
  \begin{itemize}
    \item $i$ meets one of her neighbors $j$ with probability
      $p_{ij}= w_{ij}/\sum_{j\in N_i}w_{ij}$
    \item suffers cost $(1-a_i)(x_i-x_j)^2 + a_i(x_i-s_i)^2$
  \end{itemize}
  where $\alpha_i = w_i/(\sum_{j\in N_i}w_{ij}+w_i)$
\end{definition}
%
The main difference of the original opinion formation game with our variant is that
in the first case an opinion vector $x\in [0,1]^n$ defines \emph{deterministically}
the cost $C_i(x_i,x_{-i})$ of each agent $i$, whereas in the second
case it defines (according to Definition~\ref{d:random_payoff_game})
a probability distribution on the cost $C_i(x_i,x_{-i})$ that $i$ suffers.
Recent works \cite{ZLZ17,CLL16} study games
with random payoffs. The reason is that the random payoff setting
is more suitable to model realistic situations in which
randomness naturally occurs because of incomplete information.

The cost $C_i(x_i,x_{-i})$ in (\ref{eq:BKO_cost}) can be written equivalently
\begin{equation}\label{eq:BKO11_cost_equivalent}
  C_i(x_i,x_{-i}) =
  W_i\lp( (1-\alpha_i)\sum_{j \in N_i} p_{ij}(x_i-x_j)^2
  + \alpha_i(x_i-s_i)^2\rp)
\end{equation}
where $W_i=\sum_{j\in N_i}w_{ij} + w_i$ is a positive constant independent
of the opinion vector $x\in [0,1]^n$.
Thus, the random cost in Definition~\ref{d:random_payoff_game} has a natural
interpretation: the coefficient $\alpha_i$
measures the reluctance of agent $i$ to adopt an opinion other than $s_i$, while
$p_{ij}$ can be seen as the \emph{real} influence that $j$ poses on $i$.
In Definition~\ref{d:random_payoff_game}, $p_{ij}$ is the frequency that
$i$ meets $j$, meaning that the influence that $j$ poses on $i$ is just a measure
on how often they meet. The latter aligns with the common belief
that we are influenced more by those we interact more often.
Equation (\ref{eq:BKO11_cost_equivalent}) also helps to establish the existence of PNE for
our random payoff variant. In our case, the notion of PNE extends
with respect to the expected cost of each agent. Namely,
$x^* \in [0,1]$ is a PNE if and only if
$\Exp{C(x_i^*, x_{-i}^*)} \leq \Exp{C(x_i, x_{-i}^*)}$
for each agent $i$.
Since
$\Expnew{}{C_i(x_i,x_{-i})}=
(1-\alpha_i) \sum_{j \in N_i}p_{ij}(x_i-x_j)^2 + \alpha_i(x_i-s_i)^2 $,
it follows from (\ref{eq:BKO11_cost_equivalent}) that the
opinion formation game with random payoffs has the same equilibrium $x^*$ as
the original opinion formation game.

Instead of denoting an instance of the opinion formation game using a graph $G$
and weights $w_{ij}$, $w_i$ we adopt the following more convenient notation.
%
\begin{definition}\label{d:random_payof_game_instance}
  We denote an instance of the opinion formation game with random payoffs as $(P,s,\alpha)$.
  \begin{itemize}
    \item $P$ is a $n \times n$  matrix with non-negative elements $p_{ij}$,
      with $p_{ii}=0$ and $\sum_{j=1}^n p_{ij}$ is either $0$ or $1$.
    \item $s \in [0,1]^n$ is the internal opinion vector.
    \item $\alpha \in [0,1]^n$ the self confidence coefficient vector.
  \end{itemize}
\end{definition}
%
We use the matrix $P$ to simplify notation, $p_{ij} = w_{ij}/(\sum_{j \in N_i}w_{ij}+w_i)$ 
if $j \in N_i$ and $0$ otherwise. If $N_i \neq \emptyset$ then $\sum_{j \in N_i}p_{ij}=1$. 
To properly define our game, we remark that if $N_i=\emptyset$ then
$\alpha_i=1,\sum_{j=1}^np_{ij}=0$ and agent $i$ suffers cost $(x_i-s_i)^2$. 
Abusing notation we will sometimes refer to the graph $G$. 
Another parameter of an instance $I=(P,s,\alpha)$ that 
we often use is $\rho=\min_{i \in V}\alpha_i$.


\subsection{Our Results}

We focus on the repeated version of the game in 
Definition~\ref{d:random_payoff_game}. At the beginning 
of round $t$, each agent $i$ selects an opinion 
$x_i(t) \in [0,1]$. Then she randomly meets 
one of her neighbors $W_i^t$ ($\Prob{W_i^t=j}=p_{ij}$)
and suffers disagreement cost
\[(1-\alpha_i)(x_i(t)-x_{W_i^t}(t))^2+\alpha_i(x_i(t)-s_i)^2 \] 
We are interested in simple and natural update rules
that the agents can adopt such that the resulting opinion
vector $x(t) \in [0,1]^n$ converges to $x^*$.

In Section~\ref{s:fictitious_convergence},
we study the convergence properties of $x(t)$ if all agents
update their opinion as follows:
\begin{equation}\label{eq:fictitious_play}
  x_i(t) =
  \argmin_{x \in [0,1]}
  \sum_{\tau=0}^{t-1}(1-\alpha_i)(x-x_{W_i^\tau}(\tau))^2+\alpha_i(x-s_i)^2
\end{equation}
%
The above update rule is a simple and
reasonable. Since each agent $i$ must select $x_i(t)$ 
at the beginning of the round, before knowing 
which of her neighbors she will meet and 
what opinion her neighbor will have, 
update~\ref{eq:fictitious_play} says
\enquote{\emph{play the best according to what you have observed}}.
According to this principle Brown proposed \emph{fictitious play} 
\cite{Bro51}, which is one of the most intuitive and simple 
models of playing in finite games. Abusing terminology 
we refer to (\ref{eq:fictitious_play}) as fictitious play. 
We show that in our infinite strategy game, if all agents
adopt fictitious play, the resulting opinion vector $x(t)$ converges to
$x^*$ with the following rate.
%
\begin{reptheorem}{t:fictitious_convergence}
  Let $I = (P,s, \alpha)$ be an instance of the opinion formation
  game of Definition~\ref{d:random_payoff_game} with equilibrium
  $x^* \in [0,1]^n$.  The opinion vector $x(t)\in[0,1]^n$ produced by
  update rule~\ref{eq:fictitious_play} after $t$ rounds satisfies
  \[
    \Expnew{}{\norm{\infty}{x(t) - x^*}} \leq
    C \sqrt{\log n}\frac{(\log t)^{2}}{t^{\min(1/2,\rho)}},
  \]
  where $\rho = \min_{i \in V} a_i$ and $C$ is a universal constant.
\end{reptheorem}

The update rule (\ref{eq:fictitious_play}) guarantees convergence
while vastly reducing the information exchange between the agents
at each round. In (\ref{eq:fictitious_play}) each agent $i$ learns the opinion of only one agent
at each round whereas in the classical FJ-model (\ref{eq:FJ_model}), agent $i$ must
learn the opinions of all her neighbors. In terms of
total communication needed to get within distance $\eps$ of the
equilibrium $x^*$, the update rule (\ref{eq:fictitious_play}) needs
$O(n \log n)$ communication while (\ref{eq:FJ_model}) needs
$O(|E|)$. Of course for this difference to be significant we need
each agent to have at least $O(\log n)$ friends. A large social
network like Facebook has approximately $2$ billion users and each user
has usually more that $100$ friends which is far more than $\log(2\ 10^9)$.

In Section~\ref{s:fictitious_noregret} we argue that, 
apart from its simplicity, fictitious play has no-regret
and therefore is a \emph{rational game play} for selfish agents.
%At each round $t$ each agent $i$, selects an opinion $x_i(t) \in [0,1]$
%and suffers a cost $(1-\alpha_i)(x_i(t)-x_{W_i^t}(t))^2 + \alpha_i(x_i(t)-s_i)^2$.
Since each agent $i$ selfishly wants to minimizing her individual cost,
it is natural to assume that she selects $x_i(t)$ according to
an \emph{no-regret algorithm} for the \emph{online convex optimization problem}
where the adversary chooses a function $f_t(x)=(1-\alpha_i)(x-b_t)^2 + \alpha_i(x-s_i)^2$
at each round $t$. In Theorem~\ref{t:fictitious_noregret} 
we prove that fictitious play is a no-regret algorithm 
for the above OCO problem. We remark that, in general,
fictitious play does not guarantee no-regret if the adversary can pick
functions from a larger class (see e.g. chapter 5 in \cite{Haz16}).

\begin{reptheorem}{t:fictitious_noregret}
  Consider the function $f:[0,1]^2 \mapsto [0,1]$ with
  $f(x,b) = (1-\alpha)(x-b)^2 + \alpha(x-s)^2$ for some
  constants $s,\alpha \in [0,1]$.
  Let $\{b_t\}_{t=1}^\infty$ be an arbitrary sequence with
  $b_t \in [0,1]$. If $x_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}f(x_,b_\tau)$
  then for all $t$,
  \[
    \sum_{\tau=0}^{t}f(x_\tau,b_\tau) \leq
    \min_{x \in [0,1]}\sum_{\tau=0}^tf(x,b_\tau) + \bigOh{\log t}
  \]
\end{reptheorem}

Even though the update rule (\ref{eq:fictitious_play}) has the above
desired properties, the convergence rate of the produced dynamics is
outperformed by the convergence rate of the classical FJ-model. For
a fixed instance $I=(P,s,\alpha)$, fictitious play converges with rate
$\widetilde{O}(1/t^{\text{min}(\rho,1/2)})$ while FJ-model
converges with rate $O(e^{-\rho t})$ \cite{GS14}.
As a result the following question arises
\begin{question}
  Can the agents adopt other no-regret algorithms such that the resulting
  dynamics $x(t)$ converges exponential fast to $x^*$?
\end{question}

In Section~\ref{s:lower_bound} we answer this question in the negative.
The reason that fictitious play converges slowly is that
update rule (\ref{eq:fictitious_play})
only depends on the opinions of the agents that agent $i$ meets,
$\alpha_i$, and $s_i$. This is also true for any no-regret algorithm
that $i$ uses to select $x_i(t)$ (see Section~\ref{s:lower_bound}).
We call such update rules \enquote{\emph{opinion dependent}}.
In Theorem~\ref{t:lower_bound} we
show that for any opinion dependent update rule there exists an instance
$I = (P,s,\alpha)$ where $\poly(1/\eps)$ rounds are required to
achieve convergence within error $\eps$.
\begin{reptheorem}{t:lower_bound}
  Let $A$ be an \emph{opinion dependent} update rule, which all
  agents use to update their opinions.
  For any $c>0$ there exists an instance $I=(P,s,a)$ such that
  \[
    \Exp{\norm{\infty}{x_A(t) - x^*}} = \Omega(1/t^{1+c}),
  \]
  where $x_A(t)$ denotes the opinion vector produced by $A$
  for the instance $I=(P,s,\alpha)$.
\end{reptheorem}
%
To prove Theorem~\ref{t:lower_bound}, we show that opinion dependent rules with
\enquote{small round complexity}, imply the existence
of estimators for Bernoulli distributions with
\enquote{small} sample complexity. Then with a simple argument
presented in Lemma~\ref{l:estimation_lower_bound},
we show that such estimators cannot exist.
In Section~\ref{s:lower_bound} we also
briefly discuss two well-known sample complexity lower bounds
from the statistics literature and explain why they do not work in our case.

In Section~\ref{s:cc_convergence}, we present a simple update rule that
is not opinion dependent and  achieves error rate $e^{-\bigOh{\sqrt{t}}}$.
This update rule is a function of the opinions and the indices of the agents
that $i$ met, $\alpha_i,s_i$ and the $i$-th row the matrix $P$.
We mention that the lower bound presented in Theorem~\ref{t:lower_bound}
applies for \enquote{opinion dependent rules} that also depend on the
agents' indices that $i$ met.  Therefore, the dependency on the row $P_i$ is
inevitable in order to obtain exponential convergence.
Although, the assumption that the agents are aware of the influence matrix
$P$ is up to discussion, this update rule reveals that the slow convergence of
\emph{opinion dependent} update rules is not due to the reduced information exchange
(learning the opinion of only one agent), but due to the fact
that the agents are \enquote{oblivious} to the influence matrix $P$ of the
game and they learn it during the game play.

% As is it also mentioned in \cite{CHM17} this stronger notion
% of convergence cannot be derived from the Coarse Correlated convergence
% results stated above.
