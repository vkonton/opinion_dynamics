\section{Faster Update Rules}\label{s:cc_convergence}

We already discussed that the reason that opinion dependent dynamics suffer slow
convergence is that the update rule depends only on the expressed opinions.
In this section we provide an update rule showing that information about the
graph $G$ combined with agents that do not act selfishly, can restore the
exponential convergence rate.
Our update rule, depends not only on the expressed opinions of the
agents but also on their indices and matrix $P$.
Having this knowledge, one could try to come up with an
update rule resembling the original update rule of the FJ model.
In update rule \ref{alg:cc_upper}, each agent could store the
\emph{most recent} opinions of the random neighbors that she meets in an array
and then update her
opinion according to their weighted sum (each agent knows row $i$ of $P$).
The problem with this approach is that the opinions of the neighbors
that she keeps in her array are \emph{outdated}, i.e. the opinion of
neighbor of agent $i$ is different than what she expressed in their
last meeting.  The good news are that as long as this outdatedness
is bounded we can still achieve exponential convergence to the
equilibrium.  By bounded outdatedness we mean that there exists a
number of rounds $B$ such that all agents have met all their neighbors
at least once from $t$ to $t+B$.
\begin{remark}
It is necessary to know the matrix $P$ in order for this
update rule to work. We first observe that the lower bound of
Section~\ref{s:lower_bound} also holds in case the algorithm learns the index
of the chosen neighbor. The reason is that the reduction involves only two neighbors
with different opinions, so they are distinguishable.
Therefore, if we tried to learn $P$ by observing the frequencies
of the indices of the neighbors and run update rule \ref{alg:cc_upper}
with the empirical frequencies instead of the $p_{ij}$, our lower bound
ensures that the rate of convergence would not be $O(1/t^{1+c})$ for any $c>0$.
Intuitively, if we know $P$ then the algorithm converges exponentially,
since the slow part of the process is learning the probabilities $p_{ij}$
precisely.
\end{remark}
\begin{algorithm}
  \caption{Asynchronous Update Rule}
  \label{alg:cc_upper}
  \begin{algorithmic}[1]
    \STATE Initially $x_i(0) = s_i$ for all agent $i$.
    %\STATE Let $d_i$ be the number of non-negative entries of row $i$ of $P$.
    \STATE Each agent $i$ keeps an array $M_i$ of length $d_i$.
    \STATE At round $t\geq 1$ each agent $i$:
    \bindent
    \STATE $x_i(t) = (1-\alpha_i)\sum_{j=1}^{d_i} p_{ij} M_i[j] + \alpha_is_i$
    \STATE Meets neighbor $W_i^t$ and learns the opinion $x_{W_i^t}(t)$.
    \STATE $M_i[W_i^t] \gets x_{W_i^t}(t-1)$.
    \eindent
  \end{algorithmic}
\end{algorithm}

In \cite{BT97}, they show a convergence rate guarantee for
\ref{alg:cc_upper} assuming that there exists a such a window $B$.
In the  following we briefly summarize their result.  For completeness we
give here a prove tailored for our purposes.
Using a simple induction we get that bounded outdatedness
preserves the exponential convergence.
\begin{lemma}\label{l:outdatedness_induction}
  Let $\rho = \min_i a_i$, and $\pi_{ij}(t) \in \N$ be the most recent round
  before round $t$, that agent $i$ met agent $j$.
  If for all $t\geq B$, $t-B \leq \pi_{ij}(t) \leq t-1$ then, for
  all $t \geq k B$,
  \(\norm{\infty}{x(t) - x^*} \leq (1-\rho)^k\).
\end{lemma}
\begin{proof}
  To prove our claim we use induction on $k$. For the induction base $k=1$,
  \begin{align*}
    |x_i(t) - x_i^*|
    &=
    |(1-\alpha_i)\sum_{j \in N_i}p_{ij}(x_j(\pi_{ij}(t)) -x_j^*)|\\
    &\leq
    (1-\alpha_i)\sum_{j \in N_i}p_{ij}|(x_j(\pi_{ij}(t))-x_j^*)|\leq (1-\rho)
  \end{align*}
  From the induction hypothesis we have for $\pi_{ij}(t) \geq (k-1)B$,
  that $|x_j(\pi_{ij}(t))-x_j^*| \leq (1-\rho)^{k-1}$.
  For $k\geq 2$, we again have that
  $|x_i(t) - x_i^*|\leq (1-\rho)\sum_{j \in N_i}p_{ij}|(x_j(\pi_{ij}(t))-x_j^*)|$.
  Since $t-B \leq \pi_{ij}(t)$ and $t\geq kB$, we have
  that $\pi_{ij}(t) \geq (k-1)B$ and the induction hypothesis applies.
\end{proof}

In our randomized setting there does not exist fixed length window is
not true but we can easily adapt this to hold with high probability.
To do this observe that agent $i$ simply needs to wait to meet the neighbor
$j$ with the smallest weight $p_{ij}$. Therefore, after
$\log(1/\delta)/\min_{j} p_{ij}$ rounds we have that with probability at least
$1-\delta$ agent $i$ met all her neighbors at least once.
Since we want this to be true for all agents
we shall roughly take $B = 1/\min_{p_{ij} > 0} {p_{ij}}$.
In Section~\ref{app:s:cc_convergence} of the Appendix we give the detailed
argument that leads to the Lemma~\ref{l:cc_convergence},
showing that the convergence rate of update rule \ref{alg:cc_upper} is exponential.

\begin{lemma}\label{l:cc_convergence}
  Let $x(t)$ be the dynamics corresponding
  to update rule \ref{alg:cc_upper}.
  We have
  \[
    \Exp{\norm{\infty}{x(t) - x^*}}
    \leq
    2\exp \lp(- \rho (1-\rho) \min_{ij} p_{ij} \frac{\sqrt{t}}{4\ln(nt)} \rp)
  \]
\end{lemma}
