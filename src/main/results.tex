\section{Our Results and Techniques}

For simplicity we adopt the following notation for
an instance of the game of Definition~\ref{d:random_game}.
%
\begin{definition}\label{d:instance}
  We denote an instance of the opinion formation game of Definition~\ref{d:random_game}
  as $(P,s,\alpha)$, where
    $P$ is a $n \times n$  matrix with non-negative elements $p_{ij}$,
      with $p_{ii}=0$ and $\sum_{j=1}^n p_{ij}$ is either $0$ or $1$,
    $s \in [0,1]^n$ is the internal opinion vector,
    $\alpha \in (0,1]^n$ the self confidence vector.
\end{definition}
%
Following the notation introduced earlier we have that
$p_{ij} = w_{ij}/(\sum_{j \in N_i}w_{ij}+w_{ii})$ if $j \in N_i$ and $0$ otherwise.
Obviously, if an agent $i$ does not have outgoing edges then $N_i = \emptyset$ and
$p_{ij} = 0$ for all $j$. Therefore, $\sum_{j=1}^n p_{ij}$ is $0$ if $N_i= \emptyset$
and $1$ otherwise.
Moreover, $\alpha_i=w_{ii}/(\sum_{j \in N_i}w_{ij}+w_{ii})>0$ since $w_{ii}>0$.
Note that a given instance $I=(P,s,\alpha)$ also defines an instance
of the FJ model and we denote as $x^* \in [0,1]^n$
its equilibrium point, which coincides with the Nash equilibrium
of the game of Definition~\ref{d:random_game}.
Another parameter of an instance $I=(P,s,\alpha)$ that we often use is
$\rho=\min_{i \in V}\alpha_i$. 



Throughout the paper we study \emph{dynamics}
in the game of Definition~\ref{d:random_game}.
We denote as $W_i^t$ the neighbor that agent $i$ met
at round $t$, which is a random variable whose
probability distribution is determined by the
instance $I=(P,s,\alpha)$ of the game, $\Prob{W_i^t=j}=p_{ij}$.

% If $N_i \neq \emptyset$ then $\sum_{j \in N_i}p_{ij}=1$.
% To properly define our game, we remark that if $N_i=\emptyset$ then
% $\alpha_i=1,\sum_{j=1}^n p_{ij}=0$ and agent $i$ suffers cost $(x_i-s_i)^2$.
%Abusing notation we will sometimes refer to the graph $G$.
%Another parameter of an instance $I=(P,s,\alpha)$ that we often use is
%$\rho=\min_{i \in V}\alpha_i$. Throughout the paper we consider that the agents repeatedly
%play the one shot game of Definition~\ref{d:random_game}.
%We denote as $W_i^t$ the neighbor that agent $i$ met
%at round $t$, which is a random variable whose
%probability distribution is determined by the
%instance $I=(P,s,\alpha)$ of the game, $\Prob{W_i^t}=p_{ij}$.


In Section~\ref{s:fictitious_convergence}, we study the convergence properties
of the opinion vector $x(t)$ when all agents update their opinions
according to the \enquote{Follow the Leader} principle.
Since each agent $i$ must select $x_i(t)$, before knowing which of her neighbors she
will meet and what opinion her neighbor will express, their update rule
says \enquote{\emph{play the best according to what you have observed}}.
For a given instance $(P,s,a)$ of the game %of Definition~\ref{d:random_game}
the Follow the Leader dynamics $x(t)$ is defined in Dynamics~\ref{alg:FTL_dynamics} and
Theorem \ref{t:fictitious_convergence} shows its convergence rate to $x^*$.
%
\begin{algorithm}
  \caption{Follow the Leader dynamics}
  \label{alg:FTL_dynamics}
  \begin{algorithmic}[1]
    \STATE Initially $x_i(0) = s_i$ for all agents $i$.
    \STATE At round $t\geq 0$ each agent $i$:
    \bindent
    \STATE Meets neighbor with index $W_i^t$, $\Prob{W_i^t=j}=p_{ij}$.
    \STATE Suffers cost \((1-\alpha_i) (x_i(t) - x_{W_i^t}(t))^2 + a_i (x_i(t) - s_i)^2\)
    and learns the opinion $x_{W_i^t}(t)$.
  \STATE Updates her opinion
    \inlineequation[eq:fictitious_play]{
      x_i(t+1) =
      \argmin_{x \in [0,1]}
      \sum_{\tau=0}^{t}(1-\alpha_i)(x-x_{W_i^\tau}(\tau))^2+\alpha_i(x-s_i)^2
    }
    \eindent
  \end{algorithmic}
\end{algorithm}
%

% According to this principle Brown proposed \emph{fictitious play}
% \cite{Bro51}, which is one of the most intuitive and simple
% models of playing in finite games.
% Abusing terminology we refer to (\ref{eq:fictitious_play}) as fictitious play.
% We show that in our infinite strategy game, if all agents
% adopt fictitious play, the resulting opinion vector
%
\begin{reptheorem}{t:fictitious_convergence}
  Let $I = (P,s, \alpha)$ be an instance of the opinion formation
  game of Definition~\ref{d:random_game} with equilibrium
  $x^* \in [0,1]^n$.  The opinion vector $x(t)\in[0,1]^n$ produced by
  update rule~(\ref{eq:fictitious_play}) after $t$ rounds satisfies
  \[
    \Expnew{}{\norm{\infty}{x(t) - x^*}} \leq
    C \sqrt{\log n}\frac{(\log t)^{3/2}}{t^{\min(1/2,\rho)}},
  \]
  where $\rho = \min_{i \in V} a_i$ and $C$ is a universal constant.
\end{reptheorem}


In Section~\ref{s:fictitious_noregret} we argue that,
apart from its simplicity, update rule (\ref{eq:fictitious_play}) ensures
no regret for the selfish agents, and therefore is a \emph{rational game play}.
%At each round $t$ each agent $i$, selects an opinion $x_i(t) \in [0,1]$
%and suffers a cost $(1-\alpha_i)(x_i(t)-x_{W_i^t}(t))^2 + \alpha_i(x_i(t)-s_i)^2$.
Since each agent $i$ selfishly wants to minimizing the disagreement cost
that she experiences, it is natural to assume that she selects $x_i(t)$ according to
an \emph{no regret algorithm} for the \emph{online convex optimization problem}
where the adversary chooses a function $f_t(x)=(1-\alpha_i)(x-b_t)^2 + \alpha_i(x-s_i)^2$
at each round $t$. In Theorem~\ref{t:fictitious_noregret}
we prove that \enquote{Follow the Leader} is a no regret algorithm
for the above OCO problem. We remark that this does not hold,
if the adversary can pick functions from a different class
(see e.g. chapter 5 in \cite{Haz16}).

\begin{reptheorem}{t:fictitious_noregret}
  Consider the function $f:[0,1]^2 \mapsto [0,1]$ with
  $f(x,b) = (1-\alpha)(x-b)^2 + \alpha(x-s)^2$ for some
  constants $s,\alpha \in [0,1]$.
  Let $\{b_t\}_{t=1}^\infty$ be an arbitrary sequence with
  $b_t \in [0,1]$. If $x_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}f(x_,b_\tau)$
  then for all $t$,
  \(
    \sum_{\tau=0}^{t}f(x_\tau,b_\tau) \leq
    \min_{x \in [0,1]}\sum_{\tau=0}^tf(x,b_\tau) + \bigOh{\log t}.
  \)
\end{reptheorem}

On the positive side, the FTL dynamics is a simple and natural
stochastic process that ensures convergence to $x^*$ while
vastly reducing the information exchange between the agents
per round; it only requires each agent to learn the opinion
of just one neighbor. In terms of total communication exchange
needed to get within distance $\eps$ of the equilibrium $x^*$,
the FTL dynamics requires $\widetilde{O}(n)$ communication
exchange while the FJ model needs $\bigOh{|E|}$.
%Of course for this difference
%to be significant we need each agent to have at least
%$O(\log n)$ friend which not unusual in today's large social networks.
% A large social network like Facebook has approximately $2$ billion users
% and each user has usually more that $100$ friends which is far more
% than $\log(2\ 10^9)$.
On the negative side, its convergence rate
is outperformed by the rate of FJ model.
For a fixed instance $I=(P,s,\alpha)$, the
FTL dynamics converges with rate
$\widetilde{O}(1/t^{\text{min}(\rho,1/2)})$ while FJ model
converges with rate $O(e^{-\rho t})$ \cite{GS14}.
As a result the following question arises.
\begin{question}
  Can the agents adopt other no regret algorithms such that the resulting
  dynamics $x(t)$ converges exponentially fast to $x^*$?
\end{question}

In Section~\ref{s:lower_bound} we answer this question in the negative.
The reason that FTL dynamics converges slowly is that
update rule (\ref{eq:fictitious_play})
only depends on the opinions of the neighbors that agent $i$ meets,
$\alpha_i$, and $s_i$. This is also true for any update rule that
ensures no regret to the agents (see Section~\ref{s:lower_bound}).
As already mentioned, we call such update rules \enquote{\emph{opinion dependent}}
and the produced dynamics \emph{opinion dependent dynamics}.

\begin{definition}[opinion dependent update rule]\label{d:opinion_dependent_dynamics}
An opinion dependent update rule $A$ is a sequence of
functions $(A_t)_{t=0}^\infty$ where
$A_t: [0,1]^{t+2}\mapsto [0,1]$.
\end{definition}

\begin{definition}[opinion dependent dynamics]\label{d:opinion_dependent_dynamics}
Let an opinion dependent update rule $A$. For a given instance $I=(P,s,\alpha)$
the rule $A$ produces an opinion depedent dynamics $x_A(t)$ defined as follows:
\begin{itemize}
 \item Initially each agent $i$ selects her opinion $x_i^A(0)=A_0(s_i,\alpha_i)$
 \item At round $t\geq 1$, each agent $i$ selects her opinion
   \(x_i^A(t)=A_t(x_{W_i^0}(0),\dots,x_{W_i^{t-1}}(t-1),\alpha_i,s_i)\),
where $W_i^t$ is the neighbors that $i$ meets at round $t$.
\end{itemize}
\end{definition}

Note that FTL dynamics is an opinion dependent dynamics
since update rule~(\ref{eq:fictitious_play}) can
be written equivalently, $x_i(0)=s_i$ and $x_i(t)=(1-\alpha_i)\sum_{\tau=0}^{t-1}x_{W_i^\tau}(\tau)/t + \alpha_i s_i$.
In Theorem~\ref{t:lower_bound} we
show that for any opinion dependent dynamics there exists an instance
$I = (P,s,\alpha)$ where $\Omega(1/\eps)$ rounds are required to
achieve convergence within error $\eps$.
\begin{reptheorem}{t:lower_bound}
  Let $A$ be an \emph{opinion dependent} update rule, which all
  agents use to update their opinions.
  For any $c>0$ there exists an instance $I=(P,s,a)$ such that
  \(
    \Exp{\norm{\infty}{x_A(t) - x^*}} = \Omega(1/t^{1+c}),
  \)
  where $x_A(t)$ denotes the opinion vector produced by $A$
  for the instance $I=(P,s,\alpha)$.
\end{reptheorem}
%
To prove Theorem~\ref{t:lower_bound}, we show that opinion dependent rules
that need a small number of rounds to converge, imply the existence
of estimators for Bernoulli distributions with
small sample complexity. Then with a simple argument
presented in Lemma~\ref{l:estimation_lower_bound},
we show that such estimators cannot exist.
In Section~\ref{s:lower_bound} we also
briefly discuss two well-known sample complexity lower bounds
from the statistics literature and explain why they do not work in our case.

In Section~\ref{s:cc_convergence}, we present a simple update rule that
is not opinion dependent and  achieves error rate $e^{-\bigOh{\sqrt{t}}}$.
This update rule is a function of the opinions and the indices of the agents
that $i$ met, $\alpha_i,s_i$ and the $i$-th row the matrix $P$.
We mention that the lower bound presented in Theorem~\ref{t:lower_bound}
applies for \enquote{opinion dependent rules} that also depend on the
agents' indices that $i$ met.  Therefore, the dependency on the row $P_i$ is
inevitable in order to obtain exponential convergence.
Although, the assumption that the agents are aware of the influence matrix
$P$ is up to discussion, this update rule reveals that the slow convergence of
\emph{opinion dependent} update rules is not due to the reduced information exchange
(learning the opinion of only one agent), but due to the fact
that the agents are \enquote{oblivious} to the influence matrix $P$ of the
game and they learn it during the game play.
