\section{Our Results and Techniques}
%In \cite{BKO11} the following \emph{opinion formation game} was introduced.
%A weighted directed graph $G(V,E,w)$ is assumed where and the vertices
%stand for the $n$ agents ($|V| = n$) and the acres for the social
%influence among them. Each agent $i \in V$ possess an
%\emph{internal opinion} $s_i \in [0,1]$ and a \emph{self confidence coefficient}
%$w_i>0$. The strategy of each agent $i$ is the opinion $x_i\in [0,1]$ that
%she publicly expresses incurring her cost
%%
%\begin{equation}\label{eq:BKO_cost}
%  C_i(x_i,x_{-i}) = \sum_{j \in N_i}w_{ij}(x_i-x_j)^2 + w_i(x_i -s_i)^2
%\end{equation}
%%
%where $N_i$ denotes $i$'s \emph{neighbors} and $w_{ij}$ stands for
%the social influence $j$ imposes on $i$. In \cite{BKO11} they proved that
%the above game always admits a \emph{Pure Nash Equilibrium} (PNE) $x^* \in [0,1]^n$
%and studied its efficiency with respect to the total disagreement cost.
%They proved that the \emph{Price of Anarchy}
%is less than $9/8$ in case $G$ is bidirectional and $w_{ij}=w_{ji}$.

%In the repeated version of the game defined in (\ref{eq:BKO_cost}),
%at each round $t$ each agent $i$ selects an opinion $x_i(t)$ and then suffers
%cost $C_i(x_i(t),x_{-i}(t))$. If each agent updates her opinion to be the
%\emph{best response} of $x(t-1)$,
%%
%\begin{equation}\label{eq:FJ_model}
%  x_i(t) =
%  \argmin_{x \in [0,1]}C_i(x,x_{-i}(t-1))=
%  \frac{\sum_{j \in N_i}w_{ij}x_j(t-1) + w_is_i}{\sum_{j \in N_i}w_{ij} + w_i}
%\end{equation}
%%
%we obtain the Friedkin-Johsen model (FJ-model), which is one of
%%the most influential models in opinion dynamics.
%The convergence properties of the FJ-model have been extensively studied.
%In \cite{GS14} they proved that $x(t)$ always converges
%to the PNE $x^*$ and provided bounds for the convergence time for various
%graph topologies. As a result, the
%above \emph{opinion formation game} has some nice algorithmic properties: It
%always admits a unique equilibrium point $x^*$ and there
%exists a simple but most importantly rational update rule for selfish agents
%that leads the overall system to equilibrium.

Our work is motivated by the fact that the definition of the cost $C_i(x_i,x_{-i})$
in~(\ref{eq:BKO_cost}) implies that agent $i$ meets with all of
her neighbors.  This is more clear in the update rule (\ref{eq:FJ_model}).
Each agent, in order to compute her best response, has to learn the opinion of
all her neighbors.
The latter seems quite unnatural in today's huge social networks
(e.g. Facebook, Twitter etc.), in which each user may have
several hundreds of friends. Thus, it is far more reasonable to assume
that each day an agent meets a small subset of her acquaintances and
suffers a cost based on how much she disagrees with them. To capture the above thoughts,
we use a variant of the opinion formation game in which the
disagreement cost of each agent $i$ is a random variable depending
on the random meetings of $i$.
We first present our variant of the one shot game defined in \cite{BKO11} in which
we assume that only the opinion of one random neighbor is known at each round.
%
\begin{definition}\label{d:random_game}
  For a given opinion vector $x \in [0,1]^n$, the disagreement cost of agent $i$
  is the random variable $C_i(x_i,x_{-i})$ defined as follows:
  \begin{itemize}
    \item $i$ meets one of her neighbors $j$ with probability
      $p_{ij}= w_{ij}/\sum_{j\in N_i}w_{ij}$
    \item suffers cost $C_i(x_i , x_{-i}) = (1-a_i)(x_i-x_j)^2 + a_i(x_i-s_i)^2$,
      where $\alpha_i = w_i/(\sum_{j\in N_i}w_{ij}+w_i)$
  \end{itemize}
\end{definition}
%
The main difference of the original opinion formation game with our variant is that
in the first case an opinion vector $x\in [0,1]^n$ defines \emph{deterministically}
the cost $C_i(x_i,x_{-i})$ of each agent $i$, whereas in the second
case it defines (according to Definition~\ref{d:random_game})
a probability distribution on the cost $C_i(x_i,x_{-i})$ that $i$ suffers.
The reason is that the random payoff setting is more suitable to model realistic
situations in which randomness naturally occurs because of incomplete information.

The cost $C_i(x_i,x_{-i})$ in (\ref{eq:BKO_cost}) can be written equivalently
\begin{equation}\label{eq:BKO11_cost_equivalent}
  C_i(x_i,x_{-i}) =
  W_i\lp( (1-\alpha_i)\sum_{j \in N_i} p_{ij}(x_i-x_j)^2
  + \alpha_i(x_i-s_i)^2\rp)
\end{equation}
where $W_i=\sum_{j\in N_i}w_{ij} + w_i$ is a positive constant independent
of the opinion vector $x\in [0,1]^n$.
Thus, the random cost in Definition~\ref{d:random_game} has a natural
interpretation: the coefficient $\alpha_i$ measures the reluctance of agent
$i$ to adopt an opinion other than $s_i$, while $p_{ij}$ can be seen as the
\emph{real} influence that $j$ poses on $i$.
In Definition~\ref{d:random_game}, $p_{ij}$ is the frequency that
$i$ meets $j$, meaning that the influence that $j$ poses on $i$ is just a measure
on how often they meet. The latter aligns with the common belief
that we are influenced more by those we interact more often.
Equation (\ref{eq:BKO11_cost_equivalent}) also helps to establish the existence
of PNE for the game of Definition~\ref{d:random_game}.
In our case, the notion of PNE extends with respect to the expected cost of each
agent. Namely, $x^* \in [0,1]$ is a PNE if and only if
$\Exp{C(x_i^*, x_{-i}^*)} \leq \Exp{C(x_i, x_{-i}^*)}$
for each agent $i$.
Since
$\Expnew{}{C_i(x_i,x_{-i})}=
(1-\alpha_i) \sum_{j \in N_i}p_{ij}(x_i-x_j)^2 + \alpha_i(x_i-s_i)^2 $,
it follows from (\ref{eq:BKO11_cost_equivalent}) that the
opinion formation game with random payoffs has the same equilibrium $x^*$ as
the original opinion formation game.

Instead of denoting an instance of the opinion formation game of
Definition~\ref{d:random_game} using a graph $G$ and weights $w_{ij}$, $w_i$
we adopt the following more convenient notation.
%
\begin{definition}\label{d:instance}
  We denote an instance of the opinion formation game of Definition~\ref{d:random_game}
  as $(P,s,\alpha)$.
  \begin{itemize}
    \item $P$ is a $n \times n$  matrix with non-negative elements $p_{ij}$,
      with $p_{ii}=0$ and $\sum_{j=1}^n p_{ij}$ is either $0$ or $1$.
    \item $s \in [0,1]^n$ is the internal opinion vector.
    \item $\alpha \in [0,1]^n$ the self confidence coefficient vector.
  \end{itemize}
\end{definition}
%
We use the matrix $P$ to simplify notation, $p_{ij} = w_{ij}/(\sum_{j \in N_i}w_{ij}+w_i)$
if $j \in N_i$ and $0$ otherwise. If $N_i \neq \emptyset$ then $\sum_{j \in N_i}p_{ij}=1$.
To properly define our game, we remark that if $N_i=\emptyset$ then
$\alpha_i=1,\sum_{j=1}^np_{ij}=0$ and agent $i$ suffers cost $(x_i-s_i)^2$.
Abusing notation we will sometimes refer to the graph $G$.
Another parameter of an instance $I=(P,s,\alpha)$ that we often use is
$\rho=\min_{i \in V}\alpha_i$.

In Section~\ref{s:fictitious_convergence}, we study the convergence properties
of $x(t)$ if it is generated according to a \enquote{follow the leader}
style update rule \ref{alg:fictitious_dynamics}.
\begin{algorithm}
  \caption{Dynamics}
  \label{alg:fictitious_dynamics}
  \begin{algorithmic}[1]
    \STATE Initially $x_i(0) = s_i$ for all agents $i$.
    \STATE At round $t\geq 1$ each agent $i$:
    \bindent
    \STATE Meets neighbor with index $W_i^t$.
  \STATE Suffers cost \( C_i(x_i,x_{-i}) = (1-\alpha_i) (x_i(t-1) - x_{W_i^t}(t-1))^2 + a_i (x_i(t-1) - s_i)^2\).
  \item Updates her opinion
    \begin{equation}\label{eq:fictitious_play}
      x_i(t) =
      \argmin_{x \in [0,1]}
      \sum_{\tau=0}^{t-1}(1-\alpha_i)(x-x_{W_i^\tau}(\tau))^2+\alpha_i(x-s_i)^2
    \end{equation}
    \eindent
  \end{algorithmic}
\end{algorithm}
%
Since each agent $i$ must select $x_i(t)$ at the beginning of the round, before knowing
which of her neighbors she will meet and what opinion her neighbor will have,
update~\ref{eq:fictitious_play} says
\enquote{\emph{play the best according to what you have observed}}.
According to this principle Brown proposed \emph{fictitious play}
\cite{Bro51}, which is one of the most intuitive and simple
models of playing in finite games. Abusing terminology
we refer to (\ref{eq:fictitious_play}) as fictitious play.
We show that in our infinite strategy game, if all agents
adopt fictitious play, the resulting opinion vector $x(t)$ converges to
$x^*$ with the following rate.
%
\begin{reptheorem}{t:fictitious_convergence}
  Let $I = (P,s, \alpha)$ be an instance of the opinion formation
  game of Definition~\ref{d:random_payoff_game} with equilibrium
  $x^* \in [0,1]^n$.  The opinion vector $x(t)\in[0,1]^n$ produced by
  update rule~\ref{eq:fictitious_play} after $t$ rounds satisfies
  \[
    \Expnew{}{\norm{\infty}{x(t) - x^*}} \leq
    C \sqrt{\log n}\frac{(\log t)^{2}}{t^{\min(1/2,\rho)}},
  \]
  where $\rho = \min_{i \in V} a_i$ and $C$ is a universal constant.
\end{reptheorem}

The update rule (\ref{eq:fictitious_play}) guarantees convergence
while vastly reducing the information exchange between the agents
at each round. In (\ref{eq:fictitious_play}) each agent $i$ learns the opinion of only one agent
at each round whereas in the classical FJ-model (\ref{eq:FJ_model}), agent $i$ must
learn the opinions of all her neighbors. In terms of
total communication needed to get within distance $\eps$ of the
equilibrium $x^*$, the update rule (\ref{eq:fictitious_play}) needs
$O(n \log n)$ communication while (\ref{eq:FJ_model}) needs
$O(|E|)$. Of course for this difference to be significant we need
each agent to have at least $O(\log n)$ friends. A large social
network like Facebook has approximately $2$ billion users and each user
has usually more that $100$ friends which is far more than $\log(2\ 10^9)$.

In Section~\ref{s:fictitious_noregret} we argue that,
apart from its simplicity, update rule \ref{eq:fictitious_play} has
no-regret and therefore is a \emph{rational game play} for selfish agents.
%At each round $t$ each agent $i$, selects an opinion $x_i(t) \in [0,1]$
%and suffers a cost $(1-\alpha_i)(x_i(t)-x_{W_i^t}(t))^2 + \alpha_i(x_i(t)-s_i)^2$.
Since each agent $i$ selfishly wants to minimizing her individual cost,
it is natural to assume that she selects $x_i(t)$ according to
an \emph{no-regret algorithm} for the \emph{online convex optimization problem}
where the adversary chooses a function $f_t(x)=(1-\alpha_i)(x-b_t)^2 + \alpha_i(x-s_i)^2$
at each round $t$. In Theorem~\ref{t:fictitious_noregret}
we prove that fictitious play is a no-regret algorithm
for the above OCO problem. We remark that, in general,
fictitious play does not guarantee no-regret if the adversary can pick
functions from a larger class (see e.g. chapter 5 in \cite{Haz16}).

\begin{reptheorem}{t:fictitious_noregret}
  Consider the function $f:[0,1]^2 \mapsto [0,1]$ with
  $f(x,b) = (1-\alpha)(x-b)^2 + \alpha(x-s)^2$ for some
  constants $s,\alpha \in [0,1]$.
  Let $\{b_t\}_{t=1}^\infty$ be an arbitrary sequence with
  $b_t \in [0,1]$. If $x_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}f(x_,b_\tau)$
  then for all $t$,
  \[
    \sum_{\tau=0}^{t}f(x_\tau,b_\tau) \leq
    \min_{x \in [0,1]}\sum_{\tau=0}^tf(x,b_\tau) + \bigOh{\log t}
  \]
\end{reptheorem}

Even though the update rule (\ref{eq:fictitious_play}) has the above
desired properties, the convergence rate of the produced dynamics is
outperformed by the convergence rate of the classical FJ-model. For
a fixed instance $I=(P,s,\alpha)$, fictitious play converges with rate
$\widetilde{O}(1/t^{\text{min}(\rho,1/2)})$ while FJ-model
converges with rate $O(e^{-\rho t})$ \cite{GS14}.
As a result the following question arises
\begin{question}
  Can the agents adopt other no-regret algorithms such that the resulting
  dynamics $x(t)$ converges exponential fast to $x^*$?
\end{question}

In Section~\ref{s:lower_bound} we answer this question in the negative.
The reason that fictitious play converges slowly is that
update rule (\ref{eq:fictitious_play})
only depends on the opinions of the agents that agent $i$ meets,
$\alpha_i$, and $s_i$. This is also true for any no-regret algorithm
that $i$ uses to select $x_i(t)$ (see Section~\ref{s:lower_bound}).
We call such update rules \enquote{\emph{opinion dependent}}.
In Theorem~\ref{t:lower_bound} we
show that for any opinion dependent update rule there exists an instance
$I = (P,s,\alpha)$ where $\poly(1/\eps)$ rounds are required to
achieve convergence within error $\eps$.
\begin{reptheorem}{t:lower_bound}
  Let $A$ be an \emph{opinion dependent} update rule, which all
  agents use to update their opinions.
  For any $c>0$ there exists an instance $I=(P,s,a)$ such that
  \[
    \Exp{\norm{\infty}{x_A(t) - x^*}} = \Omega(1/t^{1+c}),
  \]
  where $x_A(t)$ denotes the opinion vector produced by $A$
  for the instance $I=(P,s,\alpha)$.
\end{reptheorem}
%
To prove Theorem~\ref{t:lower_bound}, we show that opinion dependent rules with
\enquote{small round complexity}, imply the existence
of estimators for Bernoulli distributions with
\enquote{small} sample complexity. Then with a simple argument
presented in Lemma~\ref{l:estimation_lower_bound},
we show that such estimators cannot exist.
In Section~\ref{s:lower_bound} we also
briefly discuss two well-known sample complexity lower bounds
from the statistics literature and explain why they do not work in our case.

In Section~\ref{s:cc_convergence}, we present a simple update rule that
is not opinion dependent and  achieves error rate $e^{-\bigOh{\sqrt{t}}}$.
This update rule is a function of the opinions and the indices of the agents
that $i$ met, $\alpha_i,s_i$ and the $i$-th row the matrix $P$.
We mention that the lower bound presented in Theorem~\ref{t:lower_bound}
applies for \enquote{opinion dependent rules} that also depend on the
agents' indices that $i$ met.  Therefore, the dependency on the row $P_i$ is
inevitable in order to obtain exponential convergence.
Although, the assumption that the agents are aware of the influence matrix
$P$ is up to discussion, this update rule reveals that the slow convergence of
\emph{opinion dependent} update rules is not due to the reduced information exchange
(learning the opinion of only one agent), but due to the fact
that the agents are \enquote{oblivious} to the influence matrix $P$ of the
game and they learn it during the game play.
