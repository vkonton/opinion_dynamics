\section{Our Results and Techniques}
We now introduce the necessary notions to present our results in detail.
We start with some remarks on our limited information opinion formation game
defined in Definition~\ref{d:random_game}.
In the original opinion formation game an opinion vector $x\in [0,1]^n$ defines
\emph{deterministically} the cost $C_i(x_i,x_{-i})$ of each agent $i$,
whereas in our variant it defines a probability distribution on the cost
$C_i(x_i,x_{-i})$ that $i$ suffers.
The cost $C_i(x_i,x_{-i})$ in (\ref{eq:BKO_cost}) can be written equivalently
\begin{equation}\label{eq:BKO11_cost_equivalent}
  C_i(x_i,x_{-i}) =
  W_i\lp( (1-\alpha_i)\sum_{j \in N_i} p_{ij}(x_i-x_j)^2
  + \alpha_i(x_i-s_i)^2\rp),
\end{equation}
where $W_i=\sum_{j\in N_i}w_{ij} + w_i$ and
$\alpha_i = w_i/(\sum_{j} w_{ij} + w_i)$ are positive constants independent of
the opinion vector $x$. 
The coefficient $\alpha_i$ measures the reluctance of agent
$i$ to adopt an opinion other than $s_i$ and since $\sum_{j \in N_i}p_{ij}=1$,
the parameter $p_{ij}$ can be seen as the \emph{real} influence that $j$ poses on $i$.
In our repeated version of the limited information game of Definition~\ref{d:random_game},
$p_{ij}$ is the frequency that $i$ meets $j$, meaning that the influence that
$j$ poses on $i$ can also be interpreted as a measure
of how often they meet. The latter aligns with the common belief
that we are influenced more by those we interact more often.
Equation (\ref{eq:BKO11_cost_equivalent}) also helps to establish the existence
of NE for the game of Definition~\ref{d:random_game}.
In our case, the notion of NE extends with respect to the expected cost of each
agent. Namely, $x^* \in [0,1]$ is a NE if and only if
$\Exp{C(x_i^*, x_{-i}^*)} \leq \Exp{C(x_i, x_{-i}^*)}$
for each agent $i$.
Since
$\Expnew{}{C_i(x_i,x_{-i})}=
(1-\alpha_i) \sum_{j \in N_i}p_{ij}(x_i-x_j)^2 + \alpha_i(x_i-s_i)^2 $,
it follows from (\ref{eq:BKO11_cost_equivalent}) that the
opinion formation game with random payoffs has the same equilibrium $x^*$ as
the original opinion formation game.

In the following, we shall adopt the following notation for
an instance of the game of Definition~\ref{d:random_game}.
%
\begin{definition}\label{d:instance}
  We denote an instance of the opinion formation game of Definition~\ref{d:random_game}
  as $(P,s,\alpha)$.
  \begin{itemize}
    \item $P$ is a $n \times n$  matrix with non-negative elements $p_{ij}$,
      with $p_{ii}=0$ and $\sum_{j=1}^n p_{ij}$ is either $0$ or $1$.
    \item $s \in [0,1]^n$ is the internal opinion vector.
    \item $\alpha \in (0,1]^n$ the self confidence coefficient vector.
  \end{itemize}
\end{definition}
%
We use the matrix $P$ to simplify notation, $p_{ij} = w_{ij}/(\sum_{j \in N_i}w_{ij}+w_i)$
if $j \in N_i$ and $0$ otherwise.
% If $N_i \neq \emptyset$ then $\sum_{j \in N_i}p_{ij}=1$.
% To properly define our game, we remark that if $N_i=\emptyset$ then
% $\alpha_i=1,\sum_{j=1}^n p_{ij}=0$ and agent $i$ suffers cost $(x_i-s_i)^2$.
%Abusing notation we will sometimes refer to the graph $G$.
Another parameter of an instance $I=(P,s,\alpha)$ that we often use is
$\rho=\min_{i \in V}\alpha_i$. Throughout the paper we consider that the agents repeatedly 
play the one shot game of Definition~\ref{d:random_game}.
We denote as $W_i^t$ the neighbor that agent $i$ met
at round $t$, which is a random variable whose 
probability distribution is determined by the 
instance $I=(P,s,\alpha)$ of the game, $\Prob{W_i^t}=p_{ij}$.


In Section~\ref{s:fictitious_convergence}, we study the convergence properties
of the dynamics $x(t)$ when all agents update their opinions
according to the \enquote{follow the leader} principle.
Since each agent $i$ must select $x_i(t)$, before knowing which of her neighbors she
will meet and what opinion her neighbor will express, their update rule
says \enquote{\emph{play the best according to what you have observed}}.
For a given instance $(P,s,a)$ of the game %of Definition~\ref{d:random_game}
the Follow the Leader dynamics $x(t)$ is defined in \ref{alg:fictitious_dynamics} and
Theorem \ref{t:fictitious_convergence} shows its convergence rate to $x^*$.
%
\begin{algorithm}
  \caption{Follow the Leader dynamics}
  \label{alg:fictitious_dynamics}
  \begin{algorithmic}[1]
    \STATE Initially $x_i(0) = s_i$ for all agents $i$.
    \STATE At round $t\geq 0$ each agent $i$:
    \bindent
    \STATE Meets neighbor with index $W_i^t$, $\Prob{W_i^t=j}=p_{ij}$.
    \STATE Suffers cost \((1-\alpha_i) (x_i(t) - x_{W_i^t}(t))^2 + a_i (x_i(t) - s_i)^2\)
    and learns the opinion $x_{W_i^t}$.
  \item Updates her opinion
    \begin{equation}\label{eq:fictitious_play}
      x_i(t+1) =
      \argmin_{x \in [0,1]}
      \sum_{\tau=0}^{t}(1-\alpha_i)(x-x_{W_i^\tau}(\tau))^2+\alpha_i(x-s_i)^2
    \end{equation}
    \eindent
  \end{algorithmic}
\end{algorithm}
%

% According to this principle Brown proposed \emph{fictitious play}
% \cite{Bro51}, which is one of the most intuitive and simple
% models of playing in finite games.
% Abusing terminology we refer to (\ref{eq:fictitious_play}) as fictitious play.
% We show that in our infinite strategy game, if all agents
% adopt fictitious play, the resulting opinion vector
%
\begin{reptheorem}{t:fictitious_convergence}
  Let $I = (P,s, \alpha)$ be an instance of the opinion formation
  game of Definition~\ref{d:random_game} with equilibrium
  $x^* \in [0,1]^n$.  The opinion vector $x(t)\in[0,1]^n$ produced by
  update rule~(\ref{eq:fictitious_play}) after $t$ rounds satisfies
  \[
    \Expnew{}{\norm{\infty}{x(t) - x^*}} \leq
    C \sqrt{\log n}\frac{(\log t)^{2}}{t^{\min(1/2,\rho)}},
  \]
  where $\rho = \min_{i \in V} a_i$ and $C$ is a universal constant.
\end{reptheorem}


In Section~\ref{s:fictitious_noregret} we argue that,
apart from its simplicity, update rule (\ref{eq:fictitious_play}) ensures
no-regret for the agents, and therefore is a \emph{rational game play} for selfish agents.
%At each round $t$ each agent $i$, selects an opinion $x_i(t) \in [0,1]$
%and suffers a cost $(1-\alpha_i)(x_i(t)-x_{W_i^t}(t))^2 + \alpha_i(x_i(t)-s_i)^2$.
Since each agent $i$ selfishly wants to minimizing her individual cost,
it is natural to assume that she selects $x_i(t)$ according to
an \emph{no-regret algorithm} for the \emph{online convex optimization problem}
where the adversary chooses a function $f_t(x)=(1-\alpha_i)(x-b_t)^2 + \alpha_i(x-s_i)^2$
at each round $t$. In Theorem~\ref{t:fictitious_noregret}
we prove that \enquote{Follow the Leader} is a no-regret algorithm
for the above OCO problem. We remark that this does not hold,
if the adversary can pick functions from a larger class 
(see e.g. chapter 5 in \cite{Haz16}).

\begin{reptheorem}{t:fictitious_noregret}
  Consider the function $f:[0,1]^2 \mapsto [0,1]$ with
  $f(x,b) = (1-\alpha)(x-b)^2 + \alpha(x-s)^2$ for some
  constants $s,\alpha \in [0,1]$.
  Let $\{b_t\}_{t=1}^\infty$ be an arbitrary sequence with
  $b_t \in [0,1]$. If $x_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}f(x_,b_\tau)$
  then for all $t$,
  \[
    \sum_{\tau=0}^{t}f(x_\tau,b_\tau) \leq
    \min_{x \in [0,1]}\sum_{\tau=0}^tf(x,b_\tau) + \bigOh{\log t}
  \]
\end{reptheorem}
The FTL-dynamics can efficiently simulate the opinion formation
process in limited information exchange environments.  It ensures
convergence to the same equilibrium as the original FJ-model
while vastly reducing the information exchange between the agents
per round; it only requires each agent to learn the opinion
of just one neighbor.
In terms of total communication needed to get within distance $\eps$ of the
equilibrium $x^*$, the update rule (\ref{eq:fictitious_play}) needs
$O(n \log n)$ communication while (\ref{eq:FJ_model}) needs
$O(|E|)$.
Of course for this difference to be significant we need
each agent to have at least $O(\log n)$ friend which
not unusual in today's large social networks.
% A large social network like Facebook has approximately $2$ billion users
% and each user has usually more that $100$ friends which is far more
% than $\log(2\ 10^9)$.

Even though the FTL dynamics admit the above
nice properties, its convergence rate is
outperformed by the convergence rate of the FJ-model. For
a fixed instance $I=(P,s,\alpha)$, both FTL dynamics and FJ-model
converge to the same equilibrium point $x^*$, however 
FTL dynamcis converges with rate
$\widetilde{O}(1/t^{\text{min}(\rho,1/2)})$ while FJ-model
converges with rate $O(e^{-\rho t})$ \cite{GS14}.
As a result the following question arises
\begin{question}
  Can the agents adopt other no-regret algorithms such that the resulting
  dynamics $x(t)$ converges exponential fast to $x^*$?
\end{question}

In Section~\ref{s:lower_bound} we answer this question in the negative.
The reason that fictitious play converges slowly is that
update rule (\ref{eq:fictitious_play})
only depends on the opinions of the agents that agent $i$ meets,
$\alpha_i$, and $s_i$. This is also true for any no-regret algorithm
that $i$ uses to select $x_i(t)$ (see Section~\ref{s:lower_bound}).
We call such update rules \enquote{\emph{opinion dependent}}
and the produced dynamics \emph{opinion dependent dynamics}.

\begin{definition}[opinion dependent update rule]\label{d:opinion_dependent_dynamics}
An opinion dependent update rule $A$ is a sequence of 
functions $(A_t)_{t=0}^\infty$ where
$A_t: [0,1]^{t+2}\mapsto [0,1]$.
\end{definition}

\begin{definition}[opinion dependent Dynamics]\label{d:opinion_dependent_dynamics}
Let an opinion depedent update rule $A$. For a given instance $I=(P,s,\alpha)$
the rule $A$ produces an opinion depedent dynamics $x_A(t)$ defined as follows:
\begin{itemize}
 \item Initially each agent $i$ has opinion $x_i^A(0)=A_0(s_i,\alpha_i)$
 \item At each round $t\geq 1$, each agent $i$ updates her opinion as follows:
 \[x_i^A(t)=A_t(x_{W_i^0}(0),\dots,x_{W_i^{t-1}}(t-1),\alpha_i,s_i)\]
where $W_i^t$ is the neighbors that $i$ meets at round $t$.
\end{itemize}
\end{definition}


In Theorem~\ref{t:lower_bound} we
show that for any opinion dependent update rule there exists an instance
$I = (P,s,\alpha)$ where $\poly(1/\eps)$ rounds are required to
achieve convergence within error $\eps$.
\begin{reptheorem}{t:lower_bound}
  Let $A$ be an \emph{opinion dependent} update rule, which all
  agents use to update their opinions.
  For any $c>0$ there exists an instance $I=(P,s,a)$ such that
  \[
    \Exp{\norm{\infty}{x_A(t) - x^*}} = \Omega(1/t^{1+c}),
  \]
  where $x_A(t)$ denotes the opinion vector produced by $A$
  for the instance $I=(P,s,\alpha)$.
\end{reptheorem}
%
To prove Theorem~\ref{t:lower_bound}, we show that opinion dependent rules with
\enquote{small round complexity}, imply the existence
of estimators for Bernoulli distributions with
\enquote{small} sample complexity. Then with a simple argument
presented in Lemma~\ref{l:estimation_lower_bound},
we show that such estimators cannot exist.
In Section~\ref{s:lower_bound} we also
briefly discuss two well-known sample complexity lower bounds
from the statistics literature and explain why they do not work in our case.

In Section~\ref{s:cc_convergence}, we present a simple update rule that
is not opinion dependent and  achieves error rate $e^{-\bigOh{\sqrt{t}}}$.
This update rule is a function of the opinions and the indices of the agents
that $i$ met, $\alpha_i,s_i$ and the $i$-th row the matrix $P$.
We mention that the lower bound presented in Theorem~\ref{t:lower_bound}
applies for \enquote{opinion dependent rules} that also depend on the
agents' indices that $i$ met.  Therefore, the dependency on the row $P_i$ is
inevitable in order to obtain exponential convergence.
Although, the assumption that the agents are aware of the influence matrix
$P$ is up to discussion, this update rule reveals that the slow convergence of
\emph{opinion dependent} update rules is not due to the reduced information exchange
(learning the opinion of only one agent), but due to the fact
that the agents are \enquote{oblivious} to the influence matrix $P$ of the
game and they learn it during the game play.
