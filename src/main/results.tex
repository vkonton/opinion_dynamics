\section{Our Results and Techniques}\label{s:results}
As previously mentioned, an instance of the game in \cite{BKO11}
is also an instance of the game of Definition~\ref{d:random_game}.
Following the notation introduced earlier we have that
$p_{ij} = w_{ij}/\sum_{j \in N_i}w_{ij}$ if $j \in N_i$ and $0$ otherwise.
Moreover, $\alpha_i=w_{ii}/(\sum_{j \in N_i}w_{ij}+w_{ii})>0$ since $w_{ii}>0$
by the definition of the game in \cite{BKO11}.
If an agent $i$ does not have outgoing edges ($N_i = \emptyset$) then
$p_{ij} = 0$ for all $j$. Therefore $\sum_{j=1}^n p_{ij}=0$, $\alpha_i=1$ if $N_i= \emptyset$
and $\sum_{j=1}^n p_{ij}=1$, $\alpha_i \in (0,1)$ otherwise.
For simplicity we adopt the following notation for
an instance of the game of Definition~\ref{d:random_game}.
%
\begin{definition}\label{d:instance}
  We denote an instance of the opinion formation game of Definition~\ref{d:random_game}
  as $I=(P,s,\alpha)$, where
    $P$ is a $n \times n$  matrix with non-negative elements $p_{ij}$,
      with $p_{ii}=0$ and $\sum_{j=1}^n p_{ij}$ is either $0$ or $1$,
    $s \in [0,1]^n$ is the internal opinion vector,
    $\alpha \in (0,1]^n$ the self confidence vector.
\end{definition}
%

An instance $I=(P,s,\alpha)$ is also an instance
of the FJ model, since by the update rule~(\ref{eq:FJ_model})
$x_i(t)=(1-\alpha_i)\sum_{j \in N_i}p_{ij}x_j(t-1) + a_i s_i$.
It also defines the opinion vector $x^* \in [0,1]^n$ which is the
stable point of the FJ model and the Nash equilibrium
of the game in \cite{BKO11}.
\begin{definition}
For a given instance $I=(P,s,\alpha)$ the equilibrium
$x^*\in [0,1]^n$ is the unique solution of the following linear system,
for every $i \in V$, $x^*_i=(1-\alpha_i)\sum_{j \in N_i}p_{ij}x_j^* + a_i s_i$.
\end{definition}
The fact that the above linear system always admits a solution follows
by matrix norm properties.
Throughout the paper we study \emph{dynamics} of the game of
Definition~\ref{d:random_game}.  We denote as $W_i^t$ the neighbor that agent $i$
met at round $t$, which is a random variable whose probability distribution is
determined by the instance $I=(P,s,\alpha)$ of the game, $\Prob{W_i^t=j}=p_{ij}$.
Another parameter of an instance $I$ that we often use is
$\rho=\min_{i \in V}\alpha_i$.

In Section~\ref{s:fictitious_convergence}, we examine the convergence properties
of the opinion vector $x(t)$ when all agents update their opinions
according to the \emph{Follow the Leader} principle.
Since each agent $i$ must select $x_i(t)$, before knowing which of her neighbors
she will meet and what opinion her neighbor will express, this update rule
says \enquote{\emph{play the best according to what you have observed}}.
For a given instance $(P,s,a)$ of the game %of Definition~\ref{d:random_game}
the Follow the Leader dynamics $x(t)$ is defined in Dynamics~\ref{alg:FTL_dynamics} and
Theorem \ref{t:fictitious_convergence} shows its convergence rate to $x^*$.
%
\begin{algorithm}
  \caption{Follow the Leader dynamics}
  \label{alg:FTL_dynamics}
  \begin{algorithmic}[1]
    \STATE Initially $x_i(0) = s_i$ for all agents $i$.
    \STATE At round $t\geq 0$ each agent $i$:
    \bindent
    \STATE Meets neighbor with index $W_i^t$, $\Prob{W_i^t=j}=p_{ij}$.
    \STATE Suffers cost \((1-\alpha_i) (x_i(t) - x_{W_i^t}(t))^2 + a_i (x_i(t) - s_i)^2\)
    and learns the opinion $x_{W_i^t}(t)$.
  \STATE Updates her opinion
    \inlineequation[eq:fictitious_play]{
      x_i(t+1) =
      \argmin_{x \in [0,1]}
      \sum_{\tau=0}^{t}(1-\alpha_i)(x-x_{W_i^\tau}(\tau))^2+\alpha_i(x-s_i)^2
    }
    \eindent
  \end{algorithmic}
\end{algorithm}
%

% According to this principle Brown proposed \emph{fictitious play}
% \cite{Bro51}, which is one of the most intuitive and simple
% models of playing in finite games.
% Abusing terminology we refer to (\ref{eq:fictitious_play}) as fictitious play.
% We show that in our infinite strategy game, if all agents
% adopt fictitious play, the resulting opinion vector
%
\begin{reptheorem}{t:fictitious_convergence}
  Let $I = (P,s, \alpha)$ be an instance of the opinion formation
  game of Definition~\ref{d:random_game} with equilibrium
  $x^* \in [0,1]^n$.  The opinion vector $x(t)\in[0,1]^n$ produced by
  update rule~(\ref{eq:fictitious_play}) after $t$ rounds satisfies
  \[
    \Expnew{}{\norm{\infty}{x(t) - x^*}} \leq
    C \sqrt{\log n}\frac{(\log t)^{3/2}}{t^{\min(1/2,\rho)}},
  \]
  where $\rho = \min_{i \in V} a_i$ and $C$ is a universal constant.
\end{reptheorem}


In Section~\ref{s:fictitious_noregret} we argue that,
apart from its simplicity, update rule (\ref{eq:fictitious_play}) ensures
no regret to any agent that adopts it and therefore the FTL dynamics
can be considered as \emph{natural dynamics} for selfish agents.
%At each round $t$ each agent $i$, selects an opinion $x_i(t) \in [0,1]$
%and suffers a cost $(1-\alpha_i)(x_i(t)-x_{W_i^t}(t))^2 + \alpha_i(x_i(t)-s_i)^2$.
Since each agent $i$ selfishly wants to minimize the disagreement cost
that she experiences, it is natural to assume that she selects $x_i(t)$ according to
a \emph{no regret algorithm} for the \emph{online convex optimization problem}
where the adversary chooses a function $f_t(x)=(1-\alpha_i)(x-b_t)^2 + \alpha_i(x-s_i)^2$
at each round $t$. In Theorem~\ref{t:fictitious_noregret}
we prove that \emph{Follow the Leader} is a no regret algorithm
for the above OCO problem. We remark that this does not hold,
if the adversary can pick functions from a different class
(see e.g. chapter 5 in \cite{Haz16}).

\begin{reptheorem}{t:fictitious_noregret}
  Consider the function $f:[0,1]^2 \mapsto [0,1]$ with
  $f(x,b) = (1-\alpha)(x-b)^2 + \alpha(x-s)^2$ for some
  constants $s,\alpha \in [0,1]$.
  Let $(b_t)_{t=0}^\infty$ be an arbitrary sequence with
  $b_t \in [0,1]$. If $x_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}f(x_,b_\tau)$
  then for all $t$,
  \(
    \sum_{\tau=0}^{t}f(x_\tau,b_\tau) \leq
    \min_{x \in [0,1]}\sum_{\tau=0}^tf(x,b_\tau) + \bigOh{\log t}.
  \)
\end{reptheorem}

On the positive side, the FTL dynamics converges to $x^*$ and its
update rule is simple and ensures no regret to the agents.
On the negative side, its convergence rate is outperformed by the rate of FJ
model.  For a fixed instance $I=(P,s,\alpha)$, the FTL dynamics converges with
rate $\widetilde{O}(1/t^{\text{min}(\rho,1/2)})$ while FJ model
converges with rate $O(e^{-\rho t})$ \cite{GS14}.

\begin{question}
  Can the agents adopt other no regret update rules such that the resulting
  dynamics converges fast to $x^*$?
\end{question}

%In Section~\ref{s:lower_bound} we answer this quecstion in the negative.
The answer is no. In Section~\ref{s:lower_bound}, we prove that fast convergence cannot be established 
for any \emph{no regret dynamics}.
The reason that FTL dynamics converges slowly is that
 rule (\ref{eq:fictitious_play})
only depends on the opinions of the neighbors that agent $i$ meets,
$\alpha_i$, and $s_i$. This is also true for any update rule that
ensures no regret to the agents (see Section~\ref{s:lower_bound}).
As already mentioned, we call this larger class
of update rules \emph{graph oblivious}, and we prove that fast convergence
cannot be established for \emph{graph oblivious dynamics}.

\begin{definition}[graph oblivious update rule]\label{d:opinion_dependent_dynamics}
A graph oblivious update rule $A$ is a sequence of
functions $(A_t)_{t=0}^\infty$ where
$A_t: [0,1]^{t+2}\mapsto [0,1]$.
\end{definition}

\begin{definition}[graph oblivious dynamics]\label{d:opinion_dependent_dynamics}
Let a graph oblivious update rule $A$. For a given instance $I=(P,s,\alpha)$
the rule $A$ produces a graph oblivious dynamics $x_A(t)$ defined as follows:
\begin{itemize}
 \item Initially each agent $i$ selects her opinion $x_i^A(0)=A_0(s_i,\alpha_i)$
 \item At round $t\geq 1$, each agent $i$ selects her opinion
   \(x_i^A(t)=A_t(x_{W_i^0}(0),\dots,x_{W_i^{t-1}}(t-1),s_i,\alpha_i)\),
where $W_i^t$ is the neighbors that $i$ meets at round $t$.
\end{itemize}
\end{definition}
%Note that FTL dynamics is a graph oblivious dynamics
%since update rule~(\ref{eq:fictitious_play}) can
%be written equivalently, $x_i(0)=s_i$ and $x_i(t)=(1-\alpha_i)\sum_{\tau=0}^{t-1}x_{W_i^\tau}(\tau)/t + \alpha_i s_i$.
Theorem~\ref{t:lower_bound} states that for any graph oblivious dynamics
there exists an instance $I = (P,s,\alpha)$, where roughly $\Omega(1/\eps)$
rounds are required to achieve convergence within error $\eps$.
\begin{reptheorem}{t:lower_bound}
  Let $A$ be a \emph{graph oblivious} update rule, which all agents use to
  update their opinions.  For any $c>0$ there exists an instance $I=(P,s,a)$
  such that
  \(
    \Exp{\norm{\infty}{x_A(t) - x^*}} = \Omega(1/t^{1+c}),
  \)
  where $x_A(t)$ denotes the opinion vector produced by $A$ for the instance
  $I=(P,s,\alpha)$.
\end{reptheorem}
%
To prove Theorem~\ref{t:lower_bound}, we show that 
graph oblivious rules whose dynamics converge fast
imply the existence of estimators for Bernoulli distributions with
\enquote{small} sample complexity. The key part of the proof lies 
in Lemma~\ref{l:estimation_lower_bound}, in which it is proven
that such estimators cannot exist.
We also briefly discuss two well-known sample complexity lower bounds
from the statistics literature and explain why they do not work in our case.

In Section~\ref{s:cc_convergence}, we present a simple update rule that
achieves error rate $\me^{-\widetilde{O}(\sqrt{t})}$.
This update rule is a function of the opinions and the indices of the neighbors
that $i$ met, $s_i,\alpha_i$ and the $i$-th row of the matrix $P$. Obviously
this rule is not \emph{graph oblivious}, due to its dependency on the $i$-th row
and the indices, 
and thus does not ensure no regret to an agent that adopts it 
(see Example~\ref{ex:regret_side_result} in Appendix~\ref{app:s:cc_convergence}).
However it reveals that slow convergence is not a generic property of 
the limited information dynamics, but comes with the assumption that 
agents act selfishly.
%We mention that the lower bound presented in Theorem~\ref{t:lower_bound}
%applies for \emph{graph oblivious rules} that also depend on the
%agents' indices that $i$ met.  Therefore, the dependency on the row $P_i$ is
%inevitable in order to obtain fast convergence.