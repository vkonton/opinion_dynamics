\section{Follow the Leader ensures no regret}\label{s:fictitious_noregret}

In this section we provide rigorous definitions of \emph{no regret} algorithms
and explain why update rule~(\ref{eq:fictitious_play}) ensures
no regret to any agent that repeatedly plays the game of
Definition~\ref{d:random_game}.
Based on the cost that the agents experience,
we consider an appropriate \emph{Online Convex Optimization} problem.
This problem can be viewed as a \enquote{game} played between an
adversary and a player. \emph{At round }$t\geq 0$,
\begin{enumerate}
  \item \emph{the player selects a value }$x_t \in [0,1]$.
  \item \emph{the adversary observes the }$x_t$ \emph{and selects a} $b_t \in [0,1]$
  \item \emph{the player receives cost} $f(x_t,b_t)=(1-\alpha)(x_t-b_t)^2 + \alpha(x_t -s)^2$.
\end{enumerate}
where $\alpha,s$ are constants in $[0,1]$. The goal of
the player is to pick $x_t$ based on the history
$(b_0,\ldots,b_{t-1})$ in a way that minimizes her total cost.
Generally, different OCO problems can be defined by a set of functions
$\mcal{F}$ that the adversary chooses from and a feasibility
set $\mcal{K}$ from which the player picks her value (see \cite{Haz16}
for an introduction to the OCO framework).
In our case the feasibility set is $\mcal{K}=[0,1]$ and the set of functions
is
$\mcal{F}_{\alpha,s} = \{x \mapsto (1-\alpha)(x-b)^2 + \alpha(x -s)^2 : b \in [0,1]\}$.
As a result, each selection of the
constants $s,\alpha$ lead to a different OCO problem.

\begin{definition}\label{d:OCO_algo}
An algorithm $A$ for the OCO problem with $\mcal{F}_{a,s}$ and
$\mcal{K}=[0,1]$ is a sequence of functions $(A_t)_{t=0}^\infty$ where $A_t:[0,1]^t \mapsto [0,1]$.
\end{definition}

\begin{definition}\label{d:no_regret_algo}
An algorithm $A$ is no regret for the OCO problem with $\mcal{F}_{a,s}$ and
$\mcal{K}=[0,1]$ if and only if for all sequences $(b_t)_{t=0}^\infty$ that the
adversary may choose, if $x_t = A_t(b_0,\dots,b_{t-1})$ then for all $t$
\(\sum_{\tau=0}^t f(x_\tau,b_\tau)  \leq \min_{x \in [0,1]}\sum_{\tau=0}^t f(x,b_\tau) + o(t).\)
\end{definition}
Informally speaking, if the player selects the value
$x_t$ according to a \emph{no regret algorithm} then
she does not regret for not playing any fixed value no
matter what the choices of the adversary are.
Theorem~\ref{t:fictitious_noregret} states that
\emph{Follow the Leader} i.e. $x_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}f(x,b_\tau)$
is a no regret algorithm for all the OCO problems with $\mcal{F}_{\alpha,s}$.

Returning to the dynamics of the game in Definition~\ref{d:random_game},
it is reasonable to consider that each agent $i$ selects $x_i(t)$ according
to no regret algorithm $A_i$ for the OCO problem with $\mcal{F}_{s_i,\alpha_i}$,
since by Definition~\ref{d:no_regret_algo},
\[\frac{1}{t}\sum_{\tau=0}^t f_i(x_i(\tau),x_{W_i^\tau}(\tau)) \leq
\frac{1}{t}\min_{x \in [0,1]}\sum_{\tau=0}^tf_i(x,x_{W_i^\tau}(\tau)) + \frac{o(t)}{t}\]
The latter means that the time averaged total disagreement cost
that she suffers is close to the time averaged cost by expressing the
best fixed opinion and this holds no matter the opinions of the
neighbors that $i$ meets. Meaning that even if the other agents
selected their opinions maliciously, her total experienced cost
would still be in a sense minimal. Under this perspective
update rule~(\ref{eq:fictitious_play}) is a rational choice for
selfish agents and as a result FTL dynamics is 
a \emph{natural} limited information variant of the FJ model.


%Theorem~\ref{t:fictitious_noregret}
%ensures that \emph{Follow the Leader} is a no regret algorithm
%for any OCO problem $\mcal{F}_{s_i,\alpha_i}$ and explains why
%(\ref{eq:fictitious_play}) is a rational update rule for the agents.

We now present the key steps for proving Theorem~\ref{t:fictitious_noregret}.
We first prove that a similar strategy that also takes into
account the value $b_t$ admits no regret (Lemma~\ref{l:y_t}).
Obviously knowing the value $b_t$ before selecting $x_t$
is in direct contrast with the OCO framework, however proving
the no regret property for this algorithm easily extends to
establishing the no regret property of \emph{Follow the Leader}.
The proofs of the subsequent lemmas and Theorem~\ref{t:fictitious_noregret} can
be found in the Appendix~\ref{app:s:fictitious_noregret}.
\begin{replemma}{l:y_t}
Let $\{b_t\}_{t=0}^\infty$ be an arbitrary sequence with $b_t \in [0,1]$.
Let $y_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^tf(x_,b_\tau)$
then for all $t$,
\(
\sum_{\tau=0}^t f(y_\tau,b_\tau) \leq \min_{x \in [0,1]}
\sum_{\tau = 0}^tf(x,b_\tau).
\)
\end{replemma}
Now we can understand why \emph{Follow the Leader}
admits no regret. Since the cost incurred by the sequence $y_t$ is at most that
of the best fixed value, we can compare the cost incurred by $x_t$ with
that of $y_t$.  Since the functions in $\mcal{F}_{\alpha,s}$ are quadratic,
the extra term $f(x,b_t)$ that $y_t$ takes into account doesn't change
dramatically the minimum of the total sum. Namely, $x_t,y_t$
are relatively close.
%Thus, for each $t$ the
%numbers $x_t$ and $y_t$ are quite close and as a result the
%difference in their cost must be quite small. The above are
%formally stated in Lemma~\ref{l:no_regret_lemma}
%which implies directly Theorem~\ref{t:fictitious_noregret}.
%For the proof see Appendix~\ref{app:s:fictitious_noregret}.
\begin{replemma}{l:no_regret_lemma}
  For all $t\geq 0$,
  \(
    f(x_t,b_t) \leq f(y_t,b_t) + 2\frac{1-\alpha}{t+1} +
    \frac{(1-\alpha)^2}{(t+1)^2}
  \).
\end{replemma}
