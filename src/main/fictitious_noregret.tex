\section{Fictitious Play is no-regret}\label{s:fictitious_noregret}

In this section we consider the Online Convex Optimization problem
that we defined in Subsection~\ref{s:preliminaries:oco}. 
For every agent, a different OCO problem is defined.
We focus on a specific agent $i$ and denote by $\mcal{F}_i$ the class of functions
that she takes as input. 
We remind that $\mcal{F}_i$ contains all functions of the form
\[
  C^t(x) = \alpha_i(x-s_i)^2 + (1-\alpha_i)(x-b_t)^2,
\]
where $s_i,\alpha_i \in [0,1]$ and are independent of $t$ and $b_t \in [0,1]$.
 
Essentially, the agent $i$ knows the values of $\alpha_i , s_i$, so $y_t$ is the only
parameter that distinguishes the functions of $\mcal{F}_i$.
In other words, the function $C^t$ is uniquely determined by the number $y_t$.
We show that fictitious play admits no-regret for the OCO problem of every agent $i$.
For convenience of notation, since we have fixed an agent $i$, we denote by
$x^t$ the choice that $A$ makes at time $t$ in this OCO setting. 
\begin{theorem}\label{t:no_regret}
  Let $C^t$ be a sequence of functions, where each function has the form
  $C^t(x) = \alpha_i(x-s_i)^2 + (1-\alpha_i)(x-b_t)^2$,
  where $s_i,\alpha_i \in [0,1]$ and do not dependent on $t$
  and $b_t \in [0,1]$.
  If we define \(x^t = \argmin_{x \in [0,1]} \sum_{\tau = 0}^{t-1} C^\tau (x) \)
  then for each time $T$
  \[
    \sum_{t = 0}^{T} C^t(x^t) \leq
    \min_{x \in [0,1]} \sum_{t=0}^T C^t(x) + O(\log  T).
  \]
\end{theorem}

In order to prove this claim, we will first define a similar rule $y^t$
that takes into account the function $C^t$ for the "prediction" at time $t$.
Intuitively, this guarantees that the rule admits no regret.

\begin{lemma}
  Let
  \(y^t = \argmin_{x \in [0,1]} \sum_{\tau = 0}^t C^\tau (x)\)
  then
  \[
    \sum_{t=0}^T C^t(y^t) \leq \min_{x \in [0,1]} \sum_{t=0}^T C^t(x)
  \]
\end{lemma}

\begin{proof}By definition of $y^t$,
  $\min_{x \in [0,1]} \sum_{t=0}^T C^t(x) = \sum_{t=0}^T C^t(y^T)$, so
  \begin{align*}
    \sum_{t=0}^T C^t(y^t) - \min_{x \in [0,1]} \sum_{t=0}^T C^t(x) &=
    \sum_{t=0}^T C^t(y^t) - \sum_{t=0}^T C^t(y^T)\\
    &= \sum_{t=0}^{T-1} C^t(y^t) - \sum_{t=0}^{T-1} C^t(y^T)\\
    &\leq \sum_{t=0}^{T-1} C^t(y^t) - \sum_{t=0}^{T-1} C^t(y^{T-1})\\
    &= \sum_{t=0}^{T-2} C^t(y^t) - \sum_{t=0}^{T-2} C^t(y^{T-1})
  \end{align*}
  Continuing in the same way, we get
  $\sum_{t=0}^T C^t(y^t) \leq min_{x \in [0,1]} \sum_{t=0}^T C^t(x)$.
\end{proof}

Now we can derive some intuition for the reason that \emph{fictitious play}
admits no regret. Since the cost incurred by the sequence $y^t$ is at most that
of the best fixed strategy, we can compare the cost incurred by $x^t$ with
that of $y^t$.  However, the functions in $\mcal{F}_i$ are Lipschitz-continuous and more 
specifically quadratic. 
These functions are all "similar" to each other, so the extra function $C^t$ that $y^t$ takes
as input doesn't change dramatically the minimum point of the sum.
Thus, for each $t$ the numbers $x^t$ and $y^t$ are quite
close and as a result the difference in their cost must be quite small.

\begin{lemma}
  For all $t$,
  \(C^t(x^t) \leq C^t(y^t) + 2\frac{1-\alpha_i}{t+1} + \frac{(1-\alpha_i)^2}{(t+1)^2}\).
\end{lemma}
\begin{proof}
  We first prove that for all $t$,
  \begin{equation}\label{eq:abs_value}
    \lp|x^t - y^t \rp| \leq \frac{1-\alpha_i}{t+1}.
  \end{equation}
  By definition
  \(x^t = \alpha_i s_i + (1-\alpha_i)\frac{\sum_{\tau = 0}^{t-1} b_\tau}{t}\)
  and
  \( y^t = \alpha_i s_i + (1-\alpha_i)\frac{\sum_{\tau = 0}^t b_\tau}{t+1}\).
  \begin{align*}
    \lp|x^t - y^t\rp|
    &=
    (1-\alpha_i)\lp|\frac{\sum_{\tau = 0}^{t-1}b_\tau}{t}
    - \frac{\sum_{\tau = 0}^t b_\tau}{t+1}\rp|\\
    &=
    (1-\alpha_i)\lp|\frac{\sum_{\tau = 0}^{t-1}b_\tau -tb_t}{t(t+1)}\rp|\\
    &\leq
    \frac{1-\alpha}{t+1}
  \end{align*}
  The last inequality follows from the fact that $b_\tau \in [0,1]$.
  We now use inequality~(\ref{eq:abs_value}) to bound the difference
  \( C^t(x^t) - C^t(y^t) \).
  \begin{align*}
    C^t(x^t)
    &=
    \alpha_i(x^t - s_i)^2 + (1 - \alpha_i)(x^t - y_t)^2 \\
    &\leq
    \alpha_i(y^t - s_i)^2 + 2\alpha_i\lp|y^t -
    s_i\rp|\lp|x^t - y^t\rp| + \alpha_i \lp|x^t - y^t\rp|^2 \\
    &\quad + (1-\alpha_i)(y_t - y_t)^2 +
    2(1-\alpha_i)\lp|y^t - y_t\rp|\lp|x^t-y^t\rp| + (1 - \alpha_i)\lp|x^t - y^t\rp|^2\\
    &\leq
    C^t(y^t) + 2\lp|x^t - y^t\rp| + \lp|y^t - x^t\rp|^2\\
    &\leq
    C^t(y^t) + 2\frac{1-\alpha_i}{t+1} + \frac{(1-\alpha_i)^2}{(t+1)^2}
  \end{align*}
\end{proof}

Theorem~\ref{t:no_regret} easily follows since
\begin{align*}
  \sum_{t=0}^T C^t(x^t)
  &\leq
  \sum_{t=0}^T C^t(y^t) + \sum_{t=0}^T 2\frac{1-\alpha}{t+1} +
  \sum_{t=0}^T \frac{(1-\alpha)^2}{(t+1)^2}\\
  &\leq
  \min_{x \in [0,1]} \sum_{t=0}^T C^t(x) +
  2(1-\alpha)(\log T + 1) + (1-\alpha)\frac{\pi^2}{6}\\
  &\leq
  \min_{x \in [0,1]} \sum_{t=0}^T C^t(x) + O(\log T)
\end{align*}
