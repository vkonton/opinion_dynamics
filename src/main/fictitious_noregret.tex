\section{Fictitious Play is no-regret}\label{s:fictitious_noregret}

%As we have already mentioned in the repeated version of an instance $(P,s,\alpha)$,
%At round $t$ agent $i$ each agent $i$,
%\begin{enumerate}
% \item selects an opinion $x_i(t)$
% \item meets the neighbor $W_i^t$, $\Prob{W_i^t = j}=p_{ij}$
% \item suffers cost $(1-\alpha_i)(x_i(t)-x_i(t))^2 + \alpha_i(x_i(t)-s_i)^2$.
%\end{enumerate}
%In this section we prove that an agent $i$ could consider updating her opinion using 
%update rule~\ref{eq:fictitious_play} i.e. $x_i(t)= \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}(1-\alpha_i)(x-x_i(\tau))^2 + \alpha_i(x-s_i)^2$
%since it ensures her no-regret.

\noindent In this section we explain why \emph{fictitious play} is a rational behavioral assumption in
the repeated version of the opinion formation game defined in \ref{d:random_payoff_game}. Based on this 
game we consider an appropriate \emph{Online Convex Optimization} problem. This problem can be viewed 
as the following a game played between an adversary and a player. \emph{At round }$t$,
\begin{enumerate}
  \item \emph{the player selects a value }$x_t \in [0,1]$.
  \item \emph{the adversary observes the }$x_t$ \emph{and selects a} $b_t \in [0,1]$
  \item \emph{the player receives cost} $f(x_t,b_t)=(1-\alpha)(x_t-b_t)^2 + \alpha(x_t -s)^2$.
\end{enumerate}
where $\alpha,s$ are constants in $[0,1]$. The goal of the player is to pick $x_t$ based on the history
$(b_1,\ldots,b_{t-1})$ in a way that minimizes the total cost. We emphasize
that the agent has to select $x_t$ before seeing $b_t$, otherwise the
problem becomes trivial. We show that a good strategy that the player can follow is 
\[x_t = \argmin_{x \in [0,1]}\sum_{\tau=1}^{t-1}f(x,b_\tau)\]
which is known as \emph{fictitious play}. We prove that in our OCO problem,
\emph{fictitious play} is a \emph{no-regret} algorithm meaning that
it provides guarantees concerining the cost that agent $i$ receives. Informally speaking
if an algorithm $A$ is \emph{no-regret} for an OCO problem then for any selection of the
adversary, the the total cost that player suffers is less than 
the cost that she would suffer by selecting any fixed value. The gaurantees that \emph{fictitious play} 
ensures are presented in Theorem~\label{t:fictitious_noregret} which is the main result of this section.
\begin{theorem}\label{t:fictitious_noregret}
Consider the fuction $f:[0,1]^2 \mapsto [0,1]$ with $f(x,b) = (1-\alpha)(x-b)^2 + \alpha(x-s)^2$.
Let $\{b_t\}_{t=1}^\infty$ be an arbitrary sequence with $b_t \in [0,1]$. If $x_t = \argmin_{x \in [0,1]}\sum_{\tau=1}^{t-1}f(x_,b_\tau)$
then for all $t$, 
\[\sum_{\tau=1}^{t}f(x_\tau,b_\tau) \leq \min_{x \in [0,1]}\sum_{\tau=1}^tf(x,b_\tau) + \bigOh{\log t}\]
\end{theorem}

Theorem~\ref{t:fictitious_noregret} explains why update rule~(\ref{eq:fictitious_play}) is a
reasonable choice for selfish agents. In our repeated game, agent $i$ selects at round each $t$ an opinion
$x_i(t)\in [0,1]$ and then suffers cost $f_i(x_i(t),x_{W_i^t}(t))=(1-\alpha_i)(x_i(t)-x_{W_i^t}(t))^2+\alpha_i(x_i(t)-s_i)^2$. 
If agent $i$ selects $x_i(t)$ according  to the update rule \ref{eq:fictitious_play} then Theorem~\ref{t:fictitious_noregret}
applies and\[\frac{1}{t}\sum_{\tau=1}^t f_i(x_i(\tau),x_{W_i^\tau}(\tau)) \leq
\frac{1}{t}\min_{x \in [0,1]}\sum_{\tau=1}^tf_i(x,x_{W_i^\tau}(\tau)) + \bigOh{\frac{\log t}{t}}\]
meaning that the time averaged total disagreement cost that agent $i$ suffers is similar to the 
time averaged cost of the best fixed opinion.

%\noindent Theorem~\ref{t:fictitious_noregret} implies that in the repeated version our gaame. 
%Updating the opinion according to \emph{Fictitious play} is a the rational choice for an agent $i$,
%since it ensures that the time averaged aggregated cost that she experiences is similar to the 
%is always close to the average cost of choosing choosing the best fixed opinion in hindsight no matter the way
%that the other agents update their opinions and no matter which agents $i$ meets.

The rest of the section is dedicated to prove Theorem~\ref{t:fictitious_noregret}. we first prove that a similar update rule that also takes into
account the value $b_t$ admits no-regret. Obviously knowing the value $b_t$ before time selecting $x_t$ is direct contrast
with the OCO framework, however proving the no-regret property for this algorithm easily extends to proving the no-regret property 
for fictitious play. 

\begin{lemma}
Let $\{b_t\}_{t=1}^\infty$ be an arbitrary sequence with $b_t \in [0,1]$. Let $y_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^tf(x_,b_\tau)$
then for all $t$,
\[
\sum_{\tau=0}^t f(y_\tau,b_\tau) \leq \min_{x \in [0,1]}\sum_{\tau = 0}^tf(x,b_\tau)
\] 
\end{lemma}

\begin{proof}By definition of $y_t$,
  $\min_{ x \in [0,1]} \sum_{\tau=0}^t f(x,b_\tau) = \sum_{\tau=0}^t f(y_t,b_\tau)$, so
  \begin{align*}
    \sum_{\tau=0}^t f(y_\tau,b_\tau) - \min_{ x \in [0,1]} \sum_{\tau=0}^t f(x,b_\tau) &=
    \sum_{\tau=0}^t f(y_\tau,b_\tau) - \sum_{\tau=0}^t f(y_t,b_\tau)\\
    &= \sum_{\tau=0}^{t-1} f(y_\tau,b_\tau) - \sum_{\tau=0}^{t-1} f(y_t,b_\tau)\\
    &\leq \sum_{\tau=0}^{t-1} f(y_\tau,b_\tau) - \sum_{\tau=0}^{t-1} f(y_{t-1},b_\tau)\\
    &= \sum_{\tau=0}^{t-2} f(y_\tau,b_\tau) - \sum_{\tau=0}^{t-2}f(y_{t-1},b_\tau) 
  \end{align*}
  Continuing in the same way, we get
  $\sum_{\tau=0}^t f(y_\tau,b_\tau) \leq min_{ x \in [0,1]} \sum_{\tau=0}^t f(x,b_\tau)$.
\end{proof}

Now we can derive some intuition for the reason that \emph{fictitious play}
admits no regret. Since the cost incurred by the sequence $y_t$ is at most that
of the best fixed strategy, we can compare the cost incurred by $x_t$ with
that of $y_t$.  However, the functions in $\mcal{F}_i$ are Lipschitz-continuous and more
specifically quadratic.
These functions are all "similar" to each other, so the extra function $f(x_t,b_t)$ that $y_t$ takes
as input doesn't change dramatically the minimum point of the sum.
Thus, for each $t$ the numbers $x_t$ and $y_t$ are quite
close and as a result the difference in their cost must be quite small.

\begin{lemma}
  For all $t$,
  \(f(x_t,b_t) \leq f(y_t,b_t) + 2\frac{1-\alpha}{t+1} + \frac{(1-\alpha)^2}{(t+1)^2}\).
\end{lemma}
\begin{proof}
  We first prove that for all $t$,
  \begin{equation}\label{eq:abs_value}
    \lp|x_t - y_t \rp| \leq \frac{1-\alpha}{t+1}.
  \end{equation}
  By definition
  \(x_t = \alpha s + (1-\alpha)\frac{\sum_{\tau = 0}^{t-1} b_\tau}{t}\)
  and
  \( y_t = \alpha s + (1-\alpha)\frac{\sum_{\tau = 0}^t b_\tau}{t+1}\).
  \begin{align*}
    \lp|x_t - y_t\rp|
    &=
    (1-\alpha)\lp|\frac{\sum_{\tau = 0}^{t-1}b_\tau}{t}
    - \frac{\sum_{\tau = 0}^t b_\tau}{t+1}\rp|\\
    &=
    (1-\alpha)\lp|\frac{\sum_{\tau = 0}^{t-1}b_\tau -tb^t}{t(t+1)}\rp|\\
    &\leq
    \frac{1-\alpha}{t+1}
  \end{align*}
  The last inequality follows from the fact that $b_\tau \in [0,1]$.
  We now use inequality~(\ref{eq:abs_value}) to bound the difference
  \( f(x_t,b_t) - f(y_t,b_t) \).
  \begin{align*}
    f(x_t,b_t)
    &=
    \alpha(x_t - s)^2 + (1 - \alpha)(x_t - y_t)^2 \\
    &\leq
    \alpha(y_t - s)^2 + 2\alpha\lp|y_t -
    s\rp|\lp|x_t - y_t\rp| + \alpha \lp|x_t - y_t\rp|^2 \\
    &\quad + (1-\alpha)(y_t - y_t)^2 +
    2(1-\alpha)\lp|y_t - y_t\rp|\lp|x_t-y_t\rp| + (1 - \alpha)\lp|x_t - y_t\rp|^2\\
    &\leq
    f(y_t,b_t) + 2\lp|x_t - y_t\rp| + \lp|y_t - x_t\rp|^2\\
    &\leq
    f(y_t,b_t) + 2\frac{1-\alpha}{t+1} + \frac{(1-\alpha)^2}{(t+1)^2}
  \end{align*}
\end{proof}

Theorem~\ref{t:fictitious_noregret} easily follows since
\begin{align*}
  \sum_{\tau=0}^T f(x_\tau,b_\tau)
  &\leq
  \sum_{\tau=0}^T f(y_\tau,b_\tau) + \sum_{\tau=0}^T 2\frac{1-\alpha}{\tau+1} +
  \sum_{\tau=0}^T \frac{(1-\alpha)^2}{(\tau+1)^2}\\
  &\leq
  \min_{ x \in [0,1]} \sum_{\tau=0}^T f(x,y_\tau) +
  2(1-\alpha)(\log T + 1) + (1-\alpha)\frac{\pi^2}{6}\\
  &\leq
  \min_{ x \in [0,1]} \sum_{\tau=0}^T f(x,y_\tau) + O(\log T)
\end{align*}
