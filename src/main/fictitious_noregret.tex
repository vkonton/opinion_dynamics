\section{Fictitious Play is no-regret}\label{s:fictitious_noregret}

In this section we explain why \emph{fictitious play} 
is a rational behavioral assumption in
the repeated version of the opinion formation 
game (Definition~\ref{d:random_payoff_game}). 
Based on this game we consider an appropriate 
\emph{Online Convex Optimization} problem. This 
problem can be viewed
as the following a game played between an 
adversary and a player. \emph{At round }$t\geq 0$,
\begin{enumerate}
  \item \emph{the player selects a value }$x_t \in [0,1]$.
  \item \emph{the adversary observes the }$x_t$ \emph{and selects a} $b_t \in [0,1]$
  \item \emph{the player receives cost} $f(x_t,b_t)=(1-\alpha)(x_t-b_t)^2 + \alpha(x_t -s)^2$.
\end{enumerate}
where $\alpha,s$ are constants in $[0,1]$. The goal of 
the player is to pick $x_t$ based on the history
$(b_0,\ldots,b_{t-1})$ in a way that minimizes her total cost. 
Generally, different OCO problems can be defined by the set of functions
$\mcal{F}$ that the adversary chooses from and the feasibility 
set $\mcal{K}$ from which the player picks her value (see \cite{Haz16}
for an introduction to the OCO framework).
In our case $\mcal{K}=[0,1]$ and $\mcal{F}_{\alpha,s} = \{(1-\alpha)(x-b)^2 + 
\alpha(x -s)^2,~\text{for all }b \in [0,1]\}$.
As a result, each selection of the 
constants $s,\alpha$ lead to a different OCO problem.

\begin{definition}\label{d:OCO_algo}
An algorithm $A$ for the OCO problem with $\mcal{F}_{a,s}$ and
$\mcal{K}=[0,1]$ is a sequence of functions $(A_t)_{t=1}^\infty$ where $A_t:[0,1]^t \mapsto [0,1]$.
\end{definition}

\begin{definition}\label{d:no_regret_algo}
An algorithm $A$ is no-regret for the OCO problem with $\mcal{F}_{a,s}$ and
$\mcal{K}=[0,1]$ if and only if for all sequence $(b_t)_{t=1}^\infty$ that the
adversary may choose, if $x_t = A_t(b_0,\dots,b_{t-1})$ then for all $t$
\[\sum_{\tau=0}^t f(x_\tau,b_\tau)  \leq \min_{x \in [0,1]}\sum_{\tau=0}^t f(x,b_\tau) + o(t) \]
\end{definition}
Informally speaking if the player selects the value 
$x_t$ according to a \emph{no-regret algorithm} then
she does not regret for not playing any fixed value no
matter what the choices of the adversary are.
We prove that \emph{fictitious play} i.e. 
$x_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}f(x,b_\tau)$
is a no-regret algorithm for all OCO problems $\mcal{F}_{\alpha,s}$.
This is formally stated in Theorem~\ref{t:fictitious_noregret} 
and is the main result of this section.
\repeattheorem{t:fictitious_noregret}
% \begin{theorem}\label{t:fictitious_noregret}
% Consider the fuction $f:[0,1]^2 \mapsto [0,1]$ with $f(x,b) = (1-\alpha)(x-b)^2 + \alpha(x-s)^2$.
% Let $\{b_t\}_{t=0}^\infty$ be an arbitrary sequence with $b_t \in [0,1]$. If $x_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}f(x_,b_\tau)$
% then for all $t$,
% \[\sum_{\tau=0}^{t}f(x_\tau,b_\tau) \leq \min_{x \in [0,1]}\sum_{\tau=0}^tf(x,b_\tau) + \bigOh{\log t}\]
% \end{theorem}
Returning in our repeated game, it is reasonabe to 
assume that each agent $i$ selects $x_i(t)$ according
to no-regret algorithm $A_i$ for the OCO problem with $\mcal{F}_{s_i,\alpha_i}$,
since by Definition~\ref{d:no_regret_algo},
\[\frac{1}{t}\sum_{\tau=0}^t f_i(x_i(\tau),x_{W_i^\tau}(\tau)) \leq
\frac{1}{t}\min_{x \in [0,1]}\sum_{\tau=0}^tf_i(x,x_{W_i^\tau}(\tau)) + \frac{o(t)}{t}\]
The latter means that the time averaged total disagreement cost
that she suffers is similar to the time averaged cost by expressing the 
best fixed opinion and this holds no matter the opinions of the
agents that $i$ meets. Theorem~\ref{t:fictitious_noregret}
ensures that \emph{fictitious play} is a no-regret algorithm 
for any OCO problem $\mcal{F}_{s_i,\alpha_i}$ and explains why 
(\ref{eq:fictitious_play}) is a rational update rule for the agents. 

The rest of the section is dedicated to prove Theorem~\ref{t:fictitious_noregret}. 
We first prove that a similar strategy that also takes into
account the value $b_t$ admits no-regret (Lemma~\ref{l:y_t}). 
Obviously knowing the value $b_t$ before selecting $x_t$ 
is in direct contrast with the OCO framework, however proving 
the no-regret property for this algorithm easily extends to 
establishing the no-regret property of fictitious play.

\begin{lemma}\label{l:y_t}
Let $\{b_t\}_{t=0}^\infty$ be an arbitrary sequence with $b_t \in [0,1]$. Let $y_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^tf(x_,b_\tau)$
then for all $t$,
\[
\sum_{\tau=0}^t f(y_\tau,b_\tau) \leq \min_{x \in [0,1]}\sum_{\tau = 0}^tf(x,b_\tau)
\]
\end{lemma}

\begin{proof}By definition of $y_t$,
  $\sum_{\tau=0}^t f(y_t,b_\tau)=\min_{ x \in [0,1]} \sum_{\tau=0}^t f(x,b_\tau)$, so
  \begin{align*}
    \sum_{\tau=0}^t f(y_\tau,b_\tau) - \min_{ x \in [0,1]} \sum_{\tau=0}^t f(x,b_\tau) &=
    \sum_{\tau=0}^t f(y_\tau,b_\tau) - \sum_{\tau=0}^t f(y_t,b_\tau)\\
    &= \sum_{\tau=0}^{t-1} f(y_\tau,b_\tau) - \sum_{\tau=0}^{t-1} f(y_t,b_\tau)\\
    &\leq \sum_{\tau=0}^{t-1} f(y_\tau,b_\tau) - \sum_{\tau=0}^{t-1} f(y_{t-1},b_\tau)\\
%    &= \sum_{\tau=0}^{t-2} f(y_\tau,b_\tau) - \sum_{\tau=0}^{t-2}f(y_{t-1},b_\tau)
  \end{align*}
  The last inequality follows by the fact that $y_{t-1} = \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}f(x_,b_\tau)$
  Inductuvely, we prove that $\sum_{\tau=0}^t f(y_\tau,b_\tau) \leq \min_{ x \in [0,1]} \sum_{\tau=0}^t f(x,b_\tau)$.
\end{proof}

Now we can understand reason why \emph{fictitious play}
admits no regret. Since the cost incurred by the sequence $y_t$ is at most that
of the best fixed strategy, we can compare the cost incurred by $x_t$ with
that of $y_t$.  However, the functions in $\mcal{F}_{\alpha,s}$ are 
Lipschitz-continuous and more specifically quadratic.
These functions are all "similar" to each other, so the extra 
term $f(x_t,b_t)$ that $y_t$ takes into account doesn't change 
dramatically the minimum point of the sum. Thus, for each $t$ the 
numbers $x_t$ and $y_t$ are quite close and as a result the 
difference in their cost must be quite small. The above are 
formally stated and proved in Lemma~\ref{l:no_regret_lemma}.

\begin{lemma}\label{l:no_regret_lemma}
  For all $t\geq 0$,
  \(f(x_t,b_t) \leq f(y_t,b_t) + 2\frac{1-\alpha}{t+1} + \frac{(1-\alpha)^2}{(t+1)^2}\).
\end{lemma}
\begin{proof}
  We first prove that for all $t$,
  \begin{equation}\label{eq:abs_value}
    \lp|x_t - y_t \rp| \leq \frac{1-\alpha}{t+1}.
  \end{equation}
  By definition
  \(x_t = \alpha s + (1-\alpha)\frac{\sum_{\tau = 0}^{t-1} b_\tau}{t}\)
  and
  \( y_t = \alpha s + (1-\alpha)\frac{\sum_{\tau = 0}^t b_\tau}{t+1}\).
  \begin{align*}
    \lp|x_t - y_t\rp|
    &=
    (1-\alpha)\lp|\frac{\sum_{\tau = 0}^{t-1}b_\tau}{t}
    - \frac{\sum_{\tau = 0}^t b_\tau}{t+1}\rp|\\
    &=
    (1-\alpha)\lp|\frac{\sum_{\tau = 0}^{t-1}b_\tau -tb_t}{t(t+1)}\rp|\\
    &\leq
    \frac{1-\alpha}{t+1}
  \end{align*}
  The last inequality follows from the fact that $b_\tau \in [0,1]$.
  We now use inequality~(\ref{eq:abs_value}) to bound the difference
  \( f(x_t,b_t) - f(y_t,b_t) \).
  \begin{align*}
    f(x_t,b_t)
    &=
    \alpha(x_t - s)^2 + (1 - \alpha)(x_t - y_t)^2 \\
    &\leq
    \alpha(y_t - s)^2 + 2\alpha\lp|y_t -
    s\rp|\lp|x_t - y_t\rp| + \alpha \lp|x_t - y_t\rp|^2 \\
    &\quad + (1-\alpha)(y_t - y_t)^2 +
    2(1-\alpha)\lp|y_t - y_t\rp|\lp|x_t-y_t\rp| + (1 - \alpha)\lp|x_t - y_t\rp|^2\\
    &\leq
    f(y_t,b_t) + 2\lp|x_t - y_t\rp| + \lp|y_t - x_t\rp|^2\\
    &\leq
    f(y_t,b_t) + 2\frac{1-\alpha}{t+1} + \frac{(1-\alpha)^2}{(t+1)^2}
  \end{align*}
\end{proof}

Theorem~\ref{t:fictitious_noregret} easily follows since
\begin{align*}
  \sum_{\tau=0}^t f(x_\tau,b_\tau)
  &\leq
  \sum_{\tau=0}^t f(y_\tau,b_\tau) + \sum_{\tau=0}^T 2\frac{1-\alpha}{\tau+1} +
  \sum_{\tau=0}^t \frac{(1-\alpha)^2}{(\tau+1)^2}\\
  &\leq
  \min_{ x \in [0,1]} \sum_{\tau=0}^t f(x,y_\tau) +
  2(1-\alpha)(\log t + 1) + (1-\alpha)\frac{\pi^2}{6}\\
  &\leq
  \min_{ x \in [0,1]} \sum_{\tau=0}^t f(x,y_\tau) + O(\log t)
\end{align*}
