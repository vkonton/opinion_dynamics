\section{FTL Dynamics has no-regret}\label{s:fictitious_noregret}
In this section we explain why the
update rule~\ref{eq:fictitious_play} (\enquote{Follow the Leader}) ensures
no-regret to the agents that play the repeated version of the one shot
game of Definition~\ref{d:random_game}.
Based on the disagreement cost that each agent experiences,
we consider an appropriate \emph{Online Convex Optimization} problem.
This problem can be viewed as the following \enquote{game} played between an
adversary and a player. \emph{At round }$t\geq 0$,
\begin{enumerate}
  \item \emph{the player selects a value }$x_t \in [0,1]$.
  \item \emph{the adversary observes the }$x_t$ \emph{and selects a} $b_t \in [0,1]$
  \item \emph{the player receives cost} $f(x_t,b_t)=(1-\alpha)(x_t-b_t)^2 + \alpha(x_t -s)^2$.
\end{enumerate}
where $\alpha,s$ are constants in $[0,1]$. The goal of
the player is to pick $x_t$ based on the history
$(b_0,\ldots,b_{t-1})$ in a way that minimizes her total cost.
Generally, different OCO problems can be defined by a set of functions
$\mcal{F}$ that the adversary chooses from and a feasibility
set $\mcal{K}$ from which the player picks her value (see \cite{Haz16}
for an introduction to the OCO framework).
In our case the feasibility set is $\mcal{K}=[0,1]$ and the set of functions is $\mcal{F}_{\alpha,s} = \{(1-\alpha)(x-b)^2 +
\alpha(x -s)^2,~\text{for all }b \in [0,1]\}$.
As a result, each selection of the
constants $s,\alpha$ lead to a different OCO problem.

\begin{definition}\label{d:OCO_algo}
An algorithm $A$ for the OCO problem with $\mcal{F}_{a,s}$ and
$\mcal{K}=[0,1]$ is a sequence of functions $(A_t)_{t=1}^\infty$ where $A_t:[0,1]^t \mapsto [0,1]$.
\end{definition}

\begin{definition}\label{d:no_regret_algo}
An algorithm $A$ is no-regret for the OCO problem with $\mcal{F}_{a,s}$ and
$\mcal{K}=[0,1]$ if and only if for all sequences $(b_t)_{t=0}^\infty$ that the
adversary may choose, if $x_t = A_t(b_0,\dots,b_{t-1})$ then for all $t$
\[\sum_{\tau=0}^t f(x_\tau,b_\tau)  \leq \min_{x \in [0,1]}\sum_{\tau=0}^t f(x,b_\tau) + o(t) \]
\end{definition}
Informally speaking if the player selects the value
$x_t$ according to a \emph{no-regret algorithm} then
she does not regret for not playing any fixed value no
matter what the choices of the adversary are.
We prove that the \enquote{Follow the Leader} i.e.
$x_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^{t-1}f(x,b_\tau)$
is a no-regret algorithm for all the OCO problems with $\mcal{F}_{\alpha,s}$.
This is formally stated in Theorem~\ref{t:fictitious_noregret}
and is the main result of this section.
\repeattheorem{t:fictitious_noregret}
Returning in our repeated game, it is reasonabe to
assume that each agent $i$ selects $x_i(t)$ according
to no-regret algorithm $A_i$ for the OCO problem with $\mcal{F}_{s_i,\alpha_i}$,
since by Definition~\ref{d:no_regret_algo},
\[\frac{1}{t}\sum_{\tau=0}^t f_i(x_i(\tau),x_{W_i^\tau}(\tau)) \leq
\frac{1}{t}\min_{x \in [0,1]}\sum_{\tau=0}^tf_i(x,x_{W_i^\tau}(\tau)) + \frac{o(t)}{t}\]
The latter means that the time averaged total disagreement cost
that she suffers is similar to the time averaged cost by expressing the
best fixed opinion and this holds no matter the opinions of the
agents that $i$ meets. Theorem~\ref{t:fictitious_noregret}
ensures that \emph{fictitious play} is a no-regret algorithm
for any OCO problem $\mcal{F}_{s_i,\alpha_i}$ and explains why
(\ref{eq:fictitious_play}) is a rational update rule for the agents.

The rest of the section is dedicated to prove Theorem~\ref{t:fictitious_noregret}.
We first prove that a similar strategy that also takes into
account the value $b_t$ admits no-regret (Lemma~\ref{l:y_t}).
Obviously knowing the value $b_t$ before selecting $x_t$
is in direct contrast with the OCO framework, however proving
the no-regret property for this algorithm easily extends to
establishing the no-regret property of fictitious play.
We give the proof of the following simple lemma in
Appendix~\ref{app:s:fictitious_noregret}.

\begin{lemma}\label{l:y_t}
Let $\{b_t\}_{t=0}^\infty$ be an arbitrary sequence with $b_t \in [0,1]$. Let $y_t = \argmin_{x \in [0,1]}\sum_{\tau=0}^tf(x_,b_\tau)$
then for all $t$,
\[
\sum_{\tau=0}^t f(y_\tau,b_\tau) \leq \min_{x \in [0,1]}\sum_{\tau = 0}^tf(x,b_\tau)
\]
\end{lemma}

Now we can understand reason why \enquote{Follow the Leader}
admits no regret. Since the cost incurred by the sequence $y_t$ is at most that
of the best fixed strategy, we can compare the cost incurred by $x_t$ with
that of $y_t$.  However, the functions in $\mcal{F}_{\alpha,s}$ are
Lipschitz-continuous and more specifically quadratic.
These functions are all similar to each other, so the extra
term $f(x_t,b_t)$ that $y_t$ takes into account doesn't change
dramatically the minimum point of the sum. Thus, for each $t$ the
numbers $x_t$ and $y_t$ are quite close and as a result the
difference in their cost must be quite small. The above are
formally stated in Lemma~\ref{l:no_regret_lemma}. For the proof
see Appendix~\ref{app:s:fictitious_noregret}.
\begin{lemma}\label{l:no_regret_lemma}
  For all $t\geq 0$,
  \(
    f(x_t,b_t) \leq f(y_t,b_t) + 2\frac{1-\alpha}{t+1} +
    \frac{(1-\alpha)^2}{(t+1)^2}
  \).
\end{lemma}

Theorem~\ref{t:fictitious_noregret} easily follows since
\begin{align*}
  \sum_{\tau=0}^t f(x_\tau,b_\tau)
  &\leq
  \sum_{\tau=0}^t f(y_\tau,b_\tau) + \sum_{\tau=0}^T 2\frac{1-\alpha}{\tau+1} +
  \sum_{\tau=0}^t \frac{(1-\alpha)^2}{(\tau+1)^2}\\
  &\leq
  \min_{ x \in [0,1]} \sum_{\tau=0}^t f(x,y_\tau) +
  2(1-\alpha)(\log t + 1) + (1-\alpha)\frac{\pi^2}{6}\\
  &\leq
  \min_{ x \in [0,1]} \sum_{\tau=0}^t f(x,y_\tau) + O(\log t)
\end{align*}
