\section{Lower Bound for Graph Oblivious Dynamics}\label{s:lower_bound}
In this section we prove that any no regret dynamics cannot converge much
faster than FTL dynamics (Dynamics~\ref{alg:FTL_dynamics}). %For example, assume
The main result of the section is Theorem~\ref{t:lower_bound}
that establishes slow convergence for the more general class of
\emph{graph oblivious dynamics}

To illustrate why Theorem~\ref{t:lower_bound} serves our purpose, consider the
following no regret dynamics. For every instance $I=(P,s,\alpha)$, each agent $i$ uses
the Online Gradient Descent\footnote{
Online Gradient Descent is an influential no regret algorithm 
proposed by Zinkevic in \cite{Z03} for the general OCO problem, where
the adversary can select any convex function with bounded gradient. The latter
directly implies that it also ensures no regret in our simpler 
OCO problem with $\mcal{F}_{s_i,\alpha_i}$ and $\mcal{K}=[0,1]$.}
to update her opinion,
i.e. $x_i(t+1)= x_i(t) - 1/\sqrt{t}(x_i(t)-(1-\alpha_i)x_{W_i^t}(t)-\alpha_is_i)$
and $x(t)$ is the produced opinion vector. It is easy to see that the
opinion of an agent $i$ at round $t$ only depends on the observed opinion,
$\alpha_i$ and $s_i$.  As a result, Theorem~\ref{t:lower_bound} applies
and establishes the existence of an instance $I$ such that $x(t)$ 
converges slowly to $x^*$.

Using the above reasoning we can use Theorem~\ref{t:lower_bound}
to prove that any no regret dynamics converges slowly.
Let us select for each $(s,\alpha) \in [0,1]^2$ a
no regret algorithm
$A_{s,\alpha}$\footnote{
These $s,\alpha$ are scalars in $[0,1]$ and
should not be confused with the internal opinion vector $s$
and the self confidence vector $\alpha$ of an instance $I=(P,s,\alpha)$.}for the OCO problem with
$\mcal{F}_{s,\alpha}$. This selection defines the following no regret dynamics: 
for every instance $I=(P,s,\alpha)$ each agent $i$ updates her
opinion according to $A_{s_i,\alpha_i}$,
resulting in an opinion vector $x(t)$.
Note that such a selection can be encoded as a graph oblivious update rule.
Specifically, the function $A_t:\{0,1\}^{t+2} \mapsto [0,1]$ is defined
as $A_t(b_0,\ldots,b_{t-1},a,s) = A^t_{a,s}(b_0,\ldots,b_{t-1})$.
As a result, Theorem~\ref{t:lower_bound} applies again.

In Lemma~\ref{l:reduction} we show that any opinion dependent update rule $A$
can be used as an estimator of the parameter $p \in [0,1] $ of a Bernoulli random
variable.
Since we prove Theorem~\ref{t:lower_bound} using a reduction to
an estimation problem, we shall first briefly introduce some definitions and
notation. For simplicity we will restrict the following definitions
of estimators and risk to the case of estimating the parameter $p$ of Bernoulli
random variables.
Given $t$ independent samples from a Bernoulli random variable $B(p)$,
an estimator is an algorithm that takes these samples as input and
outputs an answer in $[0,1]$.
\begin{definition}\label{d:estimator}
  An estimator $\theta=(\theta_t)_{t=1}^{\infty}$
  is a sequence of functions, $\theta_t: \{0,1\}^t\mapsto [0,1]$.
\end{definition}
Perhaps the first estimator that comes to one's mind is the
\emph{sample mean}, that is $\theta_t = \sum_{i=1}^t X_i/t$.
%Of course for an estimator to be efficient we would like its answer to be
%close to the prameter $p$ of the Bernoulli that generated the samples.
To measure the efficiency of an estimator we define the \emph{risk},
which corresponds to the expected error of an estimator.
\begin{definition}\label{d:risk}
  Let $P$ be a Bernoulli distribution with mean $p$ and
  $P^t$ be the corresponding $t$-fold product distribution.
  The risk of an estimator $\theta =(\theta_t)_{t=1}^\infty$ is
  $\Expnew{(X_1,\ldots,X_t) \sim P^t}{|\theta_t(X_1,\ldots,X_t) - p|}$,
  which we will denote by
  $\Expnew{p}{|\theta_t(X_1,\ldots,X_t) - p|}$ or
  $\Expnew{p}{|\theta_t - p|}$ for brevity.
  % where
  % \[
  %   \Expnew{p}{|\theta_t(X_1,\ldots,X_t) - p|}
  %   = \sum_{(y_1,\ldots,y_t)\in\{0,1\}^t}
  %   |\theta_t(y_1,\ldots,y_t) -p|\
  %   p^{\sum_{i=1}^t y_i}\ (1-p)^{t-\sum_{i=1}^t y_i}
  % \]
\end{definition}
%The risk $\Expnew{p}{|\theta_t(Y_1,\ldots,Y_t) - p|}$ is the expected distance
%of the estimated value $\theta_t$ from the parameter $p$, when the
%distribution that generated the samples is $B(p)$.
%For convenience we also write it as $\Expnew{p}{|\theta_t - p|}$.
The risk $\Expnew{p}{|\theta_t - p|}$ quantifies the error rate of
the estimated value $\hat{p} =\theta_t(Y_1,\ldots,Y_t)$ to the
real parameter $p$ as the number of samples $t$ grows.
Since $p$ is unknown, any meaningful estimator $\theta=(\theta_t)_{t=1}^\infty$
must guarantee that $\lim_{t \to \infty} \Expnew{p}{|\theta_t - p|}=0$ for all $p$.
For example, \emph{sample mean} has error rate
$\Expnew{p}{|\theta_t-p|} \leq \frac{1}{2\sqrt{t}}$.

\begin{lemma}\label{l:reduction}
  Let $A$ a graph oblivious update rule such that for all instances $I=(P,s,a)$,
  \[\lim_{t \rightarrow \infty} t^{1+c}
  \Expnew{}{\norm{\infty}{x_A(t)-x^*}}=0.\]
  Then there exists an estimator $\theta_A=(\theta_t^A)_{t=1}^\infty$ such that for all
  $p \in [0,1]$,
  \(\lim_{t \rightarrow \infty}t^{1+c}\Expnew{p}{|\theta_t^A-p|}=0.\)
\end{lemma}
\begin{proof}
  We construct an estimator $\theta_A = (\theta^A_t)_{t=1}^\infty$
  using the update rule $A$. Consider the instance $I_p$ described in
  Figure~\ref{fig:lb_instance}.
  By straightforward computation, we get that the equilibrium point of
  the graph is $x_c^* = p/3, x_1^* = p/6+1/2, x_0^* = p/6$.
  Now consider the opinion vector $x_A(t)$ produced by the update
  rule $A$ for the instance $I_p$. Note that for $t \geq 1$,
  \begin{itemize}
   \item  $x_1^A(t)=A_t(x_c(0),\ldots,x_c(t-1),1/2,1)$
   \item  $x_0^A(t)=A_t(x_c(0),\ldots,x_c(t-1),1/2,0)$
   \item  $x_c^A(t)=A_t(x_{W_c^0}(0),\ldots,x_{W_c^{t-1}}(t-1),1/2,0)$
  \end{itemize}
  The key observation is that the opinion vector $x_A(t)$ is a deterministic function
  of the index sequence $W_c^0,\ldots,W_c^{t-1}$ and does not depend on $p$. Thus,
  we can construct the estimator $\theta_A$ with
  $\theta_t^A(W_c^0,\ldots,W_c^{t-1}) = 3x_c^A(t)$.
  For a given instance $I_p$ the choice of neighbor $W_c^t$ is given by the value of
  the Bernoulli random variable with parameter $p$ ($\Prob{W_c^t=1}=p$). As a result,
  $\Expnew{p}{|\theta_t^A-p|}=
  3\Expnew{}{|x_c^A(t)-p/3|}\leq 3\Expnew{}{\norm{\infty}{x_A(t)-x^*}}$.
  Since for any instance $I_p$, we have that
  $\lim_{t \rightarrow \infty} t^{1+c}\Expnew{}{\norm{\infty}{x_A(t)-x^*}}=0$,
  it follows that $\lim_{t \rightarrow \infty}t^{1+c}\Expnew{p}{|\theta_t^A -p|}=0$
  for all $p \in [0,1]$.
  \vspace{-5mm}
  \begin{figure}
    \centering
    \begin{tikzpicture}
      \begin{scope}[every node/.style={circle,draw}]
        \node (C) at (0,0) [label=below:$a_c \equal 1/2\comma s_c \equal 0$]{C};
        \node (1) at (-4,0)[label=below:$a_1 \equal 1/2\comma s_1 \equal 1$]{1};
        \node (2) at (4,0) [label=below:$a_0 \equal 1/2\comma s_0 \equal 0$]{0};
      \end{scope}
      \begin{scope}[>={Stealth[black]},
        every node/.style={fill=white,circle},
        every edge/.style={draw=black}]
        \path [->] (C) edge[bend right] node {$p$} (1);
        \path [->] (C) edge[bend right] node {$1-p$} (2);
        \path [->] (1) edge[bend right] node {$1$} (C);
        \path [->] (2) edge[bend right] node {$1$} (C);
      \end{scope}
    \end{tikzpicture}
    \vspace{-12mm}
    % \caption{The Lower Bound Instance}
     \caption{}
    \label{fig:lb_instance}
    \vspace{-8mm}
  \end{figure}
  %
\end{proof}
%
In order to prove Theorem~\ref{t:lower_bound} we
just need to prove the following claim.
\begin{claim}\label{cl:fixed_p}
  For any estimator $\theta = (\theta_t)_{t=1}^\infty$
  there exists a $p \in [0,1]$ such that
  \(
    \lim_{t \to \infty} t^{1+c} \Expnew{p}{|\theta_t - p} > 0.
  \)
\end{claim}
The above claim states that for any estimator $\theta=(\theta_t)_{t=1}^\infty$,
we can inspect the functions $\theta_t: \{0,1\}^t \mapsto [0,1]$ and then choose
a $p \in [0,1]$ such that the function $\Expnew{p}{|\theta_t-p|}= \Omega(1/t^{1+c})$. As
a result, we have reduced the construction of a lower bound concerning the round
complexity of a dynamical process to a lower bound concerning the sample complexity of
estimating the parameter $p$ of a Bernoulli distribution. The claim follows by
Lemma~\ref{l:estimation_lower_bound}, which we present at the end of the section.

At this point we should mention that it is known
that $\Omega(1/\eps^2)$ samples are needed to estimate the parameter $p$
of a Bernoulli random variable within additive error $\eps$.
Another well-known result is that taking the average of the samples
is the \emph{best} way to estimate the mean of a Bernoulli random variable.
These results would indicate that the best possible rate of convergence
for an \emph{graph oblivious dynamics} would be $O(1/\sqrt{t})$.
However, there is some fine print in these results which does not allow us
to use them. In order to explain the various limitations of
these methods and results we will briefly discuss some of them.
We remark that this discussion is not needed to understand the proof of
Lemma~\ref{l:estimation_lower_bound}.

The oldest sample complexity lower bound for estimation problems
is the well-known Cramer-Rao inequality.
Let the function $\theta_t: \{0,1\}^t \mapsto [0,1]$ such
that $\Expnew{p}{\theta_t}=p$ for all $p \in [0,1]$, then
\begin{equation}\label{eq:crlb}
  \Expnew{p}{(\theta_t - p)^2} \geq \frac{p(1-p)}{t}.
\end{equation}
Since $\Expnew{p}{|\theta_t - p|}$ can be lower bounded
by $\Expnew{p}{(\theta_t - p)^2}$ we can apply the Cramer-Rao inequality and
prove our claim in the case of \emph{unbiased} estimators, $\Expnew{p}{\theta_t}=p$
for all $t$. Obviously, we need to prove it for any estimator $\theta$, however
this is a first indication that our claim holds.
%Simply setting $p=1/2$ in inequality (\ref{eq:crlb}) would give us a
%$p$ satisfying the requirements of Claim~\ref{cl:fixed_p}.
%The problem with this lower bound is that the assumption that the estimator
%is unbiased is considered rather restrictive and unrealistic even in
%the statistics literature in the sense that many efficient practical
%estimators are not unbiased.  Thus, we would like to get a lower bound
%with minimal assumptions about the estimator.

Sample complexity lower bounds without assumptions about the estimator are usually
given as lower bounds for the \emph{minimax risk}, which was defined
\footnote{
  Although the minimax risk is defined for any estimation problem and loss
  function, for simplicity, we write the minimax risk for estimating the mean
  of a Bernoulli random variable.}
by Wald in \cite{Wal39} as
\[
  \min_{\theta_t} \max_{p\in[0,1]} \Expnew{p}{|\theta_t - p|}.
\]
\noindent Minimax risk captures the idea that after we pick the best possible
algorithm, an adversary inspects it and picks the worst possible
$p \in[0,1]$ to generate the samples that our algorithm will get as input.
The methods of Le'Cam, Fano, and Assouad are well-known
information-theoretic methods to establish lower bounds for the minimax risk.
For more on these methods see \cite{Yu97,Tsy08}.
As we stated before, it is well known that the minimax risk for the
case of estimating the mean of a Bernoulli is lower bounded by
$\Omega(1/\sqrt{t})$ and this lower bound can be established
by Le Cam's method.
In order to show why such results do no work for our purposes
we shall sketch how one would apply Le Cam's method to get this lower bound.
To apply Le Cam's method, one typically chooses two Bernoulli distributions
whose means are far but their total variation distance is small.
Le Cam showed that when two distributions are close in total variation then
given a sequence of samples $X_1, \ldots, X_t$ it is hard to tell whether
these samples were produced by $P_1$ or $P_2$. The hardness of this \emph{testing}
problem implies the hardness of \emph{estimating} the parameters of
a family of distributions.
For our problem the two distributions would be $B(1/2 - 1/\sqrt{t})$
and $B(1/2 + 1/\sqrt{t})$. It is not hard to see that their total variation
distance is at most $O(1/t)$, which implies a lower bound
$\Omega(1/\sqrt{t})$ for the minimax risk. The problem here is that
the parameters of the two distributions depend on the number of
samples $t$. The more samples the algorithm gets to see, the closer
the adversary takes the $2$ distributions to be.
For our problem we would like to \emph{fix} an instance and then argue
about the rate of convergence of any algorithm on this instance.
Namely, having an instance that depends on $t$ does not work for us.

Trying to get a lower bound without assumptions about the estimators
while respecting our need for a fixed (independent of $t$) $p$ we prove
Lemma~\ref{l:estimation_lower_bound}.
In fact, we show something stronger:
for \emph{almost all} $p \in [0,1]$, any estimator $\theta$ cannot
achieve rate $o(1/t^{1+c})$.
% More precisely,  suppose we select $p$ uniformly at
% random in $[0,1]$ and run the estimator $\theta$ with samples from the
% distribution $B(p)$, then with probability $1$ the error rate
% $\Expnew{p}{|\theta_t  - p |} = \Omega(1/t^{1+c})$.
% Although we do not show the sharp lower bound
% $\Omega(1/\sqrt{t})$ we prove that no fast convergence rate
% (see Definition~\ref{d:convergence_rate} is possible and we remark that our
% proof is fairly simple, intuitive, and could be of independent interest.
%
% Finally, while we do not preclude the possibility of similar results to exist
% in the statistics literature,

\begin{lemma}\label{l:estimation_lower_bound}
  Let $\theta=(\theta_t)_{t=1}^\infty$ be a Bernoulli estimator
  with error rate $\Expnew{p}{|\theta_t  - p |}$.
  For any $c>0$, if we select $p$ uniformly at random in $[0,1]$ then
   \(\lim_{t\to \infty} t^{1+c} \Expnew{p}{|\theta_t  - p |} > 0\) with
   probability $1$.
\end{lemma}
\begin{proof}
Since $\theta_t$ is a function from $\{0,1\}^t$ to $[0,1]$, $\theta_t$ can have at most
  $2^t$ different values. Without loss of generality, we assume that
  $\theta_t$ takes the same value $\theta_t(x)$ for all $x \in \{0,1\}^t$
  with the same number of $1$'s. For example,
  $\theta_3(\{1,0,0\})=\theta_3(\{0,1,0\})=\theta_3(\{0,0,1\})$.
  This is due to the fact that for any $p \in[0,1]$,
  %
  \[
    \sum_{0 \leq i \leq t} \sum_{\norm{1}{x} = i} \lp| \theta_t(x) - p \rp|
    p^i (1-p)^{t-i} \geq \sum_{0 \leq i \leq t} \binom{t}{i} \lp|
    \frac{\sum_{\norm{1}{x} = i} \theta_t(x)}{\binom{t}{i}}  - p \rp| p^i
    (1-p)^{t-i}.
  \]
  %
  For any estimator $\theta$ with error rate $\Expnew{p}{|\theta_t  - p |}$ there exists another
  estimator $\theta'$ that satisfies the above property and
  $\Expnew{p}{|\theta_t'  - p |} \leq \Expnew{p}{|\theta_t  - p |}$ for all $p \in [0,1]$.
  Thus, we can assume that $\theta_t$ takes at most $t+1$ different
  values.
  %
  Let $A$ denote the set of $p$ for which the estimator has error
  rate $o(1/t^{1+c})$, that is
  %
  \[
    A= \{p\in [0,1]: \lim_{t \to \infty} t^{1+c}\Expnew{p}{|\theta_t  - p |}=0\}.
  \]
  We show that if we select $p$ uniformly at random in $[0,1]$ then
  $\Prob{p \in A} = 0$.  We also define the set
  \[
    A_k=\{p\in [0,1]: \text{for all }t \geq k,~
    t^{1+c}\Expnew{p}{|\theta_t  - p |}\leq 1\}.
  \]
  %
  Observe that if $p \in A$ then there exists $t_p$ such that
  $p \in A_{t_p}$, meaning that
  $A \subseteq \bigcup_{k=1}^{\infty}A_k$.  As a result,
  \[
    \Prob{p \in A} \leq \Prob {p \in\bigcup_{k=1}^{\infty}A_k} \leq
    \sum_{k=1}^{\infty}\Prob{p \in A_k}.
  \]
  %
  To complete the proof we show that $\Prob{p \in A_k}=0$ for all $k$.
  Notice that $p \in A_k$ implies that for $t \geq k$, the estimator
  $\theta$ must always have a value $\theta_t(i)$ close to $p$.
  Using this intuition we define the set
  \[
    B_k = \{p \in [0,1]: \text{for all
    }t\geq k,~ t^{1+c}\min_{0\leq i \leq t}|\theta_t(i)-p| \leq 1\}.
  \]
  %
  We now show that $A_k \subseteq B_k$.
  Since $p \in A_k$ we have that for all $t\geq k$
  \[
    t^{1 + c} \min_{0 \leq i \leq t} \lp| \theta_t(i) - p \rp|
    \sum_{i=0}^t \binom{t}{i} p^i (1-p)^{t-i}
    \leq
    t^{1 + c} \sum_{i=0}^t \binom{t}{i} \lp| \theta_t(i) - p \rp| p^i (1-p)^{t-i}
    = t^{1+c} \Expnew{p}{|\theta_t  - p |}
    \leq
    1.
  \]
  %
  Thus, $\Prob{p \in A_k} \leq \Prob{p \in B_k}$.
  We write the set $B_k$ as
  \[
    B_k = \bigcap_{t=k}^{\infty}\{p \in [0,1]:~ \min_{0 \leq
      i \leq t} |\theta_t(i)-p|\leq 1/t^{1+c}
    \}.
  \]
  %
  As a result,
  \(
    \Prob{p \in B_k}
    \leq \Prob{\min_{0 \leq i \leq t}|\theta_t(i)-p| \leq 1/t^{1+c} },
    \text{for all } t \geq k.
  \)
  \begin{figure}
    \centering
    \begin{tikzpicture}[scale=7]
      \draw[-, thick] (0,0) -- (1,0);
      \foreach \x/\xtext in {0/0,0.2/$\theta_t(0)$,0.4/$\theta_t(1)$,0.8/$\theta_t(t)$,1}
      \draw[thick] (\x,0.5pt) -- (\x,-0.5pt) node[below] {\xtext};
      %\draw (0.2,0.5pt) node[above] {$c$};
      \draw[(-), thick, black] (0.12,0) -- (0.28,0);
      \draw[(-), thick, black] (0.32,0) -- (0.48,0);
      \draw (0.6,-1.2pt) node[below] {. . . };
      \draw[(-), thick, black] (0.72,0) -- (0.88,0);
      \draw[|-|] (0.12,1pt) -- (0.28,1pt) node[midway, above] {$\frac{2}{t^{1+c}}$};
      \draw[|-|] (0.32,1pt) -- (0.48,1pt) node[midway, above] {$\frac{2}{t^{1+c}}$};
      \draw[|-|] (0.72,1pt) -- (0.88,1pt) node[midway, above] {$\frac{2}{t^{1+c}}$};
      %\draw (-0.25,0) node {At time $t$};
    \end{tikzpicture}
    \caption{Estimator output at time $t$} \label{fig:estimator}
  \end{figure}
  Each value $\theta_t(i)$ \enquote{covers} length $1/t^{1+c}$ from
  its left and right, as shown in Figure~\ref{fig:estimator},
  and since there are at most $t+1$ such values,
  % we have for all $t \geq k$ the set
  % \(
  %   \{
  %   p \in [0,1]:\ {\min_{0 \leq i \leq t} |\theta_t(i)-p| \leq 1/t^{1+c} }
  %   \}
  %   =
  %   \bigcup_{i=0}^t
  %   \lp(
  %   \theta_t(i) - \frac{1}{t^{1+c}},\ \theta_t(i) + \frac{1}{t^{1+c}}
  %   \rp).
  % \)
  % For each interval in this union we have
  % $\Prob{|\theta_t(i)-p| \leq 1/t^{1+c}}\leq 2/t^{1+c}$
  by the union bound we get
  $\Prob{p \in B_k} \leq 2(t+1)/t^{1+c}$, for all $t \geq k$.
  We conclude that $\Prob{p \in B_k} =0$.
\end{proof}

%\begin{remark}\label{r:lower_bound_no regret}
%  The only point that we use that
%  the update rules are \emph{graph oblivious} is in Lemma~\ref{l:reduction}.
%  It is not difficult to see that the reduction still holds if the update rules
%  also depend on the indices of the neighbors that an agent meets.
%  As a result, Theorem~\ref{t:lower_bound} still applies.
%\end{remark}
