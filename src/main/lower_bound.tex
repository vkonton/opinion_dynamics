\section{Lower Bound for Opinion Dependent Dynamics}\label{s:lower_bound}
% In this section we prove that exponential convergence
% cannot be established for any \emph{opinion dependent dynamics}
% produced by an \emph{opinion dependent update rule}. The reason
% that this class of update rules is of particular interest is that
% it includes the dynamics produced by different no-regret OCO algorithms.

% More precisely, consider that for any instance $I=(P,s,\alpha)$, each
% agent $i$ selects her selects her opinion $x_i(t)$ according
% to Zinkevic's online learning algorithm \cite{Z03}, producing
% an opinion vector $x(t)$. This dynamical process can be fairly
% considered natural since Zinkevic's algorithm ensures no-regret
% for any OCO problem with $\mcal{F}_{\alpha_i,s_i}$ and $\mcal{K}=[0,1]$.
% Our result states that there exists an instance $I=(P,s,\alpha)$
% such that the produced vector $x(t)$ does not converge to $x^*$ faster than
% polynomially.

% The reason is fairly simple.


% that if agents update their opinions
% using Zinkevic's algorithm then by specifying the values $\alpha_i,s_i$
% and the opinions that agent $i$ observed until round $t-1$,
% one can deterministically derive the opinion $x_i(t)$ without knowing anything
% else about the instance $I=(P,s,\alpha)$. Meaning that the above dynamical
% process can be produced by suitable \emph{opinion dependent update rule}.



In the previous sections we saw that if each
agent $i$ updates her opinion according to
update rule~(\ref{eq:fictitious_play})
the resulting opinion vector $x(t)$ produced
for an instance $I=(P,s,\alpha)$ converges
to the unique equilibrium point $x^*$ and
ensures no-regret for the OCO problem
with $F_{s_i,\alpha_i}$. In this section we
prove that no-regret dynamics cannot converge much
faster. For example if for every instance
$I=(P,s,\alpha)$ each agent $i$ update her opinion according
to Online Gradient Descent which is a no-regret algorithm
proposed by Zinkevic in \cite{Z03}, i.e.
$x_i(t+1)= x_i(t) - 1/\sqrt{t}(x_i(t)-(1-\alpha_i)x_{W_i^t}(t)-\alpha_is_i)$
and $x(t)$ is the produced opinion vector.
Our results imply that there exists an instance $\mcal{I}$
such that $x(t)$ does not converge to $x^*$ faster than polynomially.
The reason is fairly simple: Let us select
for each $(s,\alpha) \in [0,1]^2$ a no-regret algorithm
$A_{s,\alpha}$\footnote{
These $s,\alpha$ are scalars in $[0,1]$ and
should not be confused with the internal opinion vector $s$
and the self confidence vector $\alpha$ of an instance $I=(P,s,\alpha)$.}for the OCO problem with
$\mcal{F}_{s,\alpha}$. Now for every instance $I=(P,s,\alpha)$ each agent $i$ updates her
opinion according to $A_{s_i,\alpha_i}$,
resulting in an opinion vector $x(t)$.
Theorem~\ref{t:lower_bound} applies since such a selection
can be encoded as an opinion dependent update rule.
Specifically, the function $A_t:\{0,1\}^{t+2} \mapsto [0,1]$ is defined
as $A_t(b_0,\ldots,b_{t-1},a,s) = A^t_{a,s}(b_0,\ldots,b_{t-1})$.
The next theorem is our fundamental result about lower bounds in
\emph{opinion dependent} update rules.
\repeattheorem{t:lower_bound}
% \begin{theorem}
%   Let $A$ be a sequence of functions with corresponding opinion dependent
%   dynamics $x_A(t)$.
%   For any $c>0$, there exists an instance $I$ such that
%   $\Exp{\norm{\infty}{x_A(t)-x^*}} = \Omega(1/t^{1+c})$.
% \end{theorem}

At first we show that any opinion dependent $A$, achieving the previous
convergence rate, can be used as an estimator of the parameter
$p \in [0,1] $ of Bernoulli random variable with the same asymptotic error
rate. This reduction is formally stated in Lemma~\ref{l:reduction}.
Since we prove Theorem~\ref{t:lower_bound} using a reduction to
an estimation problem we shall first briefly introduce some definitions and
notation. For simplicity we will restrict the following definitions
of estimators and risk to the case of estimating the parameter $p$ of Bernoulli
random variables.
Given $t$ independent samples from a Bernoulli random variable $B(p)$
an estimator is an algorithm that takes these samples as inputs and
outputs an answer in $[0,1]$.
\begin{definition}\label{d:estimator}
  An estimator $\theta=(\theta_t)_{t=1}^{\infty}$
  is a sequence of functions, $\theta_t: \{0,1\}^t\mapsto [0,1]$.
\end{definition}
Perhaps the first estimator that comes to one's mind is the
\emph{sample mean}, that is $\theta_t=(1/t) \sum_{i=1}^t X_i$.
Of course for an estimator to be efficient we would like its answer to be
close to the mean $p$ of the Bernoulli that generated the samples.
To measure the efficiency of an estimator we define the \emph{risk}
which corresponds to the expected loss of an estimator.
\begin{definition}\label{d:risk}
  For an estimator $\theta =(\theta_t)_{t=1}^\infty$ we define
  its risk
  $E_p[|\theta_t(X_1,\ldots,X_t) - p|]$,
  where
  \[
    E_p[|\theta_t(X_1,\ldots,X_t) - p|]
    = \sum_{(y_1,\ldots,y_t)\in\{0,1\}^t}
    |\theta_t(y_1,\ldots,y_t) -p|\
    p^{\sum_{i=1}^t y_i}\ (1-p)^{t-\sum_{i=1}^t y_i}
  \]
\end{definition}
The risk $E_p[|\theta_t(Y_1,\ldots,Y_t) - p|]$ is the expected distance
of the estimated value $\theta_t$ from the parameter $p$, when the
distribution that generated the samples is $B(p)$.
For convenience we also write it as $E_p[|\theta_t - p|]$.
The risk quantifies the error rate of
the estimated value $\hat{p} =\theta_t(Y_1,\ldots,Y_t)$ to the
real parameter $p$ as the number of samples $t$ grows.
Since $p$ is unknown, any meaningful estimator $\theta=(\theta_t)_{t=1}^\infty$
must guarantee that $\lim_{t \to \infty} E_p[|\theta_t - p|]=0$ for all $p$.
For example, \emph{sample mean} has error rate
$E_p[|\theta_t-p|] \leq \frac{1}{2\sqrt{t}}$.

We show now that any opinion dependent update rule $A$, achieving the
convergence rate of Theorem~\ref{t:lower_bound}, can be used as a Bernoulli
estimator with asymptotically the same error rate.
\begin{lemma}\label{l:reduction}
  Let $A$ an opinion dependent update rule such that for all instances $I$,
  $\lim \limits_{t \rightarrow \infty} t^{1+c}
  \Expnew{}{\norm{\infty}{x_A(t)-x^*}}=0$.
  Then there exists an estimator $\theta_A=(\theta_t^A)_{t=1}^\infty$ such that for all
  $p \in [0,1]$,
  \(\lim_{t \rightarrow \infty}t^{1+c}E_p[|\theta_t^A-p|]=0.\)
\end{lemma}
\begin{proof}
  We construct an estimator $\theta_A = (\theta^A_t)_{t=1}^\infty$
  using the update rule $A$. Consider the instance $I_p$ described in
  Figure~\ref{fig:lb_instance}.
  By straightforward computation, we get that the equilibrium point of
  the graph is $x_c^* = p/3, x_1^* = p/6+1/2, x_0^* = p/6$.
  Now consider the opinion vector $x_A(t)$ produced by the update
  rule $A$ for the instance $I_p$. Note that for $t \geq 1$,
  \begin{itemize}
   \item  $x_1^A(t)=A_t(x_c(0),\ldots,x_c(t-1),1/2,1)$
   \item  $x_0^A(t)=A_t(x_c(0),\ldots,x_c(t-1),1/2,0)$
   \item  $x_c^A(t)=A_t(x_{W_c^0}(0),\ldots,x_{W_c^{t-1}}(t-1),1/2,1)$
  \end{itemize}
  The key observation is that the opinion vector $x_A(t)$ is a deterministic function
  of the index sequence $W_c^0,\ldots,W_c^{t-1}$ and does not depend on $p$. Thus we
  can construct the estimator $\theta_A$ with $\theta_t^A(W_c^0,\ldots,W_c^{t-1}) = 3x_c^A(t)$.
  For a given instance $I_p$ the choice of neighbor $W_c^t$ is given by the value of
  the Bernoulli random variable with parameter $p$ ($\Prob{W_c^t=1}=p$). As a result,
  $E_p[|\theta_t^A-p|]= 3\Expnew{}{|x_c^A(t)-p/3|}\leq 3\Expnew{}{\norm{\infty}{x_A(t)-x^*}}$.
  Since for any instance $I_p$, we have that $\lim \limits_{t \rightarrow \infty} t^{1+c}\Expnew{}{\norm{\infty}{x_A(t)-x^*}}=0$.
  It follows that $\lim_{t \rightarrow \infty}t^{1+c}E_p[|\theta_t^A -p|]=0$ for all $p \in [0,1]$.
%  For a given $p \in [0,1]$, we construct an instance $I_q$ of
%  Figure~\ref{fig:lb_instance} with $q = p$
%  and simulate the update rule for all three nodes at the same time.
  \vspace{-5mm}
  \begin{figure}
    \centering
    \begin{tikzpicture}
      \begin{scope}[every node/.style={circle,draw}]
        \node (C) at (0,0) [label=below:$a_c \equal 1/2\comma s_c \equal 0$]{C};
        \node (1) at (-4,0)[label=below:$a_1 \equal 1/2\comma s_1 \equal 1$]{1};
        \node (2) at (4,0) [label=below:$a_0 \equal 1/2\comma s_0 \equal 0$]{0};
      \end{scope}
      \begin{scope}[>={Stealth[black]},
        every node/.style={fill=white,circle},
        every edge/.style={draw=black}]
        \path [->] (C) edge[bend right] node {$p$} (1);
        \path [->] (C) edge[bend right] node {$1-p$} (2);
        \path [->] (1) edge[bend right] node {$1$} (C);
        \path [->] (2) edge[bend right] node {$1$} (C);
      \end{scope}
    \end{tikzpicture}
    \vspace{-10mm}
    \caption{The Lower Bound Instance} \label{fig:lb_instance}
  \end{figure}
  %
\end{proof}
%
It follows by Lemma~\ref{l:reduction} that in order to prove Theorem~\ref{t:lower_bound} we
just need to prove the following claim.
\begin{claim}\label{cl:fixed_p}
  For any estimator $\theta = (\theta_t)_{t=1}^\infty$
  there exists a fixed $p \in [0,1]$ such that
  \(
    \lim_{t \to \infty} t^{1+c} \Expnew{p}{|\theta_t - p} > 0.
  \)
\end{claim}
The above claim states that for any estimator $\theta=(\theta_t)_{t=1}^\infty$,
we can inspect the functions $\theta_t: \{0,1\}^t \mapsto [0,1]$ and then choose
a $p \in [0,1]$ such that the function $E_p[|\theta_t-p|]= \Omega(1/t^{1+c})$. As
a result, we have reduced the construction of a lower bound concerning the round
complexity of a dynamical process to a lower bound concerning the sample complexity of
estimating the parameter $p$ of a Bernoulli distribution. The claim follows by
Lemma~\ref{l:estimation_lower_bound} which we present at the end of the section.

At this point we should mention that it is known
that $\Omega(1/\eps^2)$ samples are needed to estimate the parameter $p$
of a Bernoulli random variable within additive error $\eps$.
Another well-known result is that taking the average of the samples
is the \emph{best} way to estimate the mean of a Bernoulli random variable.
These results would indicate that the best possible rate of convergence
for an \emph{opinion dependent dynamics} would be $O(1/\sqrt{t})$.
However, there is some fine print in these results which does not allow us
to use them. In order to explain the various limitations of
these methods and results we will briefly discuss some of them.

Perhaps the oldest sample complexity lower bound for estimation problems
is the well-known Cramer-Rao inequality.
Let the function $\theta_t: \{0,1\}^t \mapsto [0,1]$ such that $E_p[\theta_t]=p$ for
all $p \in [0,1]$, then
\begin{equation}\label{eq:crlb}
  \Expnew{p}{(\theta_t - p)^2} \geq \frac{p(1-p)}{t}.
\end{equation}
Since $\Expnew{p}{|\theta_t - p|}$ can be lower bounded
by $\Expnew{p}{(\theta_t - p)^2}$ we can apply the Cramer-Rao inequality and
prove our claim in the case of \emph{unbiased} estimators, $E_p[\theta_t]=p$
for all $t$. Obviously, we need to prove it for any estimator $\theta$, however
this is a first indication that our claim holds.
%Simply setting $p=1/2$ in inequality (\ref{eq:crlb}) would give us a
%$p$ satisfying the requirements of Claim~\ref{cl:fixed_p}.
%The problem with this lower bound is that the assumption that the estimator
%is unbiased is considered rather restrictive and unrealistic even in
%the statistics literature in the sense that many efficient practical
%estimators are not unbiased.  Thus, we would like to get a lower bound
%with minimal assumptions about the estimator.

To the best of our knowledge, sample complexity lower bounds without
assumptions about the estimator are given as lower bounds for the
\emph{minimax risk}, which was defined
\footnote{
  Although the minimax risk is defined for any estimation problem and loss
  function, for simplicity, we write the minimax risk for estimating the mean
  of a Bernoulli random variable.}
by Wald in \cite{Wal39} as
\[
  \min_{\theta_t} \max_{p\in[0,1]} \Expnew{p}{|\theta_t - p|}.
\]
\noindent Minimax risk captures the idea that after we pick the best possible
algorithm, an adversary inspects it and picks the worst possible
$p \in[0,1]$ to generate the samples that our algorithm will get as input.
The methods of Le'Cam, Fano, and Assouad are well-known
information-theoretic methods to establish lower bounds for the minimax risk.
For more on these methods see \cite{Yu97,Tsy08} and the
very good lecture notes of Duchi, \cite{duchi_stats311}.
As we stated before, it is well known that the minimax risk for the
case of estimating the mean of a Bernoulli is lower bounded by
$\Omega(1/\sqrt{t})$ and this lower bound can be established
by Le Cam's method.
In order to show why such arguments do no work for our purposes
we shall sketch how one would apply Le Cam's method to get this lower bound.
To apply Le Cam's method, one typically chooses two Bernoulli distributions
whose means are far but their total variation distance is small.
Le Cam showed that when two distributions are close in total variation then
given a sequence of samples $X_1, \ldots, X_t$ it is hard to tell whether
these samples were produced by $P_1$ or $P_2$. The hardness of this \emph{testing}
problem implies the hardness of \emph{estimating} the parameters of
a family of distribution.
For our problem the two distributions would be $B(1/2 - 1/\sqrt{t})$
and $B(1/2 + 1/\sqrt{t})$. It is not hard to see that their total variation
distance is at most $O(1/t)$, which implies a lower bound
$\Omega(1/\sqrt{t})$ for the minimax risk. The problem here is that
the parameters of the two distributions depend on the number of
samples $t$. The more samples the algorithm gets to see, the closer
the adversary takes the $2$ distributions to be.
For our problem we would like to \emph{fix} an instance and then argue
about the rate of convergence of any algorithm on this instance.
Namely, having an instance that depends on $t$ does not work for us.

Trying to get a lower bound without assumptions about the estimators
while respecting our need for a fixed (independent of $t$) $p$ we prove
Lemma~\ref{l:estimation_lower_bound}.
In fact, we show something stronger:
for \emph{almost all} $p \in [0,1]$, any estimator $\theta$ cannot
achieve rate $o(1/t^{1+c})$.
More precisely,  suppose we select a $p$ uniformly at
random in $[0,1]$ and run the estimator $\theta$ with samples from the
distribution $B(p)$, then with probability $1$ the error rate $\Expnew{p}{|\theta_t  - p |} \in
\Omega(1/t^{1+c})$. Although we do not show the sharp lower bound
$\Omega(1/\sqrt{t})$ we prove that no exponential convergence rate
is possible and we remark that our proof is fairly simple, intuitive,
and could be of independent interest.
%
% Finally, while we do not preclude the possibility of similar results to exist
% in the statistics literature,

\begin{lemma}\label{l:estimation_lower_bound}
  Let a Bernoulli estimator $\theta=(\theta_t)_{t=1}^\infty$
  with error rate $\Expnew{p}{|\theta_t  - p |}$.
  For any $c>0$, if we select $p$ uniformly at random in $[0,1]$ then
   \(\lim_{t\to \infty} t^{1+c} \Expnew{p}{|\theta_t  - p |} > 0\) with
   probability $1$.
\end{lemma}
\begin{proof}
  Let an estimator $\theta = \{\theta_t\}_{t=1}^{\infty}$, where
  $\theta_t: \{0,1\}^t\mapsto [0,1]$.  The function $\theta_t$ can have at most
  $2^t$ different values. Without loss of generality we assume that
  $\theta_t$ takes the same value $\theta_t(x)$ for all $x \in \{0,1\}^t$
  with the same number of $1$'s. For example,
  $\theta_3(\{1,0,0\})=\theta_3(\{0,1,0\})=\theta_3(\{0,0,1\})$.
  This is due to the fact that for any $p \in[0,1]$,
  %
  \[
    \sum_{0 \leq i \leq t} \sum_{\norm{1}{x} = i} \lp| \theta_t(x) - p \rp|
    p^i (1-p)^{t-i} \geq \sum_{0 \leq i \leq t} \binom{t}{i} \lp|
    \frac{\sum_{\norm{1}{x} = i} \theta_t(x)}{\binom{t}{i}}  - p \rp| p^i
    (1-p)^{t-i}.
  \]
  %
  For any estimator $\theta$ with error rate $\Expnew{p}{|\theta_t  - p |}$ there exists another
  estimator $\theta'$ that satisfies the above property and
  $\Expnew{p}{|\theta_t'  - p |} \leq \Expnew{p}{|\theta_t  - p |}$ for all $p \in [0,1]$.
  Thus we can assume that $\theta_t$ takes at most $t+1$ different
  values.
  %
  Let $A$ denote the set of $p$ for which the estimator has error
  rate $o(1/t^{1+c})$, that is
  %
  \[
    A= \{p\in [0,1]: \lim_{t \to \infty} t^{1+c}\Expnew{p}{|\theta_t  - p |}=0\}
  \]
  We show that if we select $p$ uniformly at random in $[0,1]$ then
  $\Prob{p \in A} = 0$.  We also define the set
  \[
    A_k=\{p\in [0,1]: \text{for all }t \geq k,~ t^{1+c}\Expnew{p}{|\theta_t  - p |}\leq 1\}
  \]
  %
  Observe that if $p \in A$ then there exists $t_p$ such that
  $p \in A_{t_p}$, meaning that
  $A \subseteq \cup_{k=1}^{\infty}A_k$.  As a result,
  \[
    \Prob{p \in A} \leq \Prob {p \in\bigcup_{k=1}^{\infty}A_k} \leq
    \sum_{k=1}^{\infty}\Prob{p \in A_k}
  \]
  %
  To complete the proof we show that $\Prob{p \in A_k}=0$ for all $k$.
  Notice that $p \in A_k$ implies that for $t \geq k$, the estimator
  $\theta$ must always have a value $\theta_t(i)$ close to $p$.
  Using this intuition we define the set
  \[
    B_k = \{p \in [0,1]: \text{for all
    }t\geq k,~ t^{1+c}\min_{0\leq i \leq t}|\theta_t(i)-p| \leq 1\}
  \]
  %
  We now show that $A_k \subseteq B_k$.
  Since $p \in A_k$ we have that for all $t\geq k$
  \[
    t^{1 + c} \min_{0 \leq i \leq t} \lp| \theta_t(i) - p \rp|
    \sum_{i=0}^t \binom{t}{i} p^i (1-p)^{t-i}
    \leq
    t^{1 + c} \sum_{i=0}^t \binom{t}{i} \lp| \theta_t(i) - p \rp| p^i (1-p)^{t-i}
    = t^{1+c} \Expnew{p}{|\theta_t  - p |}
    \leq
    1/2.
  \]
  %
  Thus, $\Prob{p \in A_k} \leq \Prob{p \in B_k}$.
  We write the set $B_k$ as \(
    B_k = \bigcap_{t=k}^{\infty}\{p \in [0,1]:~ \min_{0 \leq
      i \leq t} |\theta_t(i)-p|\leq 1/t^{1+c}
    \}.
  \)
  %
  As a result,
  \(
    \Prob{p \in B_k}
    \leq \Prob{\min_{0 \leq i \leq t}|\theta_t(i)-p| \leq 1/t^{1+c} },
    \text{for all } t \geq k.
  \)
  \begin{figure}
    \centering
    \begin{tikzpicture}[scale=7]
      \draw[-, thick] (0,0) -- (1,0);
      \foreach \x/\xtext in {0/0,0.2/$\theta_t(0)$,0.4/$\theta_t(1)$,0.8/$\theta_t(t)$,1}
      \draw[thick] (\x,0.5pt) -- (\x,-0.5pt) node[below] {\xtext};
      %\draw (0.2,0.5pt) node[above] {$c$};
      \draw[(-), thick, black] (0.12,0) -- (0.28,0);
      \draw[(-), thick, black] (0.32,0) -- (0.48,0);
      \draw (0.6,-1.2pt) node[below] {. . . };
      \draw[(-), thick, black] (0.72,0) -- (0.88,0);
      \draw[|-|] (0.12,1pt) -- (0.28,1pt) node[midway, above] {$\frac{2}{t^{1+c}}$};
      \draw[|-|] (0.32,1pt) -- (0.48,1pt) node[midway, above] {$\frac{2}{t^{1+c}}$};
      \draw[|-|] (0.72,1pt) -- (0.88,1pt) node[midway, above] {$\frac{2}{t^{1+c}}$};
      %\draw (-0.25,0) node {At time $t$};
    \end{tikzpicture}
    \caption{Estimator output at time $t$} \label{fig:estimator}
  \end{figure}
  Each value $\theta_t(i)$ \enquote{covers} length $1/t^{1+c}$ from
  its left and right, as shown in Figure~\ref{fig:estimator},
  and since there are at most $t+1$ such values
  we have for all $t \geq k$ the set
  \(
    \{
    p \in [0,1]:\ {\min_{0 \leq i \leq t} |\theta_t(i)-p| \leq 1/t^{1+c} }
    \}
    =
    \bigcup_{i=0}^t
    \lp(
    \theta_t(i) - \frac{1}{t^{1+c}},\ \theta_t(i) + \frac{1}{t^{1+c}}
    \rp).
  \)
  For each interval in the above union we have that
  $\Prob{|\theta_t(i)-p| \leq 1/t^{1+c}}\leq 2/t^{1+c}$
  and by the union bound we get
  $\Prob{p \in B_k} \leq 2(t+1)/t^{1+c}$, for all $t \geq k$.
  We conclude that $\Prob{p \in B_k} =0$.
\end{proof}

%\begin{remark}\label{r:lower_bound_no-regret}
%  The only point that we use that
%  the update rules are \emph{opinion dependent} is in Lemma~\ref{l:reduction}.
%  It is not difficult to see that the reduction still holds if the update rules
%  also depend on the indices of the neighbors that an agent meets.
%  As a result, Theorem~\ref{t:lower_bound} still applies.
%\end{remark}
