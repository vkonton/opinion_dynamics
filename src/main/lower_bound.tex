\section{Lower Bound for Opinion Dependent Dynamics}\label{s:lower_bound}

In the previous sections we saw that if each
agent $i$ updates her opinion according to
update rule~\ref{eq:fictitious_play}
the resulting opinion vector $x(t)$ produced
for an instance $I=(P,s,\alpha)$ converges
to the unique equilibrium point $x^*$ and
ensures no-regret for the OCO problem 
with $F_{s_i,\alpha_i}$. In this section we investigate whether we
can select
for each $(s,\alpha) \in [0,1]^2$ a no-regret algorithm
$A_{s,\alpha}$ for the OCO problem with 
$\mcal{F}_{s,\alpha}$ such that if for each instance
$I=(P,s,\alpha)$ each agent $i$ updates her 
opinion according to $A_{s_i,\alpha_i}$,
the resulting opinion vector $x(t)$ always converges 
exponentially fast to $x^*$.
We answer this question in the negative, 
because such a selection of no-regret algorithms yields an
opinion dependent update rule. Specifically, the function
$A_t:\{0,1\}^{t+2} \mapsto [0,1]$ is defined
as $A_t(b_0,\ldots,b_{t-1},a,s) = A^t_{a,s}(b_0,\ldots,b_{t-1})$.

%In this section, we prove that for a
%certain class of update rules, the resulting dynamics
%$\{x(t)\}_{t=0}^\infty$ always admits slow convergence (Theorem~\ref{t:lower_bound}).
%As already mentioned, this class is called \emph{opinion dependent}
%and means that each agent $i$ updates her opinion according
%to the opinions that she observed, her $s_i$ and $\alpha_i$.

\begin{definition}[opinion dependent update rule]\label{d:opinion_dependent_dynamics}
An opinion dependent update rule $A$ is a sequence of 
functions $(A_t)_{t=0}^\infty$ where
$A_t: [0,1]^{t+2}\mapsto [0,1]$.
\end{definition}

\begin{definition}[opinion dependent Dynamics]\label{d:opinion_dependent_dynamics}
Let an opinion depedent update rule $A$. For a given instance $I=(P,s,\alpha)$
the rule $A$ produces an opinion depedent dynamics $x_A(t)$ defined as follows:
\begin{itemize}
 \item Initially each agent $i$ has opinion $x_i^A(0)=A_0(s_i,\alpha_i)$
 \item At each round $t\geq 1$, each agent $i$ updates her opinion as follows:
 \[x_i^A(t)=A_t(x_{W_i^0}(0),\dots,x_{W_i^{t-1}}(t-1),\alpha_i,s_i)\]
where $W_i^t$ is the neighbors that $i$ meets at round $t$.
\end{itemize}
\end{definition}
% By Definition~\ref{d:OCO_algo} any collection of no-regret algorithms
% $A_{s,\alpha}$ for all $s,\alpha \in [0,1]^2$, can be encoded as opinion dependent
% update rule ($A_t(b_0,\ldots,b_{t-1},s,\alpha)=A_{s,\alpha}^t(b_0,\dots,b_{t-1})$).
% As a result, a lower bound for the opinion dependent dynamics 
% answers our initial question. An \emph{opinion dependent} update rule $A$
% produces different \emph{opinion dependent dynamics} $x_A(t)$
% for different instances $I=(P,s,\alpha)$, since $I$ determines
% the $s_i,\alpha_i$ for each agent and the probability distribution
% according to which the random meetings take place. 
% An example of an opinion depedent update rule is
% (\ref{eq:fictitious_play}), since $x_i(t) = (1-\alpha_i)\sum_{\tau=0}^{t-1}x_{W_i^\tau}(\tau)/t 
% + \alpha_is_i$. For an instance $I=(P,s,\alpha)$, rule (\ref{eq:fictitious_play}) produces the 
% \emph{opinion dependent dynamics} $x(t)$ whose convergence properties 
% to $x^*$ were studied in Section~\ref{s:fictitious_convergence}. 

% One thing that seems questionable about the above class is that
% $x_i(t)$ does not depend on the indices of the neighbors that $i$ met. Another
% may be the fact that Definition~\ref{d:opinion_dependent_dynamics}
% implies that whenever two agents $i,j$ admit the same self confidence coefficient
% and internal opinion ($\alpha_i=\alpha_j,s_i=s_j$) then they adopt the same update rule.
% The only reason that these case are excluded is only to simplify notation and our
% results extend trivially to them (see Remark~\ref{r:lower_bound_no-regret}).

% We are interested in lower bounds about the convergence rate of
% \emph{opinion dependent dynamics} because these bounds also hold
% for the convergence rate of the \emph{no-regret dynamics} for our repeated game.
% Consider the following collection of no-regret algorithms $\mcal{A}$.
% For each $(s_i,\alpha_i) \in [0,1]^2$ select a no-regret algorithm 
% for the OCO problem with $\mcal{F}_{s_i,\alpha_i}$.
% For each instance $I=(P,s,\alpha)$, each agent $i$ selects
% her opinion $x_i(t)$ according to $A_{s_i,\alpha_i}$, 
% $x_i(t)=A_{s_i,\alpha_i}^t(x_{W_i^0},\ldots,x_{W_i^{t-1}})$.
% Observe that the resulting no-regret dynamics $x_{\mcal{A}}(t)$
% that collection $\mcal{A}$ produces is an opinion dependent dynamics, 
% since we can define a sequence $A_t:[0,1]^{t+2} \mapsto [0,1]$ as
% $A_t(b_0,\ldots,b_{t-1},s_i,\alpha_i)=A^t_{s,\alpha}(b_0,\ldots,b_{t-1})$.

% Assume that each agent $i$ updates $x_i(t)$ according to a no-regret algorithm $A^i$
% for the OCO problem where the adversary selects the functions
% $(1-\alpha_i)(x-b_t)^2 + \alpha_i(x-s_i)^2$. By Definition~\ref{d:no-regret_algorithms},
% we have that $x_i(t)= A^i_t(y_i(0),\ldots,y_i(t-1))$. Notice that
% each agent $i$ selects a no-regret algorithm for a different OCO
% problem defined by her $\alpha_i,s_i$. Obviously two agents $i,j$ with the same
% self confidence coefficient and internal opinion ($\alpha_i=\alpha_j,s_i=s_j$)
% select the algorithms $A_i$ and $A_j$ that admit no-regret for the same OCO problem.
% If we assume that in this case $A_i$,$A_j$ are the same then
% the respective \emph{no-regret dynamics} $\{x(t)\}_{t=1}^\infty$
% are \emph{opinion dependent} since \[x_i(t) = A_t(y_i(0),\ldots,y_i(t-1),\alpha_i,s_i)\]
% where $A_t:\{0,1\}^{t+2} \mapsto [0,1]$. As already mentioned this assumption is removed
% at the end of the section.



% We are interested in lower bounds about the convergence rate of
% \emph{opinion dependent dynamics} because these bounds also hold
% for the convergence rate of the \emph{no-regret dynamics} for our repeated game.
% Assume that each agent $i$ updates $x_i(t)$ according to a no-regret algorithm 
% for the OCO problem where the adversary selects the functions
% $(1-\alpha_i)(x-b_t)^2 + \alpha_i(x-s_i)^2$. We also assume that 
% the OCO algorithm that an agent runs is uniquely determined by his $a_i,s_i$.
% Thus, for given $a,s$ it makes sense to denote $A_{a,s}$ the algorithm 
% run for parameters $a,s$.  
%  By Definition~\ref{d:no_regret_algo},
% we have that $x_i(t)= A^t_{a_i,s_i}(b_0, \ldots, b_{t-1})$. 
% Also, $b_t$ is the neigboring opinion that $i$ learns at time $t$, 
% so $b_t = x_{W_i^t}(t)$.
% % Obviously two agents $i,j$ with the same
% % self confidence coefficient and internal opinion ($\alpha_i=\alpha_j,s_i=s_j$)
% % select the algorithms $A_i$ and $A_j$ that admit no-regret for the same OCO problem.
% Hence, if we define the function  $A_t:\{0,1\}^{t+2} \mapsto [0,1]$ 
% as $A_t(b_0,\ldots,b_{t-1},a,s) = A^t_{a,s}(b_0,\ldots,b_{t-1})$, then
% the respective \emph{no-regret dynamics} $\{x(t)\}_{t=1}^\infty$
% are \emph{opinion dependent} since 
% \[x_i(t) = A_t\lp(x_{W_i^0}(0),\ldots,x_{W_i^{t-1}}(t-1),a_i,s_i\rp)\]

The next theorem is our fundamental result about lower bounds in
\emph{opinion dependent} update rules.
\repeattheorem{t:lower_bound}
% \begin{theorem}
%   Let $A$ be a sequence of functions with corresponding opinion dependent
%   dynamics $x_A(t)$.
%   For any $c>0$, there exists an instance $I$ such that
%   $\Exp{\norm{\infty}{x_A(t)-x^*}} = \Omega(1/t^{1+c})$.
% \end{theorem}

At first we show that any opinion dependent $A$, achieving the previous
convergence rate, can be used as an estimator of the parameter
$p \in [0,1] $ of Bernoulli random variable with the same asymptotic error
rate. This reduction is formally stated in Lemma~\ref{l:reduction}.
Since we prove Theorem~\ref{t:lower_bound} using a reduction to
an estimation problem we shall first briefly introduce some definitions and
notation. For simplicity we will restrict the following definitions
of estimators and risk to the case of estimating the mean of Bernoulli
random variables.
Given $t$ independent samples from a Bernoulli random variable $B(p)$
an estimator is an algorithm that takes these samples as inputs and
outputs an answer in $[0,1]$.
\begin{definition}\label{d:estimator}
  An estimator $\theta=(\theta_t)_{t=1}^{\infty}$
  is a sequence of functions, $\theta_t: \{0,1\}^t\mapsto [0,1]$.
\end{definition}
Perhaps the first estimator that comes to one's mind is the
\emph{sample mean}, that is $\theta_t=(1/t) \sum_{i=1}^t X_i$.
Of course for an estimator to be efficient we would like its answer to be
close to the mean $p$ of the Bernoulli that generated the samples.
To measure the efficiency of an estimator we define the \emph{risk}
which corresponds to the expected loss of an estimator.
\begin{definition}\label{d:risk}
  For an estimator $\theta =(\theta_t)_{t=1}^\infty$ we define
  its risk
  $E_p[|\theta_t(X_1,\ldots,X_t) - p|]$,
  where
  \[
    E_p[|\theta_t(X_1,\ldots,X_t) - p|]
    = \sum_{(y_1,\ldots,y_t)\in\{0,1\}^t}
    |\theta_t(y_1,\ldots,y_t) -p|\
    p^{\sum_{i=1}^t y_i}\ (1-p)^{t-\sum_{i=1}^t y_i}
  \]
\end{definition}
The risk $E_p[|\theta_t(Y_1,\ldots,Y_t) - p|]$ is the expected distance
of the estimated value $\theta_t$ from the parameter $p$, when the
distribution that generated the samples is $B(p)$.
For convenience we also write it as $E_p[|\theta_t - p|]$.
The risk quantifies the error rate of
the estimated value $\hat{p} =\theta_t(Y_1,\ldots,Y_t)$ to the
real parameter $p$ as the number of samples $t$ grows.
Since $p$ is unknown, any meaningful estimator $\theta=(\theta_t)_{t=1}^\infty$
must guarantee that $\lim_{t \to \infty} E_p[|\theta_t - p|]=0$ for all $p$.
For example, \emph{sample mean} has error rate
$E_p[|\theta_t-p|] \leq \frac{1}{2\sqrt{t}}$.

We show now that any opinion dependent update rule $A$, achieving the
convergence rate of Theorem~\ref{t:lower_bound}, can be used as an
estimator of the parameter $p \in [0,1]$ of a Bernoulli random variable with
asymptotically the same error rate.
The reduction is formally stated and in Lemma~\ref{l:reduction}.
\begin{lemma}\label{l:reduction}
  Let $A$ an opinion dependent update rule such that for all instances $I$,
  $\lim \limits_{t \rightarrow \infty} t^{1+c}
  \Expnew{}{\norm{\infty}{x_A(t)-x^*}}=0$.
  Then there exists an estimator $\theta_A=(\theta_t^A)_{t=1}^\infty$ such that for all
  $p \in [0,1]$,
  \[\lim_{t \rightarrow \infty}t^{1+c}E_p[|\theta_t^A-p|]=0\]
\end{lemma}
\begin{proof}
  We sketch here the main idea. For a full proof see Section~\ref{app:s:lower_bound}
  of the Appendix.  For a given $p \in [0,1]$, we construct an instance $I_p$ such that
  $x_c^*=p$ for an agent $c$. Moreover, agent $c$ must
  receive only values $1$ or $0$ with probability $p$ and $1-p$ respectively.
  This can be easily done using the directed star graph $K_{1,2}$.
  The agent corresponding to the center node, $c$, has $\alpha_c = 1/2$ and
  whereas the leaf nodes have $a_{1,2} = 1$, $s_1 = 0$, $s_2 = 1$,
  as shown in Figure~\ref{fig:lb_instance}.
  %
  \begin{figure}\
    \centering

    \begin{tikzpicture}
      \begin{scope}[every node/.style={circle,draw}]
        % \node (C) at (0,0) [label=below:$a_c \equal 0$][label=above:$s_c \equal 0$]{C};
        % \node (1) at (-4,0)[label=below:$a_1 \equal 1$][label=above:$s_1 \equal 0$]{1};
        % \node (2) at (4,0) [label=below:$a_2 \equal 1$][label=above:$s_2 \equal 1$]{2};
        \node (C) at (0,0) [label=below:$a_c \equal 1/2\comma s_c \equal 0$]{C};
        \node (1) at (-4,0)[label=below:$a_1 \equal 1\comma s_1 \equal 0$]{1};
        \node (2) at (4,0) [label=below:$a_2 \equal 1\comma s_2 \equal 1$]{2};
      \end{scope}

      \begin{scope}[>={Stealth[black]},
        every node/.style={fill=white,circle},
        every edge/.style={draw=black}]
        \path [-] (C) edge node {$p$} (1);
        \path [-] (C) edge node {$1-p$} (2);
      \end{scope}
    \end{tikzpicture}
    \caption{The Lower Bound Instance} \label{fig:lb_instance}
  \end{figure}
  %
  It follows that the estimator $\theta_t$ with $\theta_t^A = 2x_c^A(t)$
  has error $E_p[|\theta_t^A-p|]=\frac{1}{2}\Expnew{I_p}{\norm{\infty}{x_A(t)-x^*}}$.
  Meaning that $\lim\limits_{t \rightarrow \infty}t^{1+c}E_p[|\theta_t^A -p|]=0$ for all
  $p \in [0,1]$.
\end{proof}

\noindent It follows by Lemma~\ref{l:reduction} that in order to prove Theorem~\ref{t:lower_bound} we
just need to prove the following claim.

\begin{claim}\label{cl:fixed_p}
  For any estimator $\theta = (\theta_t)_{t=1}^\infty$
  there exists a fixed $p \in [0,1]$ such that
  \[
    \lim_{t \to \infty} t^{1+c} \Expnew{p}{|\theta_t - p} > 0.
  \]
\end{claim}
The above claim states that for any estimator $\theta=(\theta_t)_{t=1}^\infty$,
we can inspect the functions $\theta_t: \{0,1\}^t \mapsto [0,1]$ and then choose
a $p \in [0,1]$ such that the function $E_p[|\theta_t-p|]= \Omega(1/t^{1+c})$. As
a result, we have reduced the construction of a lower bound concerning the round
complexity of a dynamical process to a lower bound concerning the sample complexity of
estimating the parameter $p$ of a Bernoulli distribution.

At this point we should mention that it is known
that $\Omega(1/\eps^2)$ samples are needed to estimate the parameter $p$
of a Bernoulli random variable within additive error $\eps$.
Another well-known result is that taking the average of the samples
is the \emph{best} way to estimate the mean of a Bernoulli random variable.
These results would indicate that the best possible rate of convergence
for an \emph{opinion dependent dynamics} would be $O(1/\sqrt{t})$.
However, there is some fine print in these results which does not allow us
to use them. In order to explain the various limitations of
these methods and results we will briefly discuss some of them.

Before presenting Lemma~\ref{l:estimation_lower_bound} we briefly discuss some
fundamental results concerning sample complexity lower bounds for statistical estimation.
Perhaps the oldest sample complexity lower bound for estimation problems
is the well-known Cramer-Rao inequality.
Let the function $\theta_t: \{0,1\}^t \mapsto [0,1]$ such that $E_p[\theta_t]=p$ for
all $p \in [0,1]$, then
\begin{equation}\label{eq:crlb}
  \Expnew{p}{(\theta_t - p)^2} \geq \frac{p(1-p)}{t}.
\end{equation}
Since $\Expnew{p}{|\theta_t - p|}$ can be lower bounded
by $\Expnew{p}{(\theta_t - p)^2}$ we can apply the Cramer-Rao inequality and
prove Claim~\ref{cl:fixed_p} for the \emph{unbiased} estimators
$\theta =(\theta_t)_{t=1}^\infty$. An estimator $\theta =(\theta_t)_{t=1}^\infty$
is \emph{unbiased} if $E_p[\theta_t]=p$ for all $p \in [0,1]$. Obviously, we need
to prove the claim for any estimator $\theta$, however this is a first indication
that our claim holds.

%Simply setting $p=1/2$ in inequality (\ref{eq:crlb}) would give us a
%$p$ satisfying the requirements of Claim~\ref{cl:fixed_p}.
%The problem with this lower bound is that the assumption that the estimator
%is unbiased is considered rather restrictive and unrealistic even in
%the statistics literature in the sense that many efficient practical
%estimators are not unbiased.  Thus, we would like to get a lower bound
%with minimal assumptions about the estimator.

To the best of our knowledge, sample complexity lower bounds without
assumptions about the estimator are given as lower bounds for the
\emph{minimax risk}, which was defined
\footnote{
  Although the minimax risk is defined for any estimation problem and loss
  function, for simplicity, we write the minimax risk for estimating the mean
  of a Bernoulli random variable.}
by Wald in \cite{Wal39} as
\[
  \min_{\theta_t} \max_{p\in[0,1]} \Expnew{p}{|\theta_t - p|}.
\]

Minimax risk captures the idea that after we pick the best possible
algorithm, an adversary inspects it and picks the worst possible
$p \in[0,1]$ to generate the samples that our algorithm will get as input.
The methods of Le'Cam, Fano, and Assouad are well-known
information-theoretic methods to establish lower bounds for the minimax risk.
For more on these methods see \cite{Yu97,Tsy08} and the
very good lecture notes of Duchi, \cite{duchi_stats311}.
As we stated before, it is well known that the minimax risk for the
case of estimating the mean of a Bernoulli is lower bounded by
$\Omega(1/\sqrt{t})$ and this lower bound can be established
by Le Cam's method.
In order to show why such arguments do no work for our purposes
we shall sketch how one would apply Le Cam's method to get this lower bound.
To apply Le Cam's method, one typically chooses two Bernoulli distributions
whose means are far but their total variation distance is small.
Le Cam showed that when two distributions are close in total variation then
given a sequence of samples $X_1, \ldots, X_t$ it is hard to tell whether
these samples were produced by $P_1$ or $P_2$. The hardness of this \emph{testing}
problem implies the hardness of \emph{estimating} the parameters of
a family of distribution.
For our problem the two distributions would be $B(1/2 - 1/\sqrt{t})$
and $B(1/2 + 1/\sqrt{t})$. It is not hard to see that their total variation
distance is at most $O(1/t)$, which implies a lower bound
$\Omega(1/\sqrt{t})$ for the minimax risk. The problem here is that
the parameters of the two distributions depend on the number of
samples $t$. The more samples the algorithm gets to see, the closer
the adversary takes the $2$ distributions to be.
For our problem we would like to \emph{fix} an instance and then argue
about the rate of convergence of any algorithm on this instance.
Namely, having an instance that depends on $t$ does not work for us.

Trying to get a lower bound without assumptions about the estimators
while respecting our need for a fixed (independent of $t$) $p$ we prove
Lemma~\ref{l:estimation_lower_bound}.
In fact, we show something stronger:
for \emph{almost all} $p \in [0,1]$, any estimator $\theta$ cannot
achieve rate $o(1/t^{1+c})$.
More precisely,  suppose we select a $p$ uniformly at
random in $[0,1]$ and run the estimator $\theta$ with samples from the
distribution $B(p)$, then with probability $1$ the error rate $\Expnew{p}{|\theta_t  - p |} \in
\Omega(1/t^{1+c})$. Although we do not show the sharp lower bound
$\Omega(1/\sqrt{t})$ we prove that no exponential convergence rate
is possible and we remark that our proof is fairly simple, intuitive,
and could be of independent interest.
%
% Finally, while we do not preclude the possibility of similar results to exist
% in the statistics literature,

\begin{lemma}\label{l:estimation_lower_bound}
  Let a Bernoulli estimator $\theta=(\theta_t)_{t=1}^\infty$
  with error rate $\Expnew{p}{|\theta_t  - p |}$.
  For any $c>0$, if we select $p$ uniformly at random in $[0,1]$ then
  \[\Prob{\lim_{t\to \infty} t^{1+c} \Expnew{p}{|\theta_t  - p |}\ > 0}=1\]
\end{lemma}
\begin{proof}
  Let an estimator $\theta = \{\theta_t\}_{t=1}^{\infty}$, where
  $\theta_t: \{0,1\}^t\mapsto [0,1]$.  The function $\theta_t$ can have at most
  $2^t$ different values. Without loss of generality we assume that
  $\theta_t$ takes the same value $\theta_t(x)$ for all $x \in \{0,1\}^t$
  with the same number of $1$'s. For example,
  $\theta_3(\{1,0,0\})=\theta_3(\{0,1,0\})=\theta_3(\{0,0,1\})$.
  This is due to the fact that for any $p \in[0,1]$,
  %
  \[
    \sum_{0 \leq i \leq t} \sum_{\norm{1}{x} = i} \lp| \theta_t(x) - p \rp|
    p^i (1-p)^{t-i} \geq \sum_{0 \leq i \leq t} \binom{t}{i} \lp|
    \frac{\sum_{\norm{1}{x} = i} \theta_t(x)}{\binom{t}{i}}  - p \rp| p^i
    (1-p)^{t-i}.
  \]
  %
  For any estimator $\theta$ with error rate $\Expnew{p}{|\theta_t  - p |}$ there exists another
  estimator $\theta'$ that satisfies the above property and
  $\Expnew{p}{|\theta_t'  - p |} \leq \Expnew{p}{|\theta_t  - p |}$ for all $p \in [0,1]$.
  Thus we can assume that $\theta_t$ takes at most $t+1$ different
  values.
  %
  Let $A$ denote the set of $p$ for which the estimator has error
  rate $o(1/t^{1+c})$, that is
  %
  \[
    A= \{p\in [0,1]: \lim_{t \to \infty} t^{1+c}\Expnew{p}{|\theta_t  - p |}=0\}
  \]
  We show that if we select $p$ uniformly at random in $[0,1]$ then
  $\Prob{p \in A} = 0$.  We also define the set
  \[
    A_k=\{p\in [0,1]: \text{for all }t \geq k,~ t^{1+c}\Expnew{p}{|\theta_t  - p |}\leq 1\}
  \]
  %
  Observe that if $p \in A$ then there exists $t_p$ such that
  $p \in A_{t_p}$, meaning that
  $A \subseteq \cup_{k=1}^{\infty}A_k$.  As a result,
  \[
    \Prob{p \in A} \leq \Prob {p \in\bigcup_{k=1}^{\infty}A_k} \leq
    \sum_{k=1}^{\infty}\Prob{p \in A_k}
  \]
  %
  To complete the proof we show that $\Prob{p \in A_k}=0$ for all $k$.
  Notice that $p \in A_k$ implies that for $t \geq k$, the estimator
  $\theta$ must always have a value $\theta_t(i)$ close to $p$.
  Using this intuition we define the set
  \[
    B_k = \{p \in [0,1]: \text{for all
    }t\geq k,~ t^{1+c}\min_{0\leq i \leq t}|\theta_t(i)-p| \leq 1\}
  \]
  %
  We now show that $A_k \subseteq B_k$.
  Since $p \in A_k$ we have that for all $t\geq k$
  \[
    t^{1 + c} \min_{0 \leq i \leq t} \lp| \theta_t(i) - p \rp|
    \sum_{i=0}^t \binom{t}{i} p^i (1-p)^{t-i}
    \leq
    t^{1 + c} \sum_{i=0}^t \binom{t}{i} \lp| \theta_t(i) - p \rp| p^i (1-p)^{t-i}
    = t^{1+c} \Expnew{p}{|\theta_t  - p |}
    \leq
    1/2.
  \]
  %
  Thus, $\Prob{p \in A_k} \leq \Prob{p \in B_k}$.
  At first we write the set $B_k$ in the following equivalent form
  \[
    B_k = \cap_{t=k}^{\infty}\{p \in [0,1]:~ \min_{0 \leq
      i \leq t} |\theta_t(i)-p|\leq 1/t^{1+c}
    \}
  \]
  %
  As a result,
  \[
    \Prob{p \in B_k}
    \leq \Prob{\min_{0 \leq i \leq t}|\theta_t(i)-p| \leq 1/t^{1+c} },
    \text{for all } t \geq k
  \]
  \begin{figure}
    \centering
    \begin{tikzpicture}[scale=7]
      \draw[-, thick] (0,0) -- (1,0);
      \foreach \x/\xtext in {0/0,0.2/$\theta_t(0)$,0.4/$\theta_t(1)$,0.8/$\theta_t(t)$,1}
      \draw[thick] (\x,0.5pt) -- (\x,-0.5pt) node[below] {\xtext};
      %\draw (0.2,0.5pt) node[above] {$c$};
      \draw[(-), thick, black] (0.12,0) -- (0.28,0);
      \draw[(-), thick, black] (0.32,0) -- (0.48,0);
      \draw (0.6,-1.2pt) node[below] {. . . };
      \draw[(-), thick, black] (0.72,0) -- (0.88,0);
      \draw[|-|] (0.12,1pt) -- (0.28,1pt) node[midway, above] {$\frac{2}{t^{1+c}}$};
      \draw[|-|] (0.32,1pt) -- (0.48,1pt) node[midway, above] {$\frac{2}{t^{1+c}}$};
      \draw[|-|] (0.72,1pt) -- (0.88,1pt) node[midway, above] {$\frac{2}{t^{1+c}}$};
      %\draw (-0.25,0) node {At time $t$};
    \end{tikzpicture}
    \caption{Estimator output at time $t$} \label{fig:estimator}
  \end{figure}
  Each value $\theta_t(i)$ \enquote{covers} length $1/t^{1+c}$ from
  its left and right, as shown in Figure~\ref{fig:estimator},
  and since there are at most $t+1$ such values
  we have for all $t \geq k$ the set
  \[
    \{
    p \in [0,1]\ :\ {\min_{0 \leq i \leq t} |\theta_t(i)-p| \leq 1/t^{1+c} }
    \}
    =
    \bigcup_{i=0}^t
    \lp(
    \theta_t(i) - \frac{1}{t^{1+c}},\ \theta_t(i) + \frac{1}{t^{1+c}}
    \rp).
  \]
  For each interval in the above union we have that
  $\Prob{|\theta_t(i)-p| \leq 1/t^{1+c}}\leq 2/t^{1+c}$
  and by the union bound we get
  $\Prob{p \in B_k} \leq 2(t+1)/t^{1+c}$, for all $t \geq k$.
  We conclude that $\Prob{p \in B_k} =0$.
\end{proof}

\begin{remark}\label{r:lower_bound_no-regret}
  The only point that we use that
  the update rules are \emph{opinion dependent} is in Lemma~\ref{l:reduction}.
  It is not diffcult to see that the reduction still holds if the update rules
  also depend on the indices of the neighbors that an agent meets or
  if agents $i,j$ with $s_i=s_j$ adopt different update rules.
  As a result, Theorem~\ref{t:lower_bound} still applies.
\end{remark}
