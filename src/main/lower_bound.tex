\section{Lower Bound for no-regret Dynamics}
As we have already discussed for any fixed instance $I$ with $\rho\geq 1/2$, \emph{fictitious play} acheives convergence rate $\Expnew{I}{\norm{\infty}{x(t)-x^*}}=\bigOh{1/\sqrt{t}}$, this rate is outperformed by
the rate of the original \emph{FJ model} convergence rate $\Expnew{I}{\norm{\infty}{x(t)-x^*}}=\bigOh{1/2^{t}}$. An interesting question is whether this gap
is due to the limited information exchange between the agents in the random payoff variant or can be reduced with another no-regret algorithm $A$.

\begin{claim}\label{claim}
Let $A$ a no-regret algorithm and let $x_A(t)$ the opinion vector defined in \ref{alg:no_regret_dynamics}.
For any $c>0$, there exists an instance $I_A$ such that $\Expnew{I}{\norm{\infty}{x_A(t)-x^*}} = \Omega(1/t^{1+c})$.
\end{claim}

%\noindent The above claim states that rationality in selfish agents comes with the price of slow convergence to the equilibrium point.
%Although we were not able to fully prove it, we present two results that indicate that such an algorithm $A$ is highly unlike to exists.
%We leave the full proof as an interesting open problem.We leave the full proof as an interesting open problem.\\

The above claim states that rationality in selfish agents comes with the price of slow convergence to the equilibrium point.
Although we were not able to provide a full proof of Claim~\ref{claim}, our results indicate that
the existence of an algorithm $A$, not satisfying it, is highly unlike. Based on lower bound techniques develloped
in the statistics literature, we first prove the following lower bound for any estimator $\hat{\theta}$ of Bernoulli distributions.

\begin{theorem}\label{t:integral}
Let a Bernoulli estimator $\hat{\theta}$ with error rate $R_p(t)$. Then, for all $[a,b] \subseteq [0,1]$,
\[ \lim_{t \to \infty}t^{1+c} \int_{a}^{b}R_p(t)dp = +\infty\]
\end{theorem}

\noindent Now suppose that there exists a no-regret algorithm $A$ not statisfying Claim~\ref{claim} i.e. for all instances $I$, \[\lim_{t\rightarrow \infty}t^{1+c}\Expnew{I}{\norm{\infty}{x_A(t)-x^*}}\]
In Theorem~\ref{t:reduction} we show that $A$ can be used as an estimator $\hat{\theta_A}$ for Bernoulli distributions with risk asymptotically the same.

\begin{theorem}\label{t:reduction}
Let the no-regret algorithm $A$ such that for all instances $I$, $\lim_{t \rightarrow \infty} t^{1+c} \Expnew{I}{\norm{\infty}{x_A(t)-x^*}}=0$.
Let $\hat{\theta_A}$ the estimator constructed from $A$. Then for all  $p \in Q[0,1]$, \[\lim_{t \rightarrow \infty}t^{1+c}R_p(t)=0\]
\end{theorem}

\noindent As a result, the existence of such an algorithm $A$ implies the existence of an estimator $\hat{\theta_A}$
whose risk $R_p(t)$ satisfy the following statements:

\begin{itemize}
 \item for all $p \in Q([0,1]),~ \lim_{t \rightarrow \infty}t^{1+c}R_p(t)=0$
 \item for all $[a,b] \subseteq [0,1]$, $\lim_{t \to \infty}t^{1+c} \int_{a}^{b}R_p(t)dp = +\infty$
\end{itemize}


The above two statements are controversial for any function $R_p(t)$ that satisfies minimal technical
assumptions (e.g. $R_p(t)=f(p)g(t)$), meaning that the first statement is violated (the second is ensured
by Theorem~\ref{t:integral}). Unfortunately one can construct degenerate functions $R_p(t)$
that simultaneously satisfy the above two statements. For example $R_p(t)=\Pi_{i=1}^t(p-q_i)^2$,
where the sequence $\{q_i\}_{i=1}^{\infty}$ is an enumeration of the rationals in $[0,1]$. Although
the existence of such function means that we cannot derive a full proof of Claim~\ref{claim}, their
exremely degenerate form indicates that such a risk rate $R_p(t)$ cannot be obtained by any
\emph{reasonable} estimator $\hat{\theta_A}$. Implying that Claim~$\ref{claim}$ must be satisfied by
$A$.

\subsection{Proof of Theorem \ref{t:reduction}}
\noindent In the following Lemma we show how we can use algorithm $A$ to construct an estimator $\hat{\theta_A}$ for Bernoulli distributions.
\begin{lemma}\label{l:reduction}
For any algorithm $A$, we can construct a Bernoulli estimator $\hat{\theta_A}$ such that for all $p \in Q([0,1])$, there exists an instance $I_p$ such that $$R_p(t) \leq 2E_{I_p}[||x_A(t)-x^*||_{\infty}]$$
\end{lemma}

\begin{proof}
At first we remind that an estimator $\hat{\theta}$ is a sequence of fuctions $\{\hat{\theta_t}\}_{t=1}^{\infty}$, where $\theta_t:~\{0,1\}^t\mapsto [0,1]$.  We construct such a sequence using the algorithm $A$.
We also remind that when an agent $i$ runs algorithm $A$, she selects $x_i(t)$ according to the cost functions $\{C_i^{\tau}\}$ that she has already reveived \[x_i(t)=A_t(C_i^1,\ldots,C_i^{t-1})\]

\noindent Consider an agent $i$ with $a_i=1$ and $s_i=0$ that runs $A$. Then $C_i^t(x)=x^2$ for all $t$ and $x_i(t)=A_t(x^2,\ldots,x^2)$.
The latter means that $x_i(t)$ only depends on $t$, $x_i(t)=h_0(t)$. Equivalently, if $a_i=1$ and $s_i=1$ then $x_i(t)=A_t((1-x)^2,\ldots,(1-x)^2)$ and $x_i(t)=h_1(t)$. Finally, consider an agent $i$ with $a_i=1/2$ and $s_i=0$. In this case $C_i^t = \frac{1}{2}x^2 + \frac{1}{2}(x-y_t)^2$, where $y_t \in [0,1]$ is the opinion of the neighbor $j\in N_i$ that $i$ met at
round $t$. As a result, $x_i(t)=A_t(\frac{1}{2}x^2+\frac{1}{2}(x-y_1)^2,\ldots,\frac{1}{2}x^2+\frac{1}{2}(x-y_{t-1})^2)=f_t(y_1,\ldots,y_{t-1})$. The estimator $\hat{\theta_A}$ is the following sequences $\{\hat{\theta_{t}}\}_{t=1}^{\infty}$ \[\hat{\theta_t}(Y_1,\ldots,Y_t) = \frac{1}{2}f_{t+1}(h_{Y_1}(1),\ldots,h_{Y_t}(t)) \]

\noindent Observe that $\hat{\theta_t}: \{0,1\}^t \mapsto [0,1]$ meaning that $\hat{\theta_A}$ is a valid estimator for Bernoulli distributions.\\

\noindent Now for any $p \in Q([0,1])$, we construct an appropriate instance $I_p$ s.t. $R_p(t)=\Expnew{p}{|\hat{\theta_t} - p|} \leq 2\Expnew{I_p}{\norm{\infty}{x^t-x^*}}$.
For $p=\frac{k}{n}$ consider the following instance $I_p$ with $n+1$ agents:
\begin{itemize}
 \item A central agent with $s_c=0$ and $a_c=1/2$.
 \item Directed edges from the central agent to all the other agents.
 \item $k$ agents with $s_i=0$ and $a_i=1$
 \item $n-k$ agents with $s_i=1$ and $a_i=1$
 \end{itemize}
We just need to prove that in $I_p$, $\Expnew{p}{|\hat{\theta_t}-p|} \leq 2\Expnew{I_p}{\norm{\infty}{x^t-x^*}}$. Notice that $x^*_c=\frac{p}{2}$ and $x^*_i=s_i$ if $i\neq c$. .\\
At round $t$, if the oracle returns to the center agent the value $h_1(t)$ of a $1$-agent, then $Y_t=1$ otherwise $Y_t=0$. As a result, $\Prob{Y_t=1}=p$ and
\begin{align*}
 \Expnew{I_p}{\norm{\infty}{x^t-x^*}} &\geq \Expnew{I_p}{||x^t_c-x^*_c||}\\
 &= \Expnew{p}{|\frac{\widehat{\theta_t}}{2}-\frac{p}{2}}] = R_p(t)
\end{align*}
\end{proof}

\begin{theorem}
Let the no-regret algorithm $A$ such that for all instances $I$, $\lim_{t \rightarrow \infty} t^{1+c} \Expnew{I}{\norm{\infty}{x_A(t)-x^*}}=0$.
Let $\hat{\theta_A}$ the estimator constructed from $A$. Then for all  $p \in Q[0,1]$, \[\lim_{t \rightarrow \infty}t^{1+c}R_p(t)=0\]
\end{theorem}
The proof follows by direct application of the Lemma \ref{l:reduction}

\subsection{Proof of Theorem \ref{t:integral}}
Our proof builts on a standard technique in the statistics literature, for proving lower bounds on the risk of estimators.
This technique reduces the estimation problem to the canonical hypothesis testing problem for which information
theoretic lower bounds exist. Due to lack of space we are not able to explain this very interesting reduction. The
interested reader can find a detailed explanation in \cite{}[Duchi]. We present a major Definition and Lemma
of this technique in the case of Bernoulli distribution i.e. Definition~\ref{d:2packing} and Lemma~\ref{l:fano} that are
the starting point of our proof.

\begin{definition}\label{d:2packing}
A finite family of Bernoulli distribution $\mathcal{P}=\{B(p_1),\ldots,B(p_n)\}$ is called a $2\delta$-\emph{packing} if $|p_i-p_j|>2\delta$ for all $i \neq j$.
\end{definition}



\begin{lemma}[Fano's Inequality]\label{l:fano}
Let $\hat{\theta}$ an estimator for the parameter $p$ of a Bernoulli distribution and $\mathcal{P}=\{B(p_1),\ldots,B(p_n)\}$ a $2\delta$-\emph{packing}.
Then
\[\frac{1}{n} \sum_{i=1}^n R_{p_i}(t) \geq \delta \lp(1-\frac{\log 2}{\log n} - \frac{t}{n^2\log n}\sum_{i,j}D_{kl}\lp(B(p_i),B(p_j)\rp)\rp)\]% \frac{t\sum_{i,j}D_{kl}(B(p_i),B(p_j))+\log 2 n^2}{n^2\log n}\rp)\]
\end{lemma}



\noindent At first for a fixed interval $[a,b]$ we select an appropriate $2\delta$-packing $\P$ and apply Lemma~\ref{l:fano}.
The most challenging part is to appropriately select $\P$ such that the $\sum_{i,j}D_{kl}\lp(B(p_i),B(p_j)\rp)$ can be
efficiently upper bounded. In our case $\P$ consists of $t$ different distributions and $\delta=\frac{b-a}{2t}$. For further details
one can see the proof of the Lemma~\ref{l:fano_application}.

\begin{lemma}\label{l:fano_application}
For the interval $[a,b] \subseteq [0,1]$. Let the $2\delta$-packing $\P=\{B(p_1),\ldots,B(p_t)\}$ with $p_i = a+\frac{b-a}{2t} + i \frac{b-a}{t}$. Then,
\[\frac{1}{t} \sum_{i=1}^t R_{p_i}(t) \geq \frac{b-a}{2t} \lp(c_1 - \frac{c_2}{t}\rp)\]
where $c_1, c_2 >0$ and $c_1 > \frac{1}{2}$.
\end{lemma}

\noindent Now our goal is to upper bound the quantity $\frac{1}{t} \sum_{i=1}^{t} R_{p_i}(t)$ by the quantity $\int_{a}^b R_p(t)dp$.
Our proof if based in the following simple idea. Assume that we draw $t$ samples from the $B(p)$ and $t$ samples
from the $B(q)$, where $|p-q|\leq \frac{1}{t}$. These two product distribution are approximately the same since
we only $t$ samples are drawn and their parameters differ only by $\frac{1}{t}$. At the same time we have that
$|\hat{\theta_t}-p| \simeq |\hat{\theta_t}-q|$ meaning that $R_p(t)\simeq R_q(t)$ ($R_p(t)=\Expnew{p}{|\hat{\theta_t}-p|}$).
The above intuition is formallized and proved in Lemma~\ref{l:cauchy_schwarz}.

\begin{lemma}\label{l:cauchy_schwarz}
For every $p\in [p_i -\frac{b-a}{2t},p_i + \frac{b-a}{2t}]$,
%\[\Exp{\lp|\hat{\theta}^t-p_i\rp|}_{p_i} \leq c_a^b\Exp{\lp|\hat{\theta}^t-p\rp|}_p + \lp|p - p_i\rp|\]
\[R_{p_i}(t) \leq C_a^b R_{p}(t) +  |p-p_i|\]
where $C_a^b$ is a constant that depends on $a,b$.
 \end{lemma}
\noindent Using Lemma~\ref{l:cauchy_schwarz}, we can easily upper bound $\frac{1}{t} \sum_{i=1}^{t} R_{p_i}(t)$ by $\int_{a}^b R_p(t)dp$.

\begin{lemma}\label{l:upper}
Let an interval $[a,b] \subseteq [0,1]$ and the parameters $p_i = a+\frac{b-a}{2t} + i \frac{b-a}{t}$. Then,
%\[ \frac{1}{t} \sum_{i=0}^{t-1} E_i \leq \frac{c_a^b}{b-a} \int_a^b \Exp{\lp|\hat{\theta}^t-p\rp|} dp + \frac{b-a}{4t}\]
\[ \frac{1}{t} \sum_{i=1}^{t} R_{p_i}(t) \leq \frac{C_a^b}{b-a} \int_a^b R_p(t) dp + \frac{b-a}{4t}\]
\end{lemma}
\noindent Now we are ready to prove Theorem~\ref{t:integral}.

\begin{theorem}
Let a Bernoulli estimator $\hat{\theta}$ with error rate $R_p(t)$. Then, for all $[a,b] \subseteq [0,1]$ and $c>0$,
\[ \lim_{t \to \infty}t^{1+c} \int_{a}^{b}R_p(t)dp = +\infty\]
\end{theorem}
\begin{proof}
By combining Lemmas~(\ref{l:fano_application}), ~(\ref{l:upper}) and multiplying by $t^{1+c}$ we get
%\[\frac{c_a^b}{b-a} \int_a^b \Exp{\lp|\hat{\theta}^t-p\rp|}_p dp \geq \frac{b-a}{2t}\lp(\lp(c_1 - \frac{1}{2}\rp)- \frac{c_2}{t}\rp)\]
%By :
\begin{equation*}\label{eq:final}
\frac{c_a^b}{b-a}t^{1+c} \int_a^b \Expnew{p}{\lp|\hat{\theta}^t-p\rp|}dp \geq \frac{b-a}{2}t^c\lp(\lp(c_1 - \frac{1}{2}\rp)- \frac{c_2}{t}\rp)
\end{equation*}
The coefficient of $t^c$ in the right hand side of~(\ref{eq:final}) is positive, so $\lim_{t\to\infty} t^{1+c}\int_{a}^b R_p(t)dp = +\infty$
\end{proof}
