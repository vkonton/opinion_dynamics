\section{Graph Aware Protocol - Frequencies}
Let $P$ be a discrete distribution over $[d]$, and let
$S_1, \ldots, S_t$ be $t$ i.i.d samples drawn from $P$, i.e.
$(S_1, \ldots, S_t) \sim P^t$.
The empirical distribution $\hat{P}_t$ is the following estimator of
the density of $P$.
\begin{equation}\label{eq:empirical_distribution}
  \hat{P}_N(A) = \frac{\sum_{i=1}^N \1{S_i \in A}}{N},
\end{equation}
where $A \subseteq [d]$. In words, $\hat{P}_N$ simply counts
how many times the value $i$ appeared in the samples $S_1, \ldots S_N$.
We will use the following version of the classical Vapnik and
Chervonenkis inequality.
\begin{lemma}\label{l:vc_inequality}
  Let $\mcal{A}$ be a collection of subsets of $\{0,\ldots, d\}$ and
  Let $S_{\mcal{A}}(N)$ be the Vapnik-Chervonenkis shatter coefficient, defined
  by
  \[
    S_{\mcal{A}(N)} = \max_{x_1,\ldots, x_t \in [d]}
    \lp| \{ \{x_1, \ldots, x_t\} \cap A : A \in \mcal{A} \} \rp|.
  \]
  Then
  \[
    \Expnew{P^N}{\max_{A \in \mcal{A}} \lp| \hat{P}_t(A) - P(A) \rp|}
    \leq 2 \sqrt{\frac{\log{2 S_{\mcal{A}}(N)}}{N}}
  \]
\end{lemma}

An interesting question is whether $\mrm{poly}(1/\eps)$ rounds
are necessary when protocols have access to the unlabeled oracle.
We show that this is not true by showing a protocol
that needs $\ln^2(1/\eps)$ rounds to be within error $\eps$ from
the solution $x^*$. Our protocol is graph-aware in the sense that
it is allowed to depend on the graph $G$. Therefore, we show that
without any constraints on the protocols it is hard to prove strong
lower bounds for our problem. The problem in this case is that
all agents could agree to stop updating their public opinions for
enough rounds so that everybody learns, with high probability, exactly
the average of the opinions of their neighbors. Having the exact
average they can perform a step of the original update of the FJ-model
which results in a similar and fast convergence rate.
Of course if the opinions of the neighbors are all different we can still
use an argument similar to that of the protocol of section COUPONS COLLECTOR.
Since some neighbors may share opinions we have to come up with a different
solution for this problem. Let $L \leq d$ be the number of different
opinions and $k_i$ be the number of neighbors who have opinion $\xi_i$.
Then the average of the opinions of the neighbors is
$(\sum_{j = 1}^L k_i \xi_i)/d_i$. Instead of using the average like
we did in section SAMPLE MEAN, we will instead find \emph{exactly}
the frequencies $k_i$ and then compute their average exactly with high
probability.
We next describe our protocol. Since it is the same for all agents,
For simplicity we describe it for agent $i$.
\begin{algorithm}
  \caption{Graph Aware Update Rule}
  \label{alg:frequencies}
  \begin{algorithmic}[1]
    \State $x_i(0) \gets s_i$, $t \gets 0$.
    \For{$l = 1, \ldots, \infty$}
    \State $\eps \gets 1/2^l$.
    \For{$t = 1, \ldots,\ln(1/\eps))$}
    \State Keep a map $h$ from $[0,1]$ to $[d_i]$
    and array of counters $A$ of length $d_i$ and an array $B$ of length
    $M_1$.
    \State $M_1 = O(\ln(1/\eps))$,
    $M_2 = O(d^2 \ln(d))$

    \For{$j = 1, \ldots, M_1$}

    \For{$k = 1, \ldots, M_2$}
    \State Call the unlabeled oracle $O_i^u(t)$ to get an opinion $X_i$.
    \If{$X_i$ is not in $h$}
      \State Insert $X_i$ to $h$.
    \Else
      \State $A(h(X_i)) \gets A(h(X_i)) +1$.
    \EndIf
  \State $t \gets t+1$
\EndFor
  \State Divide all entries of $A$ by $M_2$.  \label{alg:line:counters}
  \State Round all entries of $h$ to the closest multiple of $1/d$.
  \State $B(j) = \sum_{i=1}^{d_i} A(i)$.
\EndFor
\State $x_i(t) \gets \maj_{j} B(j)$.
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{theorem}
  The update rule~\ref{alg:frequencies} for $a > 1/2$ achieves
  convergence rate
  \[
    \Exp{\norm{\infty}{x_t- x^*}}
    \leq C \me^{-\sqrt{t}/(d\sqrt{\log{d}})},
\]
where $C$ is a universal constant.
\end{theorem}
\begin{proof}
  According to the update rule~\ref{alg:frequencies}
  all agents fix their opinions $x_i(t)$ for $M_1 \times M_2$ rounds.
  To estimate the sum of the opinions each agent estimates the
  frequencies $k_j/d_i$. Since the neighbors have at most $d_i$
  different opinions we can think of the opinions as natural
  numbers in $[d_i]$.  The oracle returns the opinion
  of a random neighbor and therefore the samples $X_i$ returned by
  the oracle $O_i^u$ are drawn from a discrete distribution
  $P$ supported on $[d_i]$.
  The probability $P(j)$ of the $j$-th opinion is the number of neighbors
  having this opinion $k_j/d_i$.
  To learn the probabilities $P(j)$ using samples from $P$.
  Letting $\mcal{A} = \{\{1\},\{2\},\ldots, \{d_i\}\}$ we have from
  Lemma~\ref{l:vc_inequality} that
  \[
    \Expnew{P^m}{\max_{j \in [d_i]} \lp| \hat{P}_m(j) - P(j) \rp|}
    \leq 2 \sqrt{\frac{\log{2 d_i}}{m}},
  \]
  since $S_{\mcal{A}} \leq d_i$. Therefore, an agent can draw
  $m = 100 d^2 \log(2 d)$ to learn the frequencies $k_j/d_i$
  within expected error $1/(5 d)$.
  Notice now that the array $A$ after line~\ref{alg:line:counters}
  corresponds to the empirical distribution of
  equation~(\ref{eq:empirical_distribution}).
  Notice that if the agents have estimations of the frequencies $k_j/d_i$
  with error smaller than $1/d$ then by rounding them to the closest
  multiple of $1/d_i$ they learn the frequencies exactly.
  By Markov's inequality we have that with probability at least $4/5$ the
  rounded frequencies are exactly correct. By standard Chernoff bounds we
  have that if the agents repeat the above procedure
  $\ln(1/\delta)$ times and
  keep the most frequent of the answers $B(j)$ then they will
  obtain the correct answer with probability at least
  $1-\delta$. From Lemma~\ref{l:generic_convergence} we have
  that to achieve error $\eps$ we need $\log(1/\eps)/(1/(1-\alpha))$ rounds.
  Since we need all nodes to succeed at computing the exact averages for
$\log(1/\eps)/\log(1/(1-\alpha))$ rounds we have from the union bound that for
$\delta < \frac{\eps \ln(1/(1-\alpha))}{n \ln(1/\eps)}$, with
probability at least $1-\eps$ the error is at most $\eps$.
For the expected error after
$T = O(d^2 \log d ( \log(n/\eps) + \log \log(1/\eps) + \log \log (1-a))$ rounds
we have that $\Exp{\norm{\infty}{x_T - x^*}} = O(\eps)$.
\end{proof}
