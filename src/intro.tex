\section{Introduction 2}

\emph{Opinion Formation Games: }Let $G$ a directed graph $G(V,E)$ where $V$ denotes the agents and $E$ the social relations between them. Each agent $i$ admits an internal opinion $s_i \in [0,1]$ and a self confidence coefficient $a_i \in (0,1)$. The strategy that each agent $i$ selects is the opinion that she publicly declare, $x_i \in [0,1]$ incuring her a cost $$C_i(x_i,x_{-i})=(1-a_i)\sum_{j \in N_i}(x_i -x_j)^2 + a_i(x_i-s_i)^2$$\label{eq:best_response}The above game was first introduced in []. They proved that it always admits a unique PNE $x^*$ and has PoA=$9/8$ if $G$ is undirected and $\bigOh{n}$ in the directed case. We denote an instance of this game as $I=(G,s,a)$.\\

\noindent \emph{Best Response Dynamics: }Subsequently a dynamical version of the above game were studied. Initially, each agent $i$ has an opinion $x_1(0),\ldots,x_n(0)$. At round $t \geq 1$, each agent $i$ suffers the cost: $$C_i(x_i(t-1),x_{-i}(t-1))=(1-a_i)\sum_{j \in N_i}(x_i(t-1) -x_j(t-1))^2 + a_i(x_i(t-1)-s_i)^2~(1)$$ and updates her opinion so as to minimize her individual cost $$x_i(t) = \text{arg min}_{x \in [0,1]} C_i(x,x_{-i}(t-1))=(1-a_i)\frac{\sum_{j \in N_i}x_j(t-1)}{N_i} + a_is_i $$ \label{eq:best_response}


\noindent In [] it is proved that for any instance $I=(G,s,a)$ the opinion vector $x(t)=(x_1(t),\ldots,x_n(t))$ converges to the unique equilibrium point $x^*$.\\

The dynamical process defined by (1) admits several nice properties. At first, it ensures that the overall system converges to the equilibrium point $x^*$. Secondly, the update rule is very simple but more importantly it is \emph{rational}, in the sense that each agent $i$ adopts this rule in order to minize her individual cost.\\

\emph{Opinion Formation Games with Random Payoffs: }In the game definde in $(1)$, agent's $i$ cost $C_i(x_i,x_{-i})$ is a deterministic fuction of the opinion vector $x$. Many recent works study previously studied games under the perspective of random payoffs e.g. agent's $i$ cost is a $C_i(x_i,x_{-i})$ is a random variable. This random payoff setting can be much more convient to model realistic situations, where randomness may naturally occur because of incomplete information, noise or other stochastic factors. Motivated by this line of research we the \emph{Opinion Formation Game with Random Payoffs}. 

\begin{definition}
 Let $I=(G,s,a)$ an instance of the opinion formation game and $x$ the opinion vector. Each agent $i$, 
 \begin{itemize}
  \item picks uniformly at random one of her neighbors $j \in N_i$
  \item suffers cost $C_i(x_i,x_{-i}) = (1-a_i)\sum_{j \in N_i}(x_i-x_j)^2 + a_i(x_i-s_i)^2$
 \end{itemize}

\end{definition} The reason that the Opinion Formation Games with Random Payoffs is more compatible to a realistic settings is evident. In real world networks (e.g. Facebook, Twitter e.t.c.),
each agent may have several hundrends of friends. As a result is far more reasonable to assume that each day each agent meets a random subset of her friends and suffers a cost because of their disagreement. Notice that $\Exp{C_i(x_i,x_{-i})} = (1-a_i)\sum_{j \in N_i}(x_i-x_j)^2 + a_i(x_i-s_i)^2$, meaning that the random payoff variants admits the same equilibrium point $x^*$ with the origininal game. In this work, we investigate whether there exists a natural dynamical process that leads the system to the equilibrium point $x^*$.\\

\noindent We consider the following dynamics for the above game:
 \begin{itemize}
 \item Initially, each agent $i$ has an opinion $x_1(0),\ldots,x_n(0)$.
 \item At round $t \geq 1$, each agent $i$ picks uniformly at random one of her friends $j \in N_i$ and suffers cost: $$C^t_i(x_i(t-1),x_{j}(t-1))=(1-a_i)(x_i(t-1) -x_j(t-1))^2 + a_i(x_i(t-1)-s_i)^2$$ and updates her opinion as follows 
 $$x_i(t) = \text{arg min}_{x \in [0,1]} \sum_{\tau=1}^tC^{\tau}_i(x,x_{\pi_i(\tau)}(\tau-1))=(1-a_i)\frac{\sum_{\tau=1}^t x_{\pi_i(\tau)}(t-1)}{t} + a_is_i~(2) \label{eq:fictitious_play} $$ where $\pi_i(\tau)$ denotes the index of the index of the neighbor that $i$ selected at round $\tau$.
 \end{itemize}

 
 \noindent In this work, we prove that the opinion vector $x(t)$ converges to the same equilibrium point $x^*$ and that this update rule is rational for each agent $i$, since it ensures no regret. The above dynamics is the \emph{fictitious play} in the game defined by the instance $I=(G,s,a)$. Generally speaking \emph{fictitious play} neither garantees convergence to the PNE nor no regret for the players. In section 2 and 3 respectively, we prove the following theorems:
\begin{theorem}
For every instance $I=(G,s,a)$, if $x(t)$ is the opinion vector generated by the update rule $(2)$ then $$E[||x(t)-x^*||_{\infty}]\leq C \sqrt{\log n}\frac{(\log t)^2}{\sqrt{t}}$$ where $n$ is the number of agents and $C$ a universal constant.
\end{theorem}



\begin{theorem}
For every instance $I=(G,s,a)$, for every agent $i$ $$\sum_{\tau=1}^tC_i^\tau(x_i(\tau)) \leq \text{min}_{x \in [0,1]}\sum_{\tau=1}^tC_i^\tau(x) + \bigOh{\log t}$$ 
\end{theorem}
As a result, the update rule $(2)$ ensures that each agent has cost smaller than the cost tha she would have playing any fixed strategy.\\


In section 4, we investigate where a better convergence rate, $E[||x(t)-x^*||_{\infty}]$ can be acheived if agent selected another no regret algorithm. More precisely, we investigate the following question, \emph{Is there a no-regret algorithm such that for every instance } $I,~E[||x(t)-x^*||_{\infty}] \in \bigOh{\frac{1}{t}}?$  We reduce this question to the following question in statistical estimation, \emph{Is there a Bernoulli estimator }$\hat{\theta}$\emph{, such that for all }$q \in [0,1]$, $\lim_{t \rightarrow \infty} t E_q[|\hat{\theta^t} -q|]=0?$  To the best of our knowledge this question is not answered in statistics literature. However, we use standard tecnhiques for lower bounds in the statistical estimation to prove the following theorem.
\begin{theorem}
For any Bernoulli estimator $\hat{\theta}$, for all $[a,b] \subseteq [0,1]:~$ $\lim_{t \rightarrow \infty}t \int_{a}^bE_p[|\hat{\theta^t} -p|]= +\infty$
\end{theorem}
\noindent This result indicates that the second question and consequently the first is very unlike to hold, especially for any reasonable no-regret algotithm. We believe that there exists no such algorithm and we leave this proof as an open problem.\\

Finally in section 5, we present an interesting side result. The reason that there does not exists a no-regret algorithm, that ensures faster convergence rate, is the algorithm's ignorance to the instance $I$ that defines the dynamics. More formally, a no-regret algorithm does needs the values $s_i,a_i$ and $Y_1,\ldots,Y_t$ to determine $x_i(t)$. We find interesting that we design a distributed algorithm that uses $s_i,a_i,Y_1,\ldots,Y_t$ and additionally the $d_\text{max}$ of $G$ that acheves for every instance $I$ convergence rate, $E||x(t)-x^*||_{\infty} \in \bigOh{2^{-\frac{\sqrt{t}}{d^3}}}$.   





