\section{No Regret}
We consider the following online convex optimization problem. At each time step $t$, the player $i$ selects a real number $x^t$ and a fucntion $f^t(x)$ arrives. The player then suffers $f^t(x^t)$ cost. The functions $f^t(x)$ have the following form:
$$f^t(x) = \alpha(x-s_i)^2 + (1-\alpha)(x-a_t)^2$$
where $s_i,\alpha \in [0,1]$ and are independent of $t$ and $a_t \in [0,1]$. In other words, the function $f^t$ is uniquely determined by the number $a_t$.

We show that for this class of function $ fictitious play$ admits no regret.
\begin{theorem}Let $x^t = \arg min_{x \in [0,1]} \sum_{\tau = 1}^{t-1} f^\tau (x)$ then 
$$
\sum_{t = 1}^{T} f^t(x^t) \leq min_{x \in [0,1]} \sum_{t=1}^T f^t(x) + O(log T)
$$
\end{theorem}

At first we prove that the sequence $y^t = arg min_{x \in [0,1]} \sum_{\tau = 1}^{t}f^\tau (x)$ admits no regret.
\begin{lemma} Let $y^t = argmin_{x \in [0,1]} \sum_{\tau = 1}^t f^\tau (x)$ then 
$$ \sum_{t=1}^T f^t(y^t) \leq min_{x \in [0,1]} \sum_{t=1}^T f^t(x)$$
\end{lemma}
\begin{proof}By definition of $y^t$, $min_{x \in [0,1]} \sum_{t=1}^T f^t(x) = \sum_{t=1}^T f^t(y^T)$, so
\begin{align*}
\sum_{t=1}^T f^t(y^t) - min_{x \in [0,1]} \sum_{t=1}^T f^t(x) &=
\sum_{t=1}^T f^t(y^t) - \sum_{t=1}^T f^t(y^T)\\
&= \sum_{t=1}^{T-1} f^t(y^t) - \sum_{t=1}^{T-1} f^t(y^T)\\
&\leq \sum_{t=1}^{T-1} f^t(y^t) - \sum_{t=1}^{T-1} f^t(y^{T-1})\\
&= \sum_{t=1}^{T-2} f^t(y^t) - \sum_{t=1}^{T-2} f^t(y^{T-1})
\end{align*}
Continuing in the same way, we get $\sum_{t=1}^T f^t(y^t) \leq min_{x \in [0,1]} \sum_{t=1}^T f^t(x)$.
\end{proof}

Now we can derive some intuition for the reason that $fictitious play$ admits no regret. Since the cost incured by the sequence $y^t$ is at most that of the best fixed strategy, we can compare the cost incured by $x^t$ with that of $y^t$. However, for each $t$ the numbers $x^t$ and $y^t$ are quite close and as a result the difference in their cost must be quite small. 
\begin{lemma} For all $t$, $\lp|x^t - y^t \rp| \leq \frac{1-\alpha}{t}$
\end{lemma}
\begin{proof} By definition $x^t = \alpha s_i + (1-\alpha)\frac{\sum_{\tau = 1}^{t-1} a_\tau}{t-1}$ and $ y^t = \alpha s_i + (1-\alpha)\frac{\sum_{\tau = 1}^t a_\tau}{t}$.
\begin{align*}
\lp|x^t - y^t\rp| &= (1-\alpha)\lp|\frac{\sum_{\tau = 1}^{t-1}a_\tau}{t-1} - \frac{\sum_{\tau = 1}^t a_\tau}{t}\rp|\\
&= (1-\alpha)\lp|\frac{\sum_{\tau = 1}^{t-1}a_\tau -(t-1)a_t}{t(t-1)}\rp|\\
&\leq \frac{1-\alpha}{t}
\end{align*}
The last inequality follows from the fact that $a_\tau \in [0,1]$.
\end{proof}

\begin{lemma}For all $t$, $f^t(x^t) \leq f^t(y^t) + 2\frac{1-\alpha}{t} + \frac{(1-a)^2}{t^2}$. 
\end{lemma}
\begin{proof}
\begin{align*}
f^t(x^t) &= \alpha(x^t - s_i)^2 + (1 - \alpha)(x^t - a_t)^2 \\
&\leq \alpha(y^t - s_i)^2 + 2\alpha\lp|y^t - s_i\rp|\lp|x^t - y^t\rp| + \alpha \lp|x^t - y^t\rp|^2 \\
\phantom{{}\leq{}}+ (1-\alpha)(y_t - a_t)^2 + 2(1-\alpha)\lp|y^t - a_t\rp|\lp|x^t-y^t\rp| + (1 - \alpha)\lp|x^t - y^t\rp|^2\\
&\leq f^t(y^t) + 2\lp|x^t - y^t\rp| + \lp|y^t - x^t\rp|^2\\
&\leq f^t(y^t) + 2\frac{1-\alpha}{t} + \frac{(1-\alpha)^2}{t^2}
\end{align*}
\end{proof}
Now Theorem 1 easily follows since:
\begin{align*}
\sum_{t=1}^T f^t(x^t) &\leq \sum_{t=1}^T f^t(y^t) + \sum_{t=1}^T 2\frac{1-\alpha}{t} + \sum_{t=1}^T \frac{(1-\alpha)^2}{t^2}\\
&\leq  min_{x \in [0,1]} \sum_{t=1}^T f^t(x) + 2(1-\alpha)(logT + 1) + (1-\alpha)\frac{\pi^2}{6}\\
&\leq min_{x \in [0,1]} \sum_{t=1}^T f^t(x) + O(logT)
\end{align*}







